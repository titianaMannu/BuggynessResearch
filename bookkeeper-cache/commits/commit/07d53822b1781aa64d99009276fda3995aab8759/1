{"sha":"07d53822b1781aa64d99009276fda3995aab8759","node_id":"MDY6Q29tbWl0MTU3NTk1NjowN2Q1MzgyMmIxNzgxYWE2NGQ5OTAwOTI3NmZkYTM5OTVhYWI4NzU5","commit":{"author":{"name":"Ivan Brendan Kelly","email":"ivank@apache.org","date":"2013-09-09T09:54:28Z"},"committer":{"name":"Ivan Brendan Kelly","email":"ivank@apache.org","date":"2013-09-09T09:54:28Z"},"message":"BOOKKEEPER-664: Compaction increases latency on journal writes (ivank & sijie via ivank)\n\ngit-svn-id: https://svn.apache.org/repos/asf/zookeeper/bookkeeper/branches/branch-4.2@1521021 13f79535-47bb-0310-9956-ffa450edef68","tree":{"sha":"68d234fc39d1e7e9cc05962c3bf623591d31a4f3","url":"https://api.github.com/repos/apache/bookkeeper/git/trees/68d234fc39d1e7e9cc05962c3bf623591d31a4f3"},"url":"https://api.github.com/repos/apache/bookkeeper/git/commits/07d53822b1781aa64d99009276fda3995aab8759","comment_count":0,"verification":{"verified":false,"reason":"unsigned","signature":null,"payload":null}},"url":"https://api.github.com/repos/apache/bookkeeper/commits/07d53822b1781aa64d99009276fda3995aab8759","html_url":"https://github.com/apache/bookkeeper/commit/07d53822b1781aa64d99009276fda3995aab8759","comments_url":"https://api.github.com/repos/apache/bookkeeper/commits/07d53822b1781aa64d99009276fda3995aab8759/comments","author":{"login":"ivankelly","id":54955,"node_id":"MDQ6VXNlcjU0OTU1","avatar_url":"https://avatars.githubusercontent.com/u/54955?v=4","gravatar_id":"","url":"https://api.github.com/users/ivankelly","html_url":"https://github.com/ivankelly","followers_url":"https://api.github.com/users/ivankelly/followers","following_url":"https://api.github.com/users/ivankelly/following{/other_user}","gists_url":"https://api.github.com/users/ivankelly/gists{/gist_id}","starred_url":"https://api.github.com/users/ivankelly/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ivankelly/subscriptions","organizations_url":"https://api.github.com/users/ivankelly/orgs","repos_url":"https://api.github.com/users/ivankelly/repos","events_url":"https://api.github.com/users/ivankelly/events{/privacy}","received_events_url":"https://api.github.com/users/ivankelly/received_events","type":"User","site_admin":false},"committer":{"login":"ivankelly","id":54955,"node_id":"MDQ6VXNlcjU0OTU1","avatar_url":"https://avatars.githubusercontent.com/u/54955?v=4","gravatar_id":"","url":"https://api.github.com/users/ivankelly","html_url":"https://github.com/ivankelly","followers_url":"https://api.github.com/users/ivankelly/followers","following_url":"https://api.github.com/users/ivankelly/following{/other_user}","gists_url":"https://api.github.com/users/ivankelly/gists{/gist_id}","starred_url":"https://api.github.com/users/ivankelly/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ivankelly/subscriptions","organizations_url":"https://api.github.com/users/ivankelly/orgs","repos_url":"https://api.github.com/users/ivankelly/repos","events_url":"https://api.github.com/users/ivankelly/events{/privacy}","received_events_url":"https://api.github.com/users/ivankelly/received_events","type":"User","site_admin":false},"parents":[{"sha":"6ef7015e2f078a3a37bc4ddb75cca30f2fa2e575","url":"https://api.github.com/repos/apache/bookkeeper/commits/6ef7015e2f078a3a37bc4ddb75cca30f2fa2e575","html_url":"https://github.com/apache/bookkeeper/commit/6ef7015e2f078a3a37bc4ddb75cca30f2fa2e575"}],"stats":{"total":485,"additions":358,"deletions":127},"files":[{"sha":"3e99575680204834d0d996e0e7d08043f1152467","filename":"CHANGES.txt","status":"modified","additions":2,"deletions":0,"changes":2,"blob_url":"https://github.com/apache/bookkeeper/blob/07d53822b1781aa64d99009276fda3995aab8759/CHANGES.txt","raw_url":"https://github.com/apache/bookkeeper/raw/07d53822b1781aa64d99009276fda3995aab8759/CHANGES.txt","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/CHANGES.txt?ref=07d53822b1781aa64d99009276fda3995aab8759","patch":"@@ -70,6 +70,8 @@ Release 4.2.2 - Unreleased\n \n         BOOKKEEPER-580: improve close logic (sijie & ivank via ivank)\n \n+        BOOKKEEPER-664: Compaction increases latency on journal writes (ivank & sijie via ivank)\n+\n       hedwig-server:\n \n         BOOKKEEPER-579: TestSubAfterCloseSub was put in a wrong package (sijie via ivank)"},{"sha":"fd3ebd66a520736df1b4540f9ccd7c0c222618a6","filename":"bookkeeper-server/conf/bk_server.conf","status":"modified","additions":13,"deletions":0,"changes":13,"blob_url":"https://github.com/apache/bookkeeper/blob/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/conf/bk_server.conf","raw_url":"https://github.com/apache/bookkeeper/raw/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/conf/bk_server.conf","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/conf/bk_server.conf?ref=07d53822b1781aa64d99009276fda3995aab8759","patch":"@@ -91,6 +91,19 @@ ledgerDirectories=/tmp/bk-data\n # If it is set to less than zero, the major compaction is disabled. \n # majorCompactionInterval=86400 \n \n+# Set the maximum number of entries which can be compacted without flushing.\n+# When compacting, the entries are written to the entrylog and the new offsets\n+# are cached in memory. Once the entrylog is flushed the index is updated with\n+# the new offsets. This parameter controls the number of entries added to the\n+# entrylog before a flush is forced. A higher value for this parameter means\n+# more memory will be used for offsets. Each offset consists of 3 longs.\n+# This parameter should _not_ be modified unless you know what you're doing.\n+# The default is 100,000.\n+#compactionMaxOutstandingRequests=100000\n+\n+# Set the rate at which compaction will readd entries. The unit is adds per second.\n+#compactionRate=1000\n+\n # Max file size of journal file, in mega bytes\n # A new journal file will be created when the old one reaches the file size limitation\n #"},{"sha":"919b8b63df93ce9e1902ddeacbe765cc8d898e4d","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","status":"modified","additions":1,"deletions":35,"changes":36,"blob_url":"https://github.com/apache/bookkeeper/blob/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","raw_url":"https://github.com/apache/bookkeeper/raw/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java?ref=07d53822b1781aa64d99009276fda3995aab8759","patch":"@@ -41,15 +41,13 @@\n import org.apache.bookkeeper.meta.LedgerManager;\n import org.apache.bookkeeper.meta.LedgerManagerFactory;\n import org.apache.bookkeeper.bookie.BookieException;\n-import org.apache.bookkeeper.bookie.GarbageCollectorThread.SafeEntryAdder;\n import org.apache.bookkeeper.bookie.Journal.JournalScanner;\n import org.apache.bookkeeper.bookie.LedgerDirsManager.LedgerDirsListener;\n import org.apache.bookkeeper.bookie.LedgerDirsManager.NoWritableLedgerDirException;\n import org.apache.bookkeeper.conf.ServerConfiguration;\n import org.apache.bookkeeper.jmx.BKMBeanInfo;\n import org.apache.bookkeeper.jmx.BKMBeanRegistry;\n import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;\n-import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;\n import org.apache.bookkeeper.util.BookKeeperConstants;\n import org.apache.bookkeeper.util.IOUtils;\n import org.apache.bookkeeper.util.MathUtils;\n@@ -546,8 +544,7 @@ public Bookie(ServerConfiguration conf)\n         ledgerManager = ledgerManagerFactory.newLedgerManager();\n         syncThread = new SyncThread(conf);\n         ledgerStorage = new InterleavedLedgerStorage(conf, ledgerManager,\n-                                                     ledgerDirsManager,\n-                                                     new BookieSafeEntryAdder());\n+                                                     ledgerDirsManager);\n         handles = new HandleFactoryImpl(ledgerStorage);\n         // instantiate the journal\n         journal = new Journal(conf, ledgerDirsManager);\n@@ -1209,37 +1206,6 @@ private static boolean cleanDir(File dir) {\n         return true;\n     }\n \n-    private class BookieSafeEntryAdder implements SafeEntryAdder {\n-        @Override\n-        public void safeAddEntry(final long ledgerId, final ByteBuffer buffer,\n-                                 final GenericCallback<Void> cb) {\n-            journal.logAddEntry(buffer, new WriteCallback() {\n-                    @Override\n-                    public void writeComplete(int rc, long ledgerId2, long entryId,\n-                                              InetSocketAddress addr, Object ctx) {\n-                        if (rc != BookieException.Code.OK) {\n-                            LOG.error(\"Error rewriting to journal (ledger {}, entry {})\", ledgerId2, entryId);\n-                            cb.operationComplete(rc, null);\n-                            return;\n-                        }\n-                        try {\n-                            addEntryByLedgerId(ledgerId, buffer);\n-                            cb.operationComplete(rc, null);\n-                        } catch (IOException ioe) {\n-                            LOG.error(\"Error adding to ledger storage (ledger \" + ledgerId2\n-                                      + \", entry \" + entryId + \")\", ioe);\n-                            // couldn't add to ledger storage\n-                            cb.operationComplete(BookieException.Code.IllegalOpException, null);\n-                        } catch (BookieException bke) {\n-                            LOG.error(\"Bookie error adding to ledger storage (ledger \" + ledgerId2\n-                                      + \", entry \" + entryId + \")\", bke);\n-                            // couldn't add to ledger storage\n-                            cb.operationComplete(bke.getCode(), null);\n-                        }\n-                    }\n-                }, null);\n-        }\n-    }\n     /**\n      * @param args\n      * @throws IOException"},{"sha":"26de1be6234dca2e7d8b64634f015baf3512391b","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java","status":"modified","additions":33,"deletions":0,"changes":33,"blob_url":"https://github.com/apache/bookkeeper/blob/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java","raw_url":"https://github.com/apache/bookkeeper/raw/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java?ref=07d53822b1781aa64d99009276fda3995aab8759","patch":"@@ -40,6 +40,7 @@\n import java.util.Map.Entry;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.CopyOnWriteArrayList;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -69,6 +70,11 @@\n      */\n     final long logSizeLimit;\n     private volatile BufferedChannel logChannel;\n+    private final CopyOnWriteArrayList<EntryLogListener> listeners\n+        = new CopyOnWriteArrayList<EntryLogListener>();\n+    // this indicates that a write has happened since the last flush\n+    private volatile boolean somethingWritten = false;\n+\n     /**\n      * The 1K block at the head of the entry logger file\n      * that contains the fingerprint and (future) meta-data\n@@ -107,6 +113,16 @@\n         public void process(long ledgerId, long offset, ByteBuffer entry) throws IOException;\n     }\n \n+    /**\n+     * Entry Log Listener\n+     */\n+    static interface EntryLogListener {\n+        /**\n+         * Rotate a new entry log to write.\n+         */\n+        public void onRotateEntryLog();\n+    }\n+\n     /**\n      * Create an EntryLogger that stores it's log files in the given\n      * directories\n@@ -140,6 +156,12 @@ public EntryLogger(ServerConfiguration conf,\n         initialize();\n     }\n \n+    void addListener(EntryLogListener listener) {\n+        if (null != listener) {\n+            listeners.add(listener);\n+        }\n+    }\n+\n     /**\n      * Maps entry log files to open channels.\n      */\n@@ -326,7 +348,16 @@ synchronized void flush() throws IOException {\n         if (logChannel != null) {\n             logChannel.flush(true);\n         }\n+        somethingWritten = false;\n+        for (EntryLogListener listener: listeners) {\n+            listener.onRotateEntryLog();\n+        }\n     }\n+\n+    boolean isFlushRequired() {\n+        return somethingWritten;\n+    }\n+\n     synchronized long addEntry(long ledger, ByteBuffer entry) throws IOException {\n         // Create new log if logSizeLimit reached or current disk is full\n         boolean createNewLog = shouldCreateNewEntryLog.get();\n@@ -347,6 +378,8 @@ synchronized long addEntry(long ledger, ByteBuffer entry) throws IOException {\n         logChannel.write(entry);\n         //logChannel.flush(false);\n \n+        somethingWritten = true;\n+\n         return (logId << 32L) | pos;\n     }\n "},{"sha":"56cb07a9c8f30e695333d285dc75c4aa64578a30","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","status":"modified","additions":106,"deletions":78,"changes":184,"blob_url":"https://github.com/apache/bookkeeper/blob/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","raw_url":"https://github.com/apache/bookkeeper/raw/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java?ref=07d53822b1781aa64d99009276fda3995aab8759","patch":"@@ -30,9 +30,9 @@\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.atomic.AtomicInteger;\n \n-import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;\n+import com.google.common.util.concurrent.RateLimiter;\n+\n import org.apache.bookkeeper.bookie.EntryLogger.EntryLogScanner;\n import org.apache.bookkeeper.bookie.GarbageCollector.GarbageCleaner;\n import org.apache.bookkeeper.conf.ServerConfiguration;\n@@ -48,7 +48,6 @@\n  */\n public class GarbageCollectorThread extends Thread {\n     private static final Logger LOG = LoggerFactory.getLogger(GarbageCollectorThread.class);\n-    private static final int COMPACTION_MAX_OUTSTANDING_REQUESTS = 1000;\n     private static final int SECOND = 1000;\n \n     // Maps entry log files to the set of ledgers that comprise the file and the size usage per ledger\n@@ -69,9 +68,12 @@\n     long lastMinorCompactionTime;\n     long lastMajorCompactionTime;\n \n+    final int maxOutstandingRequests;\n+    final int compactionRate;\n+    final CompactionScannerFactory scannerFactory;\n+\n     // Entry Logger Handle\n     final EntryLogger entryLogger;\n-    final SafeEntryAdder safeEntryAdder;\n \n     // Ledger Cache Handle\n     final LedgerCache ledgerCache;\n@@ -89,77 +91,88 @@\n     final GarbageCollector garbageCollector;\n     final GarbageCleaner garbageCleaner;\n \n+    private static class Offset {\n+        final long ledger;\n+        final long entry;\n+        final long offset;\n \n-    /**\n-     * Interface for adding entries. When the write callback is triggered, the\n-     * entry must be guaranteed to be presisted.\n-     */\n-    interface SafeEntryAdder {\n-        public void safeAddEntry(long ledgerId, ByteBuffer buffer, GenericCallback<Void> cb);\n+        Offset(long ledger, long entry, long offset) {\n+            this.ledger = ledger;\n+            this.entry = entry;\n+            this.offset = offset;\n+        }\n     }\n \n     /**\n      * A scanner wrapper to check whether a ledger is alive in an entry log file\n      */\n-    class CompactionScanner implements EntryLogScanner {\n-        EntryLogMetadata meta;\n-        Object completionLock = new Object();\n-        AtomicInteger outstandingRequests = new AtomicInteger(0);\n-        AtomicBoolean allSuccessful = new AtomicBoolean(true);\n+    class CompactionScannerFactory implements EntryLogger.EntryLogListener {\n+        List<Offset> offsets = new ArrayList<Offset>();\n+\n+        EntryLogScanner newScanner(final EntryLogMetadata meta) {\n+            final RateLimiter rateLimiter = RateLimiter.create(compactionRate);\n+            return new EntryLogScanner() {\n+                @Override\n+                public boolean accept(long ledgerId) {\n+                    return meta.containsLedger(ledgerId);\n+                }\n \n-        public CompactionScanner(EntryLogMetadata meta) {\n-            this.meta = meta;\n-        }\n+                @Override\n+                public void process(final long ledgerId, long offset, ByteBuffer entry)\n+                        throws IOException {\n+                    rateLimiter.acquire();\n+                    synchronized (CompactionScannerFactory.this) {\n+                        if (offsets.size() > maxOutstandingRequests) {\n+                            waitEntrylogFlushed();\n+                        }\n+                        entry.getLong(); // discard ledger id, we already have it\n+                        long entryId = entry.getLong();\n+                        entry.rewind();\n \n-        @Override\n-        public boolean accept(long ledgerId) {\n-            return meta.containsLedger(ledgerId);\n+                        flushed.set(false);\n+                        long newoffset = entryLogger.addEntry(ledgerId, entry);\n+                        offsets.add(new Offset(ledgerId, entryId, newoffset));\n+                    }\n+                }\n+            };\n         }\n \n+        AtomicBoolean flushed = new AtomicBoolean(false);\n+        Object flushLock = new Object();\n+\n         @Override\n-        public void process(final long ledgerId, long offset, ByteBuffer entry)\n-            throws IOException {\n-            if (!allSuccessful.get()) {\n-                return;\n+        public void onRotateEntryLog() {\n+            synchronized (flushLock) {\n+                flushed.set(true);\n+                flushLock.notifyAll();\n             }\n+        }\n \n-            outstandingRequests.incrementAndGet();\n-            synchronized (completionLock) {\n-                while (outstandingRequests.get() >= COMPACTION_MAX_OUTSTANDING_REQUESTS) {\n-                    try {\n-                        completionLock.wait();\n-                    } catch (InterruptedException ie) {\n-                        LOG.error(\"Interrupted while waiting to re-add entry\", ie);\n-                        Thread.currentThread().interrupt();\n-                        throw new IOException(\"Interrupted while waiting to re-add entry\", ie);\n+        synchronized private void waitEntrylogFlushed() throws IOException {\n+            try {\n+                synchronized (flushLock) {\n+                    while (!flushed.get() && running) {\n+                        flushLock.wait(1000);\n+                    }\n+                    if (!flushed.get() && !running) {\n+                        throw new IOException(\"Shutdown before flushed\");\n                     }\n                 }\n+            } catch (InterruptedException ie) {\n+                Thread.currentThread().interrupt();\n+                throw new IOException(\"Interrupted waiting for flush\", ie);\n             }\n-            safeEntryAdder.safeAddEntry(ledgerId, entry, new GenericCallback<Void>() {\n-                    @Override\n-                    public void operationComplete(int rc, Void result) {\n-                        if (rc != BookieException.Code.OK) {\n-                            LOG.error(\"Error {} re-adding entry for ledger {})\",\n-                                      rc, ledgerId);\n-                            allSuccessful.set(false);\n-                        }\n-                        synchronized(completionLock) {\n-                            outstandingRequests.decrementAndGet();\n-                            completionLock.notifyAll();\n-                        }\n-                    }\n-                });\n-        }\n \n-        void awaitComplete() throws InterruptedException, IOException {\n-            synchronized(completionLock) {\n-                while (outstandingRequests.get() > 0) {\n-                    completionLock.wait();\n-                }\n-                if (allSuccessful.get() == false) {\n-                    throw new IOException(\"Couldn't re-add all entries\");\n-                }\n+            for (Offset o : offsets) {\n+                ledgerCache.putEntryOffset(o.ledger, o.entry, o.offset);\n             }\n+            offsets.clear();\n+        }\n+\n+        synchronized void flush() throws IOException {\n+            waitEntrylogFlushed();\n+\n+            ledgerCache.flushLedger(true);\n         }\n     }\n \n@@ -175,17 +188,19 @@ public GarbageCollectorThread(ServerConfiguration conf,\n                                   final LedgerCache ledgerCache,\n                                   EntryLogger entryLogger,\n                                   SnapshotMap<Long, Boolean> activeLedgers,\n-                                  SafeEntryAdder safeEntryAdder,\n                                   LedgerManager ledgerManager)\n         throws IOException {\n         super(\"GarbageCollectorThread\");\n \n         this.ledgerCache = ledgerCache;\n         this.entryLogger = entryLogger;\n         this.activeLedgers = activeLedgers;\n-        this.safeEntryAdder = safeEntryAdder;\n \n         this.gcWaitTime = conf.getGcWaitTime();\n+        this.maxOutstandingRequests = conf.getCompactionMaxOutstandingRequests();\n+        this.compactionRate = conf.getCompactionRate();\n+        this.scannerFactory = new CompactionScannerFactory();\n+        entryLogger.addListener(this.scannerFactory);\n \n         this.garbageCleaner = new GarbageCollector.GarbageCleaner() {\n             @Override\n@@ -353,16 +368,42 @@ public int compare(EntryLogMetadata m1, EntryLogMetadata m2) {\n         List<EntryLogMetadata> logsToCompact = new ArrayList<EntryLogMetadata>();\n         logsToCompact.addAll(entryLogMetaMap.values());\n         Collections.sort(logsToCompact, sizeComparator);\n+        List<Long> toRemove = new ArrayList<Long>();\n+\n         for (EntryLogMetadata meta : logsToCompact) {\n             if (meta.getUsage() >= threshold) {\n                 break;\n             }\n             LOG.debug(\"Compacting entry log {} below threshold {}.\", meta.entryLogId, threshold);\n-            compactEntryLog(meta.entryLogId);\n+            try {\n+                compactEntryLog(scannerFactory, meta);\n+                toRemove.add(meta.entryLogId);\n+            } catch (LedgerDirsManager.NoWritableLedgerDirException nwlde) {\n+                LOG.warn(\"No writable ledger directory available, aborting compaction\", nwlde);\n+                break;\n+            } catch (IOException ioe) {\n+                // if compact entry log throws IOException, we don't want to remove that\n+                // entry log. however, if some entries from that log have been readded\n+                // to the entry log, and the offset updated, it's ok to flush that\n+                LOG.error(\"Error compacting entry log. Log won't be deleted\", ioe);\n+            }\n+\n             if (!running) { // if gc thread is not running, stop compaction\n                 return;\n             }\n         }\n+        try {\n+            // compaction finished, flush any outstanding offsets\n+            scannerFactory.flush();\n+        } catch (IOException ioe) {\n+            LOG.error(\"Cannot flush compacted entries, skip removal\", ioe);\n+            return;\n+        }\n+\n+        // offsets have been flushed, its now safe to remove the old entrylogs\n+        for (Long l : toRemove) {\n+            removeEntryLog(l);\n+        }\n     }\n \n     /**\n@@ -399,13 +440,8 @@ private void removeEntryLog(long entryLogId) {\n      * @param entryLogId\n      *          Entry Log File Id\n      */\n-    protected void compactEntryLog(long entryLogId) {\n-        EntryLogMetadata entryLogMeta = entryLogMetaMap.get(entryLogId);\n-        if (null == entryLogMeta) {\n-            LOG.warn(\"Can't get entry log meta when compacting entry log \" + entryLogId + \".\");\n-            return;\n-        }\n-\n+    protected void compactEntryLog(CompactionScannerFactory scannerFactory,\n+                                   EntryLogMetadata entryLogMeta) throws IOException {\n         // Similar with Sync Thread\n         // try to mark compacting flag to make sure it would not be interrupted\n         // by shutdown during compaction. otherwise it will receive\n@@ -417,19 +453,11 @@ protected void compactEntryLog(long entryLogId) {\n             return;\n         }\n \n-        LOG.info(\"Compacting entry log : \" + entryLogId);\n+        LOG.info(\"Compacting entry log : {}\", entryLogMeta.entryLogId);\n \n         try {\n-            CompactionScanner scanner = new CompactionScanner(entryLogMeta);\n-            entryLogger.scanEntryLog(entryLogId, scanner);\n-            scanner.awaitComplete();\n-            // after moving entries to new entry log, remove this old one\n-            removeEntryLog(entryLogId);\n-        } catch (IOException e) {\n-            LOG.info(\"Premature exception when compacting \" + entryLogId, e);\n-        } catch (InterruptedException ie) {\n-            Thread.currentThread().interrupt();\n-            LOG.warn(\"Interrupted while compacting\", ie);\n+            entryLogger.scanEntryLog(entryLogMeta.entryLogId,\n+                                     scannerFactory.newScanner(entryLogMeta));\n         } finally {\n             // clear compacting flag\n             compacting.set(false);"},{"sha":"800e6bb90c78eff85a3194ccbc31cec9381964e4","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/InterleavedLedgerStorage.java","status":"modified","additions":5,"deletions":13,"changes":18,"blob_url":"https://github.com/apache/bookkeeper/blob/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/InterleavedLedgerStorage.java","raw_url":"https://github.com/apache/bookkeeper/raw/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/InterleavedLedgerStorage.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/InterleavedLedgerStorage.java?ref=07d53822b1781aa64d99009276fda3995aab8759","patch":"@@ -53,18 +53,14 @@\n     // has lower remaining percentage to reclaim disk space.\n     final GarbageCollectorThread gcThread;\n \n-    // this indicates that a write has happened since the last flush\n-    private volatile boolean somethingWritten = false;\n-\n     InterleavedLedgerStorage(ServerConfiguration conf,\n-                             LedgerManager ledgerManager, LedgerDirsManager ledgerDirsManager,\n-                             GarbageCollectorThread.SafeEntryAdder safeEntryAdder)\n+                             LedgerManager ledgerManager, LedgerDirsManager ledgerDirsManager)\n \t\t\tthrows IOException {\n         activeLedgers = new SnapshotMap<Long, Boolean>();\n         entryLogger = new EntryLogger(conf, ledgerDirsManager);\n         ledgerCache = new LedgerCacheImpl(conf, activeLedgers, ledgerDirsManager);\n         gcThread = new GarbageCollectorThread(conf, ledgerCache, entryLogger,\n-                activeLedgers, safeEntryAdder, ledgerManager);\n+                activeLedgers, ledgerManager);\n     }\n \n     @Override\n@@ -127,8 +123,6 @@ synchronized public long addEntry(ByteBuffer entry) throws IOException {\n          */\n         ledgerCache.putEntryOffset(ledgerId, entryId, pos);\n \n-        somethingWritten = true;\n-\n         return entryId;\n     }\n \n@@ -151,16 +145,14 @@ public ByteBuffer getEntry(long ledgerId, long entryId) throws IOException {\n \n     @Override\n     public boolean isFlushRequired() {\n-        return somethingWritten;\n-    };\n+        return entryLogger.isFlushRequired();\n+    }\n \n     @Override\n     public void flush() throws IOException {\n-\n-        if (!somethingWritten) {\n+        if (!isFlushRequired()) {\n             return;\n         }\n-        somethingWritten = false;\n         boolean flushFailed = false;\n \n         try {"},{"sha":"d9363f81991626cb95ce197f9ed9c09c1760f46d","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java","status":"modified","additions":55,"deletions":0,"changes":55,"blob_url":"https://github.com/apache/bookkeeper/blob/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java","raw_url":"https://github.com/apache/bookkeeper/raw/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java?ref=07d53822b1781aa64d99009276fda3995aab8759","patch":"@@ -32,6 +32,9 @@\n     protected final static String MINOR_COMPACTION_THRESHOLD = \"minorCompactionThreshold\";\n     protected final static String MAJOR_COMPACTION_INTERVAL = \"majorCompactionInterval\";\n     protected final static String MAJOR_COMPACTION_THRESHOLD = \"majorCompactionThreshold\";\n+    protected final static String COMPACTION_MAX_OUTSTANDING_REQUESTS\n+        = \"compactionMaxOutstandingRequests\";\n+    protected final static String COMPACTION_RATE = \"compactionRate\";\n \n     // Gc Parameters\n     protected final static String GC_WAIT_TIME = \"gcWaitTime\";\n@@ -755,4 +758,56 @@ public ServerConfiguration setAutoRecoveryDaemonEnabled(boolean enabled) {\n     public boolean isAutoRecoveryDaemonEnabled() {\n         return getBoolean(AUTO_RECOVERY_DAEMON_ENABLED, false);\n     }\n+\n+    /**\n+     * Get the maximum number of entries which can be compacted without flushing.\n+     * Default is 100,000.\n+     *\n+     * @return the maximum number of unflushed entries\n+     */\n+    public int getCompactionMaxOutstandingRequests() {\n+        return getInt(COMPACTION_MAX_OUTSTANDING_REQUESTS, 100000);\n+    }\n+\n+    /**\n+     * Set the maximum number of entries which can be compacted without flushing.\n+     *\n+     * When compacting, the entries are written to the entrylog and the new offsets\n+     * are cached in memory. Once the entrylog is flushed the index is updated with\n+     * the new offsets. This parameter controls the number of entries added to the\n+     * entrylog before a flush is forced. A higher value for this parameter means\n+     * more memory will be used for offsets. Each offset consists of 3 longs.\n+     *\n+     * This parameter should _not_ be modified unless you know what you're doing.\n+     * The default is 100,000.\n+     *\n+     * @param maxOutstandingRequests number of entries to compact before flushing\n+     *\n+     * @return ServerConfiguration\n+     */\n+    public ServerConfiguration setCompactionMaxOutstandingRequests(int maxOutstandingRequests) {\n+        setProperty(COMPACTION_MAX_OUTSTANDING_REQUESTS, maxOutstandingRequests);\n+        return this;\n+    }\n+\n+    /**\n+     * Get the rate of compaction adds. Default is 1,000.\n+     *\n+     * @return rate of compaction (adds per second)\n+     */\n+    public int getCompactionRate() {\n+        return getInt(COMPACTION_RATE, 1000);\n+    }\n+\n+    /**\n+     * Set the rate of compaction adds.\n+     *\n+     * @param rate rate of compaction adds (adds per second)\n+     *\n+     * @return ServerConfiguration\n+     */\n+    public ServerConfiguration setCompactionRate(int rate) {\n+        setProperty(COMPACTION_RATE, rate);\n+        return this;\n+    }\n }"},{"sha":"e9e07ad64425f4b2c17652591bf3ed160d21e43a","filename":"bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","status":"modified","additions":143,"deletions":1,"changes":144,"blob_url":"https://github.com/apache/bookkeeper/blob/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","raw_url":"https://github.com/apache/bookkeeper/raw/07d53822b1781aa64d99009276fda3995aab8759/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java?ref=07d53822b1781aa64d99009276fda3995aab8759","patch":"@@ -21,15 +21,29 @@\n  *\n  */\n import java.io.File;\n-import java.util.Arrays;\n+import java.io.IOException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.Collections;\n import java.util.Enumeration;\n \n+import org.apache.bookkeeper.meta.LedgerManager;\n+import org.apache.bookkeeper.conf.ServerConfiguration;\n import org.apache.bookkeeper.client.LedgerEntry;\n import org.apache.bookkeeper.client.LedgerHandle;\n import org.apache.bookkeeper.client.BookKeeper.DigestType;\n import org.apache.bookkeeper.test.BookKeeperClusterTestCase;\n import org.apache.bookkeeper.util.TestUtils;\n \n+import org.apache.zookeeper.AsyncCallback;\n+import org.apache.bookkeeper.client.LedgerMetadata;\n+import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;\n+import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;\n+import org.apache.bookkeeper.versioning.Version;\n+\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -292,4 +306,132 @@ public void testCompactionSmallEntryLogs() throws Exception {\n         // since those entries has been compacted to new entry log\n         verifyLedger(lhs[0].getId(), 0, lhs[0].getLastAddConfirmed());\n     }\n+\n+    /**\n+     * Test that compaction doesnt add to index without having persisted\n+     * entrylog first. This is needed because compaction doesn't go through the journal.\n+     * {@see https://issues.apache.org/jira/browse/BOOKKEEPER-530}\n+     * {@see https://issues.apache.org/jira/browse/BOOKKEEPER-664}\n+     */\n+    @Test(timeout=60000)\n+    public void testCompactionSafety() throws Exception {\n+        tearDown(); // I dont want the test infrastructure\n+        ServerConfiguration conf = new ServerConfiguration();\n+        final Set<Long> ledgers = Collections.newSetFromMap(new ConcurrentHashMap<Long, Boolean>());\n+        LedgerManager manager = new LedgerManager() {\n+                @Override\n+                public void createLedger(LedgerMetadata metadata, GenericCallback<Long> cb) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void removeLedgerMetadata(long ledgerId, Version version,\n+                                                 GenericCallback<Void> vb) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void readLedgerMetadata(long ledgerId, GenericCallback<LedgerMetadata> readCb) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void writeLedgerMetadata(long ledgerId, LedgerMetadata metadata,\n+                        GenericCallback<Void> cb) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void asyncProcessLedgers(Processor<Long> processor,\n+                                                AsyncCallback.VoidCallback finalCb,\n+                        Object context, int successRc, int failureRc) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void close() throws IOException {}\n+\n+                void unsupported() {\n+                    LOG.error(\"Unsupported operation called\", new Exception());\n+                    throw new RuntimeException(\"Unsupported op\");\n+                }\n+                @Override\n+                public LedgerRangeIterator getLedgerRanges() {\n+                    final AtomicBoolean hasnext = new AtomicBoolean(true);\n+                    return new LedgerManager.LedgerRangeIterator() {\n+                        @Override\n+                        public boolean hasNext() throws IOException {\n+                            return hasnext.get();\n+                        }\n+                        @Override\n+                        public LedgerManager.LedgerRange next() throws IOException {\n+                            hasnext.set(false);\n+                            return new LedgerManager.LedgerRange(ledgers);\n+                        }\n+                    };\n+                 }\n+            };\n+\n+        File tmpDir = File.createTempFile(\"bkTest\", \".dir\");\n+        tmpDir.delete();\n+        tmpDir.mkdir();\n+        File curDir = Bookie.getCurrentDirectory(tmpDir);\n+        Bookie.checkDirectoryStructure(curDir);\n+        conf.setLedgerDirNames(new String[] {tmpDir.toString()});\n+\n+        conf.setEntryLogSizeLimit(EntryLogger.LOGFILE_HEADER_SIZE + 3 * (4+ENTRY_SIZE));\n+        conf.setGcWaitTime(100);\n+        conf.setMinorCompactionThreshold(0.7f);\n+        conf.setMajorCompactionThreshold(0.0f);\n+        conf.setMinorCompactionInterval(1);\n+        conf.setMajorCompactionInterval(10);\n+        conf.setPageLimit(1);\n+\n+        final byte[] KEY = \"foobar\".getBytes();\n+        File log0 = new File(curDir, \"0.log\");\n+        LedgerDirsManager dirs = new LedgerDirsManager(conf);\n+        assertFalse(\"Log shouldnt exist\", log0.exists());\n+        InterleavedLedgerStorage storage = new InterleavedLedgerStorage(conf, manager, dirs);\n+        ledgers.add(1l);\n+        ledgers.add(2l);\n+        ledgers.add(3l);\n+        storage.setMasterKey(1, KEY);\n+        storage.setMasterKey(2, KEY);\n+        storage.setMasterKey(3, KEY);\n+        storage.addEntry(genEntry(1, 1, ENTRY_SIZE));\n+        storage.addEntry(genEntry(2, 1, ENTRY_SIZE));\n+        storage.addEntry(genEntry(2, 2, ENTRY_SIZE));\n+        storage.addEntry(genEntry(3, 2, ENTRY_SIZE));\n+        storage.flush();\n+        storage.shutdown();\n+\n+        assertTrue(\"Log should exist\", log0.exists());\n+        ledgers.remove(2l);\n+        ledgers.remove(3l);\n+\n+        storage = new InterleavedLedgerStorage(conf, manager, dirs);\n+        storage.start();\n+        for (int i = 0; i < 10; i++) {\n+            if (!log0.exists()) {\n+                break;\n+            }\n+            Thread.sleep(1000);\n+            storage.entryLogger.flush(); // simulate sync thread\n+        }\n+        assertFalse(\"Log shouldnt exist\", log0.exists());\n+\n+        ledgers.add(4l);\n+        storage.setMasterKey(4, KEY);\n+        storage.addEntry(genEntry(4, 1, ENTRY_SIZE)); // force ledger 1 page to flush\n+\n+        storage = new InterleavedLedgerStorage(conf, manager, dirs);\n+        storage.getEntry(1, 1); // entry should exist\n+    }\n+\n+    private ByteBuffer genEntry(long ledger, long entry, int size) {\n+        byte[] data = new byte[size];\n+        ByteBuffer bb = ByteBuffer.wrap(new byte[size]);\n+        bb.putLong(ledger);\n+        bb.putLong(entry);\n+        while (bb.hasRemaining()) {\n+            bb.put((byte)0xFF);\n+        }\n+        bb.flip();\n+        return bb;\n+    }\n }"}]}

