{"sha":"9602c32f16184e10873d53c269e270e3cd0ebeed","node_id":"MDY6Q29tbWl0MTU3NTk1Njo5NjAyYzMyZjE2MTg0ZTEwODczZDUzYzI2OWUyNzBlM2NkMGViZWVk","commit":{"author":{"name":"Sijie Guo","email":"sijie@apache.org","date":"2013-10-22T05:44:13Z"},"committer":{"name":"Sijie Guo","email":"sijie@apache.org","date":"2013-10-22T05:44:13Z"},"message":"BOOKKEEPER-664: Compaction increases latency on journal writes (ivank via sijie)\n\ngit-svn-id: https://svn.apache.org/repos/asf/zookeeper/bookkeeper/trunk@1534503 13f79535-47bb-0310-9956-ffa450edef68","tree":{"sha":"92133aaccbff38a5ec7c5d53a0dc11027494a654","url":"https://api.github.com/repos/apache/bookkeeper/git/trees/92133aaccbff38a5ec7c5d53a0dc11027494a654"},"url":"https://api.github.com/repos/apache/bookkeeper/git/commits/9602c32f16184e10873d53c269e270e3cd0ebeed","comment_count":0,"verification":{"verified":false,"reason":"unsigned","signature":null,"payload":null}},"url":"https://api.github.com/repos/apache/bookkeeper/commits/9602c32f16184e10873d53c269e270e3cd0ebeed","html_url":"https://github.com/apache/bookkeeper/commit/9602c32f16184e10873d53c269e270e3cd0ebeed","comments_url":"https://api.github.com/repos/apache/bookkeeper/commits/9602c32f16184e10873d53c269e270e3cd0ebeed/comments","author":{"login":"sijie","id":1217863,"node_id":"MDQ6VXNlcjEyMTc4NjM=","avatar_url":"https://avatars.githubusercontent.com/u/1217863?v=4","gravatar_id":"","url":"https://api.github.com/users/sijie","html_url":"https://github.com/sijie","followers_url":"https://api.github.com/users/sijie/followers","following_url":"https://api.github.com/users/sijie/following{/other_user}","gists_url":"https://api.github.com/users/sijie/gists{/gist_id}","starred_url":"https://api.github.com/users/sijie/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sijie/subscriptions","organizations_url":"https://api.github.com/users/sijie/orgs","repos_url":"https://api.github.com/users/sijie/repos","events_url":"https://api.github.com/users/sijie/events{/privacy}","received_events_url":"https://api.github.com/users/sijie/received_events","type":"User","site_admin":false},"committer":{"login":"sijie","id":1217863,"node_id":"MDQ6VXNlcjEyMTc4NjM=","avatar_url":"https://avatars.githubusercontent.com/u/1217863?v=4","gravatar_id":"","url":"https://api.github.com/users/sijie","html_url":"https://github.com/sijie","followers_url":"https://api.github.com/users/sijie/followers","following_url":"https://api.github.com/users/sijie/following{/other_user}","gists_url":"https://api.github.com/users/sijie/gists{/gist_id}","starred_url":"https://api.github.com/users/sijie/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sijie/subscriptions","organizations_url":"https://api.github.com/users/sijie/orgs","repos_url":"https://api.github.com/users/sijie/repos","events_url":"https://api.github.com/users/sijie/events{/privacy}","received_events_url":"https://api.github.com/users/sijie/received_events","type":"User","site_admin":false},"parents":[{"sha":"24dc8ac30c97620cf4dbef48deff2bb957932eb8","url":"https://api.github.com/repos/apache/bookkeeper/commits/24dc8ac30c97620cf4dbef48deff2bb957932eb8","html_url":"https://github.com/apache/bookkeeper/commit/24dc8ac30c97620cf4dbef48deff2bb957932eb8"}],"stats":{"total":521,"additions":391,"deletions":130},"files":[{"sha":"aff9a7ba6def2b8995cd8300ed9190ee74daeb3e","filename":"CHANGES.txt","status":"modified","additions":2,"deletions":0,"changes":2,"blob_url":"https://github.com/apache/bookkeeper/blob/9602c32f16184e10873d53c269e270e3cd0ebeed/CHANGES.txt","raw_url":"https://github.com/apache/bookkeeper/raw/9602c32f16184e10873d53c269e270e3cd0ebeed/CHANGES.txt","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/CHANGES.txt?ref=9602c32f16184e10873d53c269e270e3cd0ebeed","patch":"@@ -176,6 +176,8 @@ Trunk (unreleased changes)\n \n       BOOKKEEPER-657: Journal Improvement (Robin Dhamankar via sijie)\n \n+      BOOKKEEPER-664: Compaction increases latency on journal writes (ivank via sijie)\n+\n     NEW FEATURE:\n \n       BOOKKEEPER-562: Ability to tell if a ledger is closed or not (fpj)"},{"sha":"fd3ebd66a520736df1b4540f9ccd7c0c222618a6","filename":"bookkeeper-server/conf/bk_server.conf","status":"modified","additions":13,"deletions":0,"changes":13,"blob_url":"https://github.com/apache/bookkeeper/blob/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/conf/bk_server.conf","raw_url":"https://github.com/apache/bookkeeper/raw/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/conf/bk_server.conf","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/conf/bk_server.conf?ref=9602c32f16184e10873d53c269e270e3cd0ebeed","patch":"@@ -91,6 +91,19 @@ ledgerDirectories=/tmp/bk-data\n # If it is set to less than zero, the major compaction is disabled. \n # majorCompactionInterval=86400 \n \n+# Set the maximum number of entries which can be compacted without flushing.\n+# When compacting, the entries are written to the entrylog and the new offsets\n+# are cached in memory. Once the entrylog is flushed the index is updated with\n+# the new offsets. This parameter controls the number of entries added to the\n+# entrylog before a flush is forced. A higher value for this parameter means\n+# more memory will be used for offsets. Each offset consists of 3 longs.\n+# This parameter should _not_ be modified unless you know what you're doing.\n+# The default is 100,000.\n+#compactionMaxOutstandingRequests=100000\n+\n+# Set the rate at which compaction will readd entries. The unit is adds per second.\n+#compactionRate=1000\n+\n # Max file size of journal file, in mega bytes\n # A new journal file will be created when the old one reaches the file size limitation\n #"},{"sha":"c4238179843f0ff8636e2cf14b61527bfc0469f3","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","status":"modified","additions":1,"deletions":34,"changes":35,"blob_url":"https://github.com/apache/bookkeeper/blob/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","raw_url":"https://github.com/apache/bookkeeper/raw/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java?ref=9602c32f16184e10873d53c269e270e3cd0ebeed","patch":"@@ -41,7 +41,6 @@\n import java.util.concurrent.TimeoutException;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n-import org.apache.bookkeeper.bookie.GarbageCollectorThread.SafeEntryAdder;\n import org.apache.bookkeeper.bookie.Journal.JournalScanner;\n import org.apache.bookkeeper.bookie.LedgerDirsManager.LedgerDirsListener;\n import org.apache.bookkeeper.bookie.LedgerDirsManager.NoWritableLedgerDirException;\n@@ -420,8 +419,7 @@ public Bookie(ServerConfiguration conf)\n         // instantiate the journal\n         journal = new Journal(conf, ledgerDirsManager);\n         ledgerStorage = new InterleavedLedgerStorage(conf, ledgerManager,\n-                                                     ledgerDirsManager, journal,\n-                                                     new BookieSafeEntryAdder());\n+                                                     ledgerDirsManager, journal);\n         syncThread = new SyncThread(conf, getLedgerDirsListener(),\n                                     ledgerStorage, journal);\n \n@@ -1087,37 +1085,6 @@ private static boolean cleanDir(File dir) {\n         return true;\n     }\n \n-    private class BookieSafeEntryAdder implements SafeEntryAdder {\n-        @Override\n-        public void safeAddEntry(final long ledgerId, final ByteBuffer buffer,\n-                                 final GenericCallback<Void> cb) {\n-            journal.logAddEntry(buffer, new WriteCallback() {\n-                    @Override\n-                    public void writeComplete(int rc, long ledgerId2, long entryId,\n-                                              InetSocketAddress addr, Object ctx) {\n-                        if (rc != BookieException.Code.OK) {\n-                            LOG.error(\"Error rewriting to journal (ledger {}, entry {})\", ledgerId2, entryId);\n-                            cb.operationComplete(rc, null);\n-                            return;\n-                        }\n-                        try {\n-                            addEntryByLedgerId(ledgerId, buffer);\n-                            cb.operationComplete(rc, null);\n-                        } catch (IOException ioe) {\n-                            LOG.error(\"Error adding to ledger storage (ledger \" + ledgerId2\n-                                      + \", entry \" + entryId + \")\", ioe);\n-                            // couldn't add to ledger storage\n-                            cb.operationComplete(BookieException.Code.IllegalOpException, null);\n-                        } catch (BookieException bke) {\n-                            LOG.error(\"Bookie error adding to ledger storage (ledger \" + ledgerId2\n-                                      + \", entry \" + entryId + \")\", bke);\n-                            // couldn't add to ledger storage\n-                            cb.operationComplete(bke.getCode(), null);\n-                        }\n-                    }\n-                }, null);\n-        }\n-    }\n     /**\n      * @param args\n      * @throws IOException"},{"sha":"9b6ab1011abeace284e7c697068bb5b2139610d7","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java","status":"modified","additions":18,"deletions":4,"changes":22,"blob_url":"https://github.com/apache/bookkeeper/blob/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java","raw_url":"https://github.com/apache/bookkeeper/raw/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java?ref=9602c32f16184e10873d53c269e270e3cd0ebeed","patch":"@@ -43,6 +43,7 @@\n import java.util.Map.Entry;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.CopyOnWriteArrayList;\n \n import org.apache.bookkeeper.bookie.LedgerDirsManager.LedgerDirsListener;\n import org.apache.bookkeeper.conf.ServerConfiguration;\n@@ -73,7 +74,8 @@\n     final long logSizeLimit;\n     private List<BufferedChannel> logChannelsToFlush;\n     private volatile BufferedChannel logChannel;\n-    private final EntryLogListener listener;\n+    private final CopyOnWriteArrayList<EntryLogListener> listeners\n+        = new CopyOnWriteArrayList<EntryLogListener>();\n \n     /**\n      * The 1K block at the head of the entry logger file\n@@ -136,7 +138,9 @@ public EntryLogger(ServerConfiguration conf,\n             LedgerDirsManager ledgerDirsManager, EntryLogListener listener)\n                     throws IOException {\n         this.ledgerDirsManager = ledgerDirsManager;\n-        this.listener = listener;\n+        if (listener != null) {\n+            addListener(listener);\n+        }\n         // log size limit\n         this.logSizeLimit = conf.getEntryLogSizeLimit();\n \n@@ -163,6 +167,12 @@ public EntryLogger(ServerConfiguration conf,\n         initialize();\n     }\n \n+    void addListener(EntryLogListener listener) {\n+        if (null != listener) {\n+            listeners.add(listener);\n+        }\n+    }\n+\n     /**\n      * Maps entry log files to open channels.\n      */\n@@ -236,7 +246,7 @@ void createNewLog() throws IOException {\n             // so the readers could access the data from filesystem.\n             logChannel.flush(false);\n             logChannelsToFlush.add(logChannel);\n-            if (null != listener) {\n+            for (EntryLogListener listener : listeners) {\n                 listener.onRotateEntryLog();\n             }\n         }\n@@ -432,12 +442,16 @@ synchronized long addEntry(long ledger, ByteBuffer entry, boolean rollLog) throw\n         return (logId << 32L) | pos;\n     }\n \n+    static long logIdForOffset(long offset) {\n+        return offset >> 32L;\n+    }\n+\n     synchronized boolean reachEntryLogLimit(long size) {\n         return logChannel.position() + size > logSizeLimit;\n     }\n \n     byte[] readEntry(long ledgerId, long entryId, long location) throws IOException, Bookie.NoEntryException {\n-        long entryLogId = location >> 32L;\n+        long entryLogId = logIdForOffset(location);\n         long pos = location & 0xffffffffL;\n         ByteBuffer sizeBuff = ByteBuffer.allocate(4);\n         pos -= 4; // we want to get the ledgerId and length to check"},{"sha":"6e04469303cd13c31006af9d618b475dd1d6076f","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","status":"modified","additions":108,"deletions":78,"changes":186,"blob_url":"https://github.com/apache/bookkeeper/blob/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","raw_url":"https://github.com/apache/bookkeeper/raw/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java?ref=9602c32f16184e10873d53c269e270e3cd0ebeed","patch":"@@ -30,9 +30,9 @@\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.atomic.AtomicInteger;\n \n-import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;\n+import com.google.common.util.concurrent.RateLimiter;\n+\n import org.apache.bookkeeper.bookie.EntryLogger.EntryLogScanner;\n import org.apache.bookkeeper.bookie.GarbageCollector.GarbageCleaner;\n import org.apache.bookkeeper.conf.ServerConfiguration;\n@@ -48,7 +48,6 @@\n  */\n public class GarbageCollectorThread extends Thread {\n     private static final Logger LOG = LoggerFactory.getLogger(GarbageCollectorThread.class);\n-    private static final int COMPACTION_MAX_OUTSTANDING_REQUESTS = 1000;\n     private static final int SECOND = 1000;\n \n     // Maps entry log files to the set of ledgers that comprise the file and the size usage per ledger\n@@ -69,9 +68,12 @@\n     long lastMinorCompactionTime;\n     long lastMajorCompactionTime;\n \n+    final int maxOutstandingRequests;\n+    final int compactionRate;\n+    final CompactionScannerFactory scannerFactory;\n+\n     // Entry Logger Handle\n     final EntryLogger entryLogger;\n-    final SafeEntryAdder safeEntryAdder;\n \n     // Ledger Cache Handle\n     final LedgerCache ledgerCache;\n@@ -89,77 +91,90 @@\n     final GarbageCollector garbageCollector;\n     final GarbageCleaner garbageCleaner;\n \n+    private static class Offset {\n+        final long ledger;\n+        final long entry;\n+        final long offset;\n \n-    /**\n-     * Interface for adding entries. When the write callback is triggered, the\n-     * entry must be guaranteed to be presisted.\n-     */\n-    interface SafeEntryAdder {\n-        public void safeAddEntry(long ledgerId, ByteBuffer buffer, GenericCallback<Void> cb);\n+        Offset(long ledger, long entry, long offset) {\n+            this.ledger = ledger;\n+            this.entry = entry;\n+            this.offset = offset;\n+        }\n     }\n \n     /**\n      * A scanner wrapper to check whether a ledger is alive in an entry log file\n      */\n-    class CompactionScanner implements EntryLogScanner {\n-        EntryLogMetadata meta;\n-        Object completionLock = new Object();\n-        AtomicInteger outstandingRequests = new AtomicInteger(0);\n-        AtomicBoolean allSuccessful = new AtomicBoolean(true);\n+    class CompactionScannerFactory implements EntryLogger.EntryLogListener {\n+        List<Offset> offsets = new ArrayList<Offset>();\n+\n+        EntryLogScanner newScanner(final EntryLogMetadata meta) {\n+            final RateLimiter rateLimiter = RateLimiter.create(compactionRate);\n+            return new EntryLogScanner() {\n+                @Override\n+                public boolean accept(long ledgerId) {\n+                    return meta.containsLedger(ledgerId);\n+                }\n \n-        public CompactionScanner(EntryLogMetadata meta) {\n-            this.meta = meta;\n-        }\n+                @Override\n+                public void process(final long ledgerId, long offset, ByteBuffer entry)\n+                        throws IOException {\n+                    rateLimiter.acquire();\n+                    synchronized (CompactionScannerFactory.this) {\n+                        if (offsets.size() > maxOutstandingRequests) {\n+                            waitEntrylogFlushed();\n+                        }\n+                        entry.getLong(); // discard ledger id, we already have it\n+                        long entryId = entry.getLong();\n+                        entry.rewind();\n \n-        @Override\n-        public boolean accept(long ledgerId) {\n-            return meta.containsLedger(ledgerId);\n+                        long newoffset = entryLogger.addEntry(ledgerId, entry);\n+                        offsets.add(new Offset(ledgerId, entryId, newoffset));\n+                    }\n+                }\n+            };\n         }\n \n+        Object flushLock = new Object();\n+\n         @Override\n-        public void process(final long ledgerId, long offset, ByteBuffer entry)\n-            throws IOException {\n-            if (!allSuccessful.get()) {\n-                return;\n+        public void onRotateEntryLog() {\n+            synchronized (flushLock) {\n+                flushLock.notifyAll();\n             }\n+        }\n \n-            outstandingRequests.incrementAndGet();\n-            synchronized (completionLock) {\n-                while (outstandingRequests.get() >= COMPACTION_MAX_OUTSTANDING_REQUESTS) {\n-                    try {\n-                        completionLock.wait();\n-                    } catch (InterruptedException ie) {\n-                        LOG.error(\"Interrupted while waiting to re-add entry\", ie);\n-                        Thread.currentThread().interrupt();\n-                        throw new IOException(\"Interrupted while waiting to re-add entry\", ie);\n+        synchronized private void waitEntrylogFlushed() throws IOException {\n+            try {\n+                synchronized (flushLock) {\n+                    Offset lastOffset = offsets.get(offsets.size()-1);\n+                    long lastOffsetLogId = EntryLogger.logIdForOffset(lastOffset.offset);\n+                    while (lastOffsetLogId < entryLogger.getLeastUnflushedLogId() && running) {\n+                        flushLock.wait(1000);\n+\n+                        lastOffset = offsets.get(offsets.size()-1);\n+                        lastOffsetLogId = EntryLogger.logIdForOffset(lastOffset.offset);\n+                    }\n+                    if (lastOffsetLogId >= entryLogger.getLeastUnflushedLogId() && !running) {\n+                        throw new IOException(\"Shutdown before flushed\");\n                     }\n                 }\n+            } catch (InterruptedException ie) {\n+                Thread.currentThread().interrupt();\n+                throw new IOException(\"Interrupted waiting for flush\", ie);\n             }\n-            safeEntryAdder.safeAddEntry(ledgerId, entry, new GenericCallback<Void>() {\n-                    @Override\n-                    public void operationComplete(int rc, Void result) {\n-                        if (rc != BookieException.Code.OK) {\n-                            LOG.error(\"Error {} re-adding entry for ledger {})\",\n-                                      rc, ledgerId);\n-                            allSuccessful.set(false);\n-                        }\n-                        synchronized(completionLock) {\n-                            outstandingRequests.decrementAndGet();\n-                            completionLock.notifyAll();\n-                        }\n-                    }\n-                });\n-        }\n \n-        void awaitComplete() throws InterruptedException, IOException {\n-            synchronized(completionLock) {\n-                while (outstandingRequests.get() > 0) {\n-                    completionLock.wait();\n-                }\n-                if (allSuccessful.get() == false) {\n-                    throw new IOException(\"Couldn't re-add all entries\");\n-                }\n+            for (Offset o : offsets) {\n+                ledgerCache.putEntryOffset(o.ledger, o.entry, o.offset);\n             }\n+            offsets.clear();\n+        }\n+\n+        synchronized void flush() throws IOException {\n+            waitEntrylogFlushed();\n+\n+            ledgerCache.flushLedger(true);\n         }\n     }\n \n@@ -175,17 +190,19 @@ public GarbageCollectorThread(ServerConfiguration conf,\n                                   final LedgerCache ledgerCache,\n                                   EntryLogger entryLogger,\n                                   SnapshotMap<Long, Boolean> activeLedgers,\n-                                  SafeEntryAdder safeEntryAdder,\n                                   LedgerManager ledgerManager)\n         throws IOException {\n         super(\"GarbageCollectorThread\");\n \n         this.ledgerCache = ledgerCache;\n         this.entryLogger = entryLogger;\n         this.activeLedgers = activeLedgers;\n-        this.safeEntryAdder = safeEntryAdder;\n \n         this.gcWaitTime = conf.getGcWaitTime();\n+        this.maxOutstandingRequests = conf.getCompactionMaxOutstandingRequests();\n+        this.compactionRate = conf.getCompactionRate();\n+        this.scannerFactory = new CompactionScannerFactory();\n+        entryLogger.addListener(this.scannerFactory);\n \n         this.garbageCleaner = new GarbageCollector.GarbageCleaner() {\n             @Override\n@@ -354,16 +371,42 @@ public int compare(EntryLogMetadata m1, EntryLogMetadata m2) {\n         List<EntryLogMetadata> logsToCompact = new ArrayList<EntryLogMetadata>();\n         logsToCompact.addAll(entryLogMetaMap.values());\n         Collections.sort(logsToCompact, sizeComparator);\n+        List<Long> toRemove = new ArrayList<Long>();\n+\n         for (EntryLogMetadata meta : logsToCompact) {\n             if (meta.getUsage() >= threshold) {\n                 break;\n             }\n             LOG.debug(\"Compacting entry log {} below threshold {}.\", meta.entryLogId, threshold);\n-            compactEntryLog(meta.entryLogId);\n+            try {\n+                compactEntryLog(scannerFactory, meta);\n+                toRemove.add(meta.entryLogId);\n+            } catch (LedgerDirsManager.NoWritableLedgerDirException nwlde) {\n+                LOG.warn(\"No writable ledger directory available, aborting compaction\", nwlde);\n+                break;\n+            } catch (IOException ioe) {\n+                // if compact entry log throws IOException, we don't want to remove that\n+                // entry log. however, if some entries from that log have been readded\n+                // to the entry log, and the offset updated, it's ok to flush that\n+                LOG.error(\"Error compacting entry log. Log won't be deleted\", ioe);\n+            }\n+\n             if (!running) { // if gc thread is not running, stop compaction\n                 return;\n             }\n         }\n+        try {\n+            // compaction finished, flush any outstanding offsets\n+            scannerFactory.flush();\n+        } catch (IOException ioe) {\n+            LOG.error(\"Cannot flush compacted entries, skip removal\", ioe);\n+            return;\n+        }\n+\n+        // offsets have been flushed, its now safe to remove the old entrylogs\n+        for (Long l : toRemove) {\n+            removeEntryLog(l);\n+        }\n     }\n \n     /**\n@@ -401,13 +444,8 @@ private void removeEntryLog(long entryLogId) {\n      * @param entryLogId\n      *          Entry Log File Id\n      */\n-    protected void compactEntryLog(long entryLogId) {\n-        EntryLogMetadata entryLogMeta = entryLogMetaMap.get(entryLogId);\n-        if (null == entryLogMeta) {\n-            LOG.warn(\"Can't get entry log meta when compacting entry log \" + entryLogId + \".\");\n-            return;\n-        }\n-\n+    protected void compactEntryLog(CompactionScannerFactory scannerFactory,\n+                                   EntryLogMetadata entryLogMeta) throws IOException {\n         // Similar with Sync Thread\n         // try to mark compacting flag to make sure it would not be interrupted\n         // by shutdown during compaction. otherwise it will receive\n@@ -419,19 +457,11 @@ protected void compactEntryLog(long entryLogId) {\n             return;\n         }\n \n-        LOG.info(\"Compacting entry log : \" + entryLogId);\n+        LOG.info(\"Compacting entry log : {}\", entryLogMeta.entryLogId);\n \n         try {\n-            CompactionScanner scanner = new CompactionScanner(entryLogMeta);\n-            entryLogger.scanEntryLog(entryLogId, scanner);\n-            scanner.awaitComplete();\n-            // after moving entries to new entry log, remove this old one\n-            removeEntryLog(entryLogId);\n-        } catch (IOException e) {\n-            LOG.info(\"Premature exception when compacting \" + entryLogId, e);\n-        } catch (InterruptedException ie) {\n-            Thread.currentThread().interrupt();\n-            LOG.warn(\"Interrupted while compacting\", ie);\n+            entryLogger.scanEntryLog(entryLogMeta.entryLogId,\n+                                     scannerFactory.newScanner(entryLogMeta));\n         } finally {\n             // clear compacting flag\n             compacting.set(false);"},{"sha":"4676a0eb2fcb0f1b22bdd3ae5bdfa0a64cc12c65","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/InterleavedLedgerStorage.java","status":"modified","additions":4,"deletions":3,"changes":7,"blob_url":"https://github.com/apache/bookkeeper/blob/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/InterleavedLedgerStorage.java","raw_url":"https://github.com/apache/bookkeeper/raw/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/InterleavedLedgerStorage.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/InterleavedLedgerStorage.java?ref=9602c32f16184e10873d53c269e270e3cd0ebeed","patch":"@@ -80,14 +80,14 @@ synchronized Checkpoint getLastCheckpoint() {\n     private volatile boolean somethingWritten = false;\n \n     InterleavedLedgerStorage(ServerConfiguration conf, LedgerManager ledgerManager,\n-            LedgerDirsManager ledgerDirsManager, CheckpointSource checkpointSource,\n-            GarbageCollectorThread.SafeEntryAdder safeEntryAdder) throws IOException {\n+                             LedgerDirsManager ledgerDirsManager, CheckpointSource checkpointSource)\n+            throws IOException {\n         activeLedgers = new SnapshotMap<Long, Boolean>();\n         this.checkpointSource = checkpointSource;\n         entryLogger = new EntryLogger(conf, ledgerDirsManager, this);\n         ledgerCache = new LedgerCacheImpl(conf, activeLedgers, ledgerDirsManager);\n         gcThread = new GarbageCollectorThread(conf, ledgerCache, entryLogger,\n-                activeLedgers, safeEntryAdder, ledgerManager);\n+                activeLedgers, ledgerManager);\n     }\n \n     @Override\n@@ -207,6 +207,7 @@ public Checkpoint checkpoint(Checkpoint checkpoint) throws IOException {\n         // current entry logger file isn't flushed yet.\n         flushOrCheckpoint(true);\n         // after the ledger storage finished checkpointing, try to clear the done checkpoint\n+\n         checkpointHolder.clearLastCheckpoint(lastCheckpoint);\n         return lastCheckpoint;\n     }"},{"sha":"7298eb8f14949e135bf43ba8b243d7bb90e222fd","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java","status":"modified","additions":76,"deletions":10,"changes":86,"blob_url":"https://github.com/apache/bookkeeper/blob/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java","raw_url":"https://github.com/apache/bookkeeper/raw/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java?ref=9602c32f16184e10873d53c269e270e3cd0ebeed","patch":"@@ -36,6 +36,9 @@\n     protected final static String MINOR_COMPACTION_THRESHOLD = \"minorCompactionThreshold\";\n     protected final static String MAJOR_COMPACTION_INTERVAL = \"majorCompactionInterval\";\n     protected final static String MAJOR_COMPACTION_THRESHOLD = \"majorCompactionThreshold\";\n+    protected final static String COMPACTION_MAX_OUTSTANDING_REQUESTS\n+        = \"compactionMaxOutstandingRequests\";\n+    protected final static String COMPACTION_RATE = \"compactionRate\";\n \n     // Gc Parameters\n     protected final static String GC_WAIT_TIME = \"gcWaitTime\";\n@@ -788,16 +791,6 @@ public boolean getJournalFlushWhenQueueEmpty() {\n         return getBoolean(JOURNAL_FLUSH_WHEN_QUEUE_EMPTY, false);\n     }\n \n-    /**\n-     * Should we remove pages from page cache after force write\n-     *\n-     * @return remove pages from cache\n-     */\n-    @Beta\n-    public boolean getJournalRemovePagesFromCache() {\n-        return getBoolean(JOURNAL_REMOVE_FROM_PAGE_CACHE, false);\n-    }\n-\n     /**\n      * Set whether the bookie is able to go into read-only mode.\n      * If this is set to false, the bookie will shutdown on encountering\n@@ -908,4 +901,77 @@ public boolean isAutoRecoveryDaemonEnabled() {\n         return getBoolean(AUTO_RECOVERY_DAEMON_ENABLED, false);\n     }\n \n+    /**\n+     * Get the maximum number of entries which can be compacted without flushing.\n+     * Default is 100,000.\n+     *\n+     * @return the maximum number of unflushed entries\n+     */\n+    public int getCompactionMaxOutstandingRequests() {\n+        return getInt(COMPACTION_MAX_OUTSTANDING_REQUESTS, 100000);\n+    }\n+\n+    /**\n+     * Set the maximum number of entries which can be compacted without flushing.\n+     *\n+     * When compacting, the entries are written to the entrylog and the new offsets\n+     * are cached in memory. Once the entrylog is flushed the index is updated with\n+     * the new offsets. This parameter controls the number of entries added to the\n+     * entrylog before a flush is forced. A higher value for this parameter means\n+     * more memory will be used for offsets. Each offset consists of 3 longs.\n+     *\n+     * This parameter should _not_ be modified unless you know what you're doing.\n+     * The default is 100,000.\n+     *\n+     * @param maxOutstandingRequests number of entries to compact before flushing\n+     *\n+     * @return ServerConfiguration\n+     */\n+    public ServerConfiguration setCompactionMaxOutstandingRequests(int maxOutstandingRequests) {\n+        setProperty(COMPACTION_MAX_OUTSTANDING_REQUESTS, maxOutstandingRequests);\n+        return this;\n+    }\n+\n+    /**\n+     * Get the rate of compaction adds. Default is 1,000.\n+     *\n+     * @return rate of compaction (adds per second)\n+     */\n+    public int getCompactionRate() {\n+        return getInt(COMPACTION_RATE, 1000);\n+    }\n+\n+    /**\n+     * Set the rate of compaction adds.\n+     *\n+     * @param rate rate of compaction adds (adds per second)\n+     *\n+     * @return ServerConfiguration\n+     */\n+    public ServerConfiguration setCompactionRate(int rate) {\n+        setProperty(COMPACTION_RATE, rate);\n+        return this;\n+    }\n+\n+    /**\n+     * Should we remove pages from page cache after force write\n+     *\n+     * @return remove pages from cache\n+     */\n+    @Beta\n+    public boolean getJournalRemovePagesFromCache() {\n+        return getBoolean(JOURNAL_REMOVE_FROM_PAGE_CACHE, false);\n+    }\n+\n+    /**\n+     * Sets that whether should we remove pages from page cache after force write.\n+     *\n+     * @param enabled\n+     *            - true if we need to remove pages from page cache. otherwise, false\n+     * @return ServerConfiguration\n+     */\n+    public ServerConfiguration setJournalRemovePagesFromCache(boolean enabled) {\n+        setProperty(JOURNAL_REMOVE_FROM_PAGE_CACHE, enabled);\n+        return this;\n+    }\n }"},{"sha":"c9a704f3f5edf5a2eb8394617d20ab143a10af40","filename":"bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","status":"modified","additions":169,"deletions":1,"changes":170,"blob_url":"https://github.com/apache/bookkeeper/blob/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","raw_url":"https://github.com/apache/bookkeeper/raw/9602c32f16184e10873d53c269e270e3cd0ebeed/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java?ref=9602c32f16184e10873d53c269e270e3cd0ebeed","patch":"@@ -21,15 +21,30 @@\n  *\n  */\n import java.io.File;\n-import java.util.Arrays;\n+import java.io.IOException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.Collections;\n import java.util.Enumeration;\n \n+import org.apache.bookkeeper.meta.LedgerManager;\n+import org.apache.bookkeeper.conf.ServerConfiguration;\n import org.apache.bookkeeper.client.LedgerEntry;\n import org.apache.bookkeeper.client.LedgerHandle;\n import org.apache.bookkeeper.client.BookKeeper.DigestType;\n import org.apache.bookkeeper.test.BookKeeperClusterTestCase;\n import org.apache.bookkeeper.util.TestUtils;\n \n+import org.apache.zookeeper.AsyncCallback;\n+import org.apache.bookkeeper.client.LedgerMetadata;\n+import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;\n+import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;\n+import org.apache.bookkeeper.versioning.Version;\n+\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -294,4 +309,157 @@ public void testCompactionSmallEntryLogs() throws Exception {\n         // since those entries has been compacted to new entry log\n         verifyLedger(lhs[0].getId(), 0, lhs[0].getLastAddConfirmed());\n     }\n+\n+    /**\n+     * Test that compaction doesnt add to index without having persisted\n+     * entrylog first. This is needed because compaction doesn't go through the journal.\n+     * {@see https://issues.apache.org/jira/browse/BOOKKEEPER-530}\n+     * {@see https://issues.apache.org/jira/browse/BOOKKEEPER-664}\n+     */\n+    @Test(timeout=60000)\n+    public void testCompactionSafety() throws Exception {\n+        tearDown(); // I dont want the test infrastructure\n+        ServerConfiguration conf = new ServerConfiguration();\n+        final Set<Long> ledgers = Collections.newSetFromMap(new ConcurrentHashMap<Long, Boolean>());\n+        LedgerManager manager = new LedgerManager() {\n+                @Override\n+                public void createLedger(LedgerMetadata metadata, GenericCallback<Long> cb) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void removeLedgerMetadata(long ledgerId, Version version,\n+                                                 GenericCallback<Void> vb) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void readLedgerMetadata(long ledgerId, GenericCallback<LedgerMetadata> readCb) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void writeLedgerMetadata(long ledgerId, LedgerMetadata metadata,\n+                        GenericCallback<Void> cb) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void asyncProcessLedgers(Processor<Long> processor,\n+                                                AsyncCallback.VoidCallback finalCb,\n+                        Object context, int successRc, int failureRc) {\n+                    unsupported();\n+                }\n+                @Override\n+                public void close() throws IOException {}\n+\n+                void unsupported() {\n+                    LOG.error(\"Unsupported operation called\", new Exception());\n+                    throw new RuntimeException(\"Unsupported op\");\n+                }\n+                @Override\n+                public LedgerRangeIterator getLedgerRanges() {\n+                    final AtomicBoolean hasnext = new AtomicBoolean(true);\n+                    return new LedgerManager.LedgerRangeIterator() {\n+                        @Override\n+                        public boolean hasNext() throws IOException {\n+                            return hasnext.get();\n+                        }\n+                        @Override\n+                        public LedgerManager.LedgerRange next() throws IOException {\n+                            hasnext.set(false);\n+                            return new LedgerManager.LedgerRange(ledgers);\n+                        }\n+                    };\n+                 }\n+            };\n+\n+        File tmpDir = File.createTempFile(\"bkTest\", \".dir\");\n+        tmpDir.delete();\n+        tmpDir.mkdir();\n+        File curDir = Bookie.getCurrentDirectory(tmpDir);\n+        Bookie.checkDirectoryStructure(curDir);\n+        conf.setLedgerDirNames(new String[] {tmpDir.toString()});\n+\n+        conf.setEntryLogSizeLimit(EntryLogger.LOGFILE_HEADER_SIZE + 3 * (4+ENTRY_SIZE));\n+        conf.setGcWaitTime(100);\n+        conf.setMinorCompactionThreshold(0.7f);\n+        conf.setMajorCompactionThreshold(0.0f);\n+        conf.setMinorCompactionInterval(1);\n+        conf.setMajorCompactionInterval(10);\n+        conf.setPageLimit(1);\n+\n+        CheckpointSource checkpointSource = new CheckpointSource() {\n+                AtomicInteger idGen = new AtomicInteger(0);\n+                class MyCheckpoint implements CheckpointSource.Checkpoint {\n+                    int id = idGen.incrementAndGet();\n+                    @Override\n+                    public int compareTo(CheckpointSource.Checkpoint o) {\n+                        if (o == CheckpointSource.Checkpoint.MAX) {\n+                            return -1;\n+                        } else if (o == CheckpointSource.Checkpoint.MIN) {\n+                            return 1;\n+                        }\n+                        return id - ((MyCheckpoint)o).id;\n+                    }\n+                }\n+\n+                @Override\n+                public CheckpointSource.Checkpoint newCheckpoint() {\n+                    return new MyCheckpoint();\n+                }\n+\n+                public void checkpointComplete(CheckpointSource.Checkpoint checkpoint, boolean compact)\n+                        throws IOException {\n+                }\n+            };\n+        final byte[] KEY = \"foobar\".getBytes();\n+        File log0 = new File(curDir, \"0.log\");\n+        LedgerDirsManager dirs = new LedgerDirsManager(conf);\n+        assertFalse(\"Log shouldnt exist\", log0.exists());\n+        InterleavedLedgerStorage storage = new InterleavedLedgerStorage(conf, manager,\n+                                                                        dirs, checkpointSource);\n+        ledgers.add(1l);\n+        ledgers.add(2l);\n+        ledgers.add(3l);\n+        storage.setMasterKey(1, KEY);\n+        storage.setMasterKey(2, KEY);\n+        storage.setMasterKey(3, KEY);\n+        storage.addEntry(genEntry(1, 1, ENTRY_SIZE));\n+        storage.addEntry(genEntry(2, 1, ENTRY_SIZE));\n+        storage.addEntry(genEntry(2, 2, ENTRY_SIZE));\n+        storage.addEntry(genEntry(3, 2, ENTRY_SIZE));\n+        storage.flush();\n+        storage.shutdown();\n+\n+        assertTrue(\"Log should exist\", log0.exists());\n+        ledgers.remove(2l);\n+        ledgers.remove(3l);\n+\n+        storage = new InterleavedLedgerStorage(conf, manager, dirs, checkpointSource);\n+        storage.start();\n+        for (int i = 0; i < 10; i++) {\n+            if (!log0.exists()) {\n+                break;\n+            }\n+            Thread.sleep(1000);\n+            storage.entryLogger.flush(); // simulate sync thread\n+        }\n+        assertFalse(\"Log shouldnt exist\", log0.exists());\n+\n+        ledgers.add(4l);\n+        storage.setMasterKey(4, KEY);\n+        storage.addEntry(genEntry(4, 1, ENTRY_SIZE)); // force ledger 1 page to flush\n+\n+        storage = new InterleavedLedgerStorage(conf, manager, dirs, checkpointSource);\n+        storage.getEntry(1, 1); // entry should exist\n+    }\n+\n+    private ByteBuffer genEntry(long ledger, long entry, int size) {\n+        byte[] data = new byte[size];\n+        ByteBuffer bb = ByteBuffer.wrap(new byte[size]);\n+        bb.putLong(ledger);\n+        bb.putLong(entry);\n+        while (bb.hasRemaining()) {\n+            bb.put((byte)0xFF);\n+        }\n+        bb.flip();\n+        return bb;\n+    }\n }"}]}

