{"sha":"b83d00086a263af5b51e3e061170934c53447976","node_id":"MDY6Q29tbWl0MTU3NTk1NjpiODNkMDAwODZhMjYzYWY1YjUxZTNlMDYxMTcwOTM0YzUzNDQ3OTc2","commit":{"author":{"name":"Flavio Paiva Junqueira","email":"fpj@apache.org","date":"2011-11-17T08:27:30Z"},"committer":{"name":"Flavio Paiva Junqueira","email":"fpj@apache.org","date":"2011-11-17T08:27:30Z"},"message":"BOOKKEEPER-109: Add documentation to describe how bookies flushes data (Sijie Guo via fpj)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/zookeeper/bookkeeper/trunk@1203103 13f79535-47bb-0310-9956-ffa450edef68","tree":{"sha":"9a39de5c227cac19f2df54778ae24b65c329ce76","url":"https://api.github.com/repos/apache/bookkeeper/git/trees/9a39de5c227cac19f2df54778ae24b65c329ce76"},"url":"https://api.github.com/repos/apache/bookkeeper/git/commits/b83d00086a263af5b51e3e061170934c53447976","comment_count":0,"verification":{"verified":false,"reason":"unsigned","signature":null,"payload":null}},"url":"https://api.github.com/repos/apache/bookkeeper/commits/b83d00086a263af5b51e3e061170934c53447976","html_url":"https://github.com/apache/bookkeeper/commit/b83d00086a263af5b51e3e061170934c53447976","comments_url":"https://api.github.com/repos/apache/bookkeeper/commits/b83d00086a263af5b51e3e061170934c53447976/comments","author":{"login":"fpj","id":572920,"node_id":"MDQ6VXNlcjU3MjkyMA==","avatar_url":"https://avatars.githubusercontent.com/u/572920?v=4","gravatar_id":"","url":"https://api.github.com/users/fpj","html_url":"https://github.com/fpj","followers_url":"https://api.github.com/users/fpj/followers","following_url":"https://api.github.com/users/fpj/following{/other_user}","gists_url":"https://api.github.com/users/fpj/gists{/gist_id}","starred_url":"https://api.github.com/users/fpj/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fpj/subscriptions","organizations_url":"https://api.github.com/users/fpj/orgs","repos_url":"https://api.github.com/users/fpj/repos","events_url":"https://api.github.com/users/fpj/events{/privacy}","received_events_url":"https://api.github.com/users/fpj/received_events","type":"User","site_admin":false},"committer":{"login":"fpj","id":572920,"node_id":"MDQ6VXNlcjU3MjkyMA==","avatar_url":"https://avatars.githubusercontent.com/u/572920?v=4","gravatar_id":"","url":"https://api.github.com/users/fpj","html_url":"https://github.com/fpj","followers_url":"https://api.github.com/users/fpj/followers","following_url":"https://api.github.com/users/fpj/following{/other_user}","gists_url":"https://api.github.com/users/fpj/gists{/gist_id}","starred_url":"https://api.github.com/users/fpj/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fpj/subscriptions","organizations_url":"https://api.github.com/users/fpj/orgs","repos_url":"https://api.github.com/users/fpj/repos","events_url":"https://api.github.com/users/fpj/events{/privacy}","received_events_url":"https://api.github.com/users/fpj/received_events","type":"User","site_admin":false},"parents":[{"sha":"4be547c861f024e48a7b0074effb8c26ff0a97fc","url":"https://api.github.com/repos/apache/bookkeeper/commits/4be547c861f024e48a7b0074effb8c26ff0a97fc","html_url":"https://github.com/apache/bookkeeper/commit/4be547c861f024e48a7b0074effb8c26ff0a97fc"}],"stats":{"total":59,"additions":54,"deletions":5},"files":[{"sha":"06459c1f7a51c118364543f08cf1608d44ed77bd","filename":"CHANGES.txt","status":"modified","additions":2,"deletions":0,"changes":2,"blob_url":"https://github.com/apache/bookkeeper/blob/b83d00086a263af5b51e3e061170934c53447976/CHANGES.txt","raw_url":"https://github.com/apache/bookkeeper/raw/b83d00086a263af5b51e3e061170934c53447976/CHANGES.txt","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/CHANGES.txt?ref=b83d00086a263af5b51e3e061170934c53447976","patch":"@@ -109,3 +109,5 @@ IMPROVEMENTS:\n  hedwig-client/\n \n   BOOKKEEPER-44: Reuse publish channel to default server to avoid too many connect requests to default server when lots of producers came in same time (Sijie Guo via breed)\n+\n+  BOOKKEEPER-109: Add documentation to describe how bookies flushes data (Sijie Guo via fpj)"},{"sha":"68afb7283be3d1465caa527a8c9113524da15437","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","status":"modified","additions":5,"deletions":5,"changes":10,"blob_url":"https://github.com/apache/bookkeeper/blob/b83d00086a263af5b51e3e061170934c53447976/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","raw_url":"https://github.com/apache/bookkeeper/raw/b83d00086a263af5b51e3e061170934c53447976/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java?ref=b83d00086a263af5b51e3e061170934c53447976","patch":"@@ -128,19 +128,19 @@ public long getEntry() {\n      * <p>\n      * Before flushing, SyncThread first records a log marker {journalId, journalPos} in memory,\n      * which indicates entries before this log marker would be persisted to ledger files.\n-     * Then sync thread begans flush ledger index pages to ledger index files, flush entry\n+     * Then sync thread begins flushing ledger index pages to ledger index files, flush entry\n      * logger to ensure all entries persisted to entry loggers for future reads.\n      * </p>\n      * <p>\n      * After all data has been persisted to ledger index files and entry loggers, it is safe\n      * to persist the log marker to disk. If bookie failed after persist log mark,\n-     * bookie is able to relay journal entries started from last log mark without lossing\n+     * bookie is able to relay journal entries started from last log mark without losing\n      * any entries.\n      * </p>\n      * <p>\n      * Those journal files whose id are less than the log id in last log mark, could be\n      * removed safely after persisting last log mark. We provide a setting to let user keeping\n-     * number of old journal files which may be used for munually recovery in critical disaster.\n+     * number of old journal files which may be used for manual recovery in critical disaster.\n      * </p>\n      */\n     class SyncThread extends Thread {\n@@ -190,7 +190,7 @@ public void run() {\n                 }\n                 lastLogMark.rollLog();\n \n-                // list the journals whose has been marked\n+                // list the journals that have been marked\n                 List<Long> logs = listJournalIds(journalDirectory, new JournalIdFilter() {\n                     @Override\n                     public boolean accept(long journalId) {\n@@ -769,7 +769,7 @@ public void run() {\n                             }\n                             toFlush.clear();\n \n-                            // check wether journal file is over file limit\n+                            // check whether journal file is over file limit\n                             if (bc.position() > MAX_JOURNAL_SIZE) {\n                                 logFile.close();\n                                 logFile = null;"},{"sha":"1dca6ae68b101186fd81d2e686941bdf8f5c9c54","filename":"doc/bookkeeperOverview.textile","status":"modified","additions":47,"deletions":0,"changes":47,"blob_url":"https://github.com/apache/bookkeeper/blob/b83d00086a263af5b51e3e061170934c53447976/doc/bookkeeperOverview.textile","raw_url":"https://github.com/apache/bookkeeper/raw/b83d00086a263af5b51e3e061170934c53447976/doc/bookkeeperOverview.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/bookkeeperOverview.textile?ref=b83d00086a263af5b51e3e061170934c53447976","patch":"@@ -116,3 +116,50 @@ p. The trick to making everything work is to have a correct idea of a last entry\n # Find the highest consecutively recorded entry, _LR_ ; \n # Make sure that all entries between _LC_ and _LR_ are on a quorum of bookies; \n \n+h1. Data Management in Bookie Server\n+\n+h2. Basic\n+\n+p. Bookie servers manage data in a log-structured way, which is implemented using three kind of files:\n+\n+* _Journal_ : A journal file contains the BookKeeper transaction logs. Before any update takes place, a Bookie server ensures that a transaction describing the update is written to non-volatile storage. A new journal file is created once the Bookie server starts or the older journal file reaches the journal file size threshold.\n+* _Entry Log_ : An entry log file manages the written entries received from BookKeeper clients. Entries from different ledgers are aggregated and written sequentially, while their offsets are kept as pointers in _LedgerCache_ for fast lookup. A new entry log file is created once the Bookie server starts or the older entry log file reaches the entry log size threshold. Old entry log files are removed by the _Garbage Collector Thread_ once they are not associated with any active ledger.\n+* _Index File_ : An index file is created for each ledger, which comprises a header and several fixed-length index pages, recording the offsets of data stored in entry log files. \n+\n+p. Since updating index files would introduce random disk I/O, for performance consideration, index files are updated lazily by a _Sync Thread_ running in the background. Before index pages are persisted to disk, they are gathered in _LedgerCache_ for lookup.\n+\n+* _LedgerCache_ : A memory pool caches ledger index pages, which more efficiently manage disk head scheduling.\n+\n+h2. Add Entry\n+\n+p. When a Bookie server receives entries from clients to be written, these entries will go through the following steps to be persisted to disk:\n+\n+# Append the entry in _Entry Log_, return its position { logId , offset } ;\n+# Update the index of this entry in _Ledger Cache_ ;\n+# Append a transaction of update of this entry in _Journal_ ;\n+# Respond to BookKeeper client ;\n+\n+* For performance reasons, _Entry Log_ buffers entries in memory and commit them in batches, while _Ledger Cache_ holds index pages in memory and flushes them lazily. We will discuss data flush and how to ensure data integrity in the following section 'Data Flush'.\n+\n+h2. Data Flush\n+\n+p. Ledger index pages are flushed to index files in the following two cases:\n+\n+# _LedgerCache_ memory reaches its limit. There is no more space available to hold newer index pages. Dirty index pages will be evicted from _LedgerCache_ and persisted to index files.\n+# A background thread _Sync Thread_ is responsible for flushing index pages from _LedgerCache_ to index files periodically.\n+\n+p. Besides flushing index pages, _Sync Thread_ is responsible for rolling journal files in case that journal files use too much disk space. \n+\n+p. The data flush flow in _Sync Thread_ is as follows:\n+\n+# Records a _LastLogMark_ in memory. The _LastLogMark_ contains two parts: first one is _txnLogId_ (file id of a journal) and the second one is _txnLogPos_ (offset in a journal). The _LastLogMark_ indicates that those entries before it have been persisted to both index and entry log files.\n+# Flushes dirty index pages from _LedgerCache_ to index file, and flushes entry log files to ensure all buffered entries in entry log files are persisted to disk.\n+#* Ideally, a Bookie server just needs to flush index pages and entry log files that contains entries before _LastLogMark_. There is no such information in _LedgerCache_ and _Entry Log_ mapping to journal files, though. Consequently, the thread flushes _LedgerCache_ and _Entry Log_ entirely here, and may flush entries after the _LastLogMark_. Flushing more is not a problem, though, just redundant.\n+# Persists _LastLogMark_ to disk, which means entries added before _LastLogMark_ whose entry data and index page were also persisted to disk. It is the time to safely remove journal files created earlier than _txnLogId_.\n+#* If a Bookie server has crashed before persisting _LastLogMark_ to disk, it still has journal files containing entries for which index pages may not have been persisted. Consequently, when this Bookie server restarts, it inspects journal files to restore those entries; data isn't lost.\n+\n+p. Using the above data flush mechanism, it is safe for the _Sync Thread_ to skip data flushing when the Bookie server shuts down. However, in _Entry Logger_, it uses _BufferedChannel_ to write entries in batches and there might be data buffered in _BufferedChannel_ upon a shut down. Bookie server needs to ensure _Entry Logger_ flushes its buffered data during shutting down. Otherwise, _Entry Log_ files become corrupted with partial entries.\n+\n+p. As described above, _EntryLogger#flush_ is invoked in the following two cases:\n+* in _Sync Thread_ : used to ensure entries added before _LastLogMark_ are persisted to disk.\n+* in _ShutDown_ : used to ensure its buffered data persisted to disk to avoid data corruption with partial entries."}]}

