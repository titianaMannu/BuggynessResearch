{"sha":"d20cb28943e80c1d39cca5916f459651e29dd573","node_id":"MDY6Q29tbWl0MTU3NTk1NjpkMjBjYjI4OTQzZTgwYzFkMzljY2E1OTE2ZjQ1OTY1MWUyOWRkNTcz","commit":{"author":{"name":"Benjamin Reed","email":"breed@apache.org","date":"2011-06-28T02:03:01Z"},"committer":{"name":"Benjamin Reed","email":"breed@apache.org","date":"2011-06-28T02:03:01Z"},"message":"initial setup for hedwig and bookkeeper doc\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/zookeeper/bookkeeper/trunk@1140397 13f79535-47bb-0310-9956-ffa450edef68","tree":{"sha":"c8f595edaadb4dcca599758658ad7dd0ef05d569","url":"https://api.github.com/repos/apache/bookkeeper/git/trees/c8f595edaadb4dcca599758658ad7dd0ef05d569"},"url":"https://api.github.com/repos/apache/bookkeeper/git/commits/d20cb28943e80c1d39cca5916f459651e29dd573","comment_count":0,"verification":{"verified":false,"reason":"unsigned","signature":null,"payload":null}},"url":"https://api.github.com/repos/apache/bookkeeper/commits/d20cb28943e80c1d39cca5916f459651e29dd573","html_url":"https://github.com/apache/bookkeeper/commit/d20cb28943e80c1d39cca5916f459651e29dd573","comments_url":"https://api.github.com/repos/apache/bookkeeper/commits/d20cb28943e80c1d39cca5916f459651e29dd573/comments","author":{"login":"breed","id":143779,"node_id":"MDQ6VXNlcjE0Mzc3OQ==","avatar_url":"https://avatars.githubusercontent.com/u/143779?v=4","gravatar_id":"","url":"https://api.github.com/users/breed","html_url":"https://github.com/breed","followers_url":"https://api.github.com/users/breed/followers","following_url":"https://api.github.com/users/breed/following{/other_user}","gists_url":"https://api.github.com/users/breed/gists{/gist_id}","starred_url":"https://api.github.com/users/breed/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/breed/subscriptions","organizations_url":"https://api.github.com/users/breed/orgs","repos_url":"https://api.github.com/users/breed/repos","events_url":"https://api.github.com/users/breed/events{/privacy}","received_events_url":"https://api.github.com/users/breed/received_events","type":"User","site_admin":false},"committer":{"login":"breed","id":143779,"node_id":"MDQ6VXNlcjE0Mzc3OQ==","avatar_url":"https://avatars.githubusercontent.com/u/143779?v=4","gravatar_id":"","url":"https://api.github.com/users/breed","html_url":"https://github.com/breed","followers_url":"https://api.github.com/users/breed/followers","following_url":"https://api.github.com/users/breed/following{/other_user}","gists_url":"https://api.github.com/users/breed/gists{/gist_id}","starred_url":"https://api.github.com/users/breed/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/breed/subscriptions","organizations_url":"https://api.github.com/users/breed/orgs","repos_url":"https://api.github.com/users/breed/repos","events_url":"https://api.github.com/users/breed/events{/privacy}","received_events_url":"https://api.github.com/users/breed/received_events","type":"User","site_admin":false},"parents":[{"sha":"267ed95a2ae2ef1a8f6a3166b217d86a28166391","url":"https://api.github.com/repos/apache/bookkeeper/commits/267ed95a2ae2ef1a8f6a3166b217d86a28166391","html_url":"https://github.com/apache/bookkeeper/commit/267ed95a2ae2ef1a8f6a3166b217d86a28166391"}],"stats":{"total":1819,"additions":1066,"deletions":753},"files":[{"sha":"f29b1d5d59917736f5e3b2d5f4ba02e6e2d4a6d9","filename":"doc/bookkeeperConfig.textile","status":"added","additions":47,"deletions":0,"changes":47,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperConfig.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperConfig.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/bookkeeperConfig.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,47 @@\n+Title:        BookKeeper Administrator's Guide\n+Notice: Licensed under the Apache License, Version 2.0 (the \"License\");\n+        you may not use this file except in compliance with the License. You may\n+        obtain a copy of the License at \"http://www.apache.org/licenses/LICENSE-2.0\":http://www.apache.org/licenses/LICENSE-2.0.\n+        .\n+        .        \n+        Unless required by applicable law or agreed to in writing,\n+        software distributed under the License is distributed on an \"AS IS\"\n+        BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+        implied. See the License for the specific language governing permissions\n+        and limitations under the License.\n+        .\n+        .\n+\n+h1. Abstract\n+\n+This document contains information about deploying, administering and mantaining BookKeeper. It also discusses best practices and common problems. \n+\n+As BookKeeper is still a prototype, this article is likely to change significantly over time. \n+\n+h1. System requirements\n+\n+p. A typical BookKeeper installation comprises a set of bookies and a set of ZooKeeper replicas. The exact number of bookies depends on the quorum mode, desired throughput, and number of clients using this installation simultaneously. The minimum number of bookies is three for self-verifying (stores a message authentication code along with each entry) and four for generic (does not store a message authentication codewith each entry), and there is no upper limit on the number of bookies. Increasing the number of bookies, in fact, enables higher throughput. \n+\n+p. For performance, we require each server to have at least two disks. It is possible to run a bookie with a single disk, but performance will be significantly lower in this case. Of course, it works with one disk, but performance is significantly lower. \n+\n+p. For ZooKeeper, there is no constraint with respect to the number of replicas. Having a single machine running ZooKeeper in standalone mode is sufficient for BookKeeper. For resilience purposes, it might be a good idea to run ZooKeeper in quorum mode with multiple servers. Please refer to the ZooKeeper documentation for detail on how to configure ZooKeeper with multiple replicas \n+\n+h1. Running bookies\n+\n+p. To run a bookie, we execute the following command: \n+\n+ @java -cp .:./zookeeper-<version>-bookkeeper.jar:./zookeeper-<version>.jar\\ :../log4j/apache-log4j-1.2.15/log4j-1.2.15.jar -Dlog4j.configuration=log4j.properties\\ org.apache.bookkeeper.proto.BookieServer 3181 127.0.0.1:2181 /path_to_log_device/\\ /path_to_ledger_device/ @ \n+\n+p. The parameters are: \n+\n+* Port number that the bookie listens on; \n+* Comma separated list of ZooKeeper servers with a hostname:port format; \n+* Path for Log Device (stores bookie write-ahead log); \n+* Path for Ledger Device (stores ledger entries); \n+\n+\n+p. Ideally, @/path_to_log_device/@ and @/path_to_ledger_device/@ are each in a different device. \n+\n+h1. ZooKeeper Metadata\n+\n+p. For BookKeeper, we require a ZooKeeper installation to store metadata, and to pass the list of ZooKeeper servers as parameter to the constructor of the BookKeeper class ( @org.apache.bookkeeper.client,BookKeeper@ ). To setup ZooKeeper, please check the \"ZooKeeper documentation\":index.html. "},{"sha":"602d34e5e735773da155f862b311c9b8e4f94d2e","filename":"doc/bookkeeperOverview.textile","status":"added","additions":118,"deletions":0,"changes":118,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperOverview.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperOverview.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/bookkeeperOverview.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,118 @@\n+Title:        BookKeeper overview\n+Notice: Licensed under the Apache License, Version 2.0 (the \"License\");\n+        you may not use this file except in compliance with the License. You may\n+        obtain a copy of the License at \"http://www.apache.org/licenses/LICENSE-2.0\":http://www.apache.org/licenses/LICENSE-2.0.\n+        .        \n+        Unless required by applicable law or agreed to in writing,\n+        software distributed under the License is distributed on an \"AS IS\"\n+        BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+        implied. See the License for the specific language governing permissions\n+        and limitations under the License.\n+        .\n+\n+h1. Abstract\n+\n+This guide contains detailed information about using BookKeeper for logging. It discusses the basic operations BookKeeper supports, and how to create logs and perform basic read and write operations on these logs.\n+\n+h1. BookKeeper introduction\n+\n+p. BookKeeper is a replicated service to reliably log streams of records. In BookKeeper, servers are \"bookies\", log streams are \"ledgers\", and each unit of a log (aka record) is a \"ledger entry\". BookKeeper is designed to be reliable; bookies, the servers that store ledgers, can crash, corrupt data, discard data, but as long as there are enough bookies behaving correctly the service as a whole behaves correctly. \n+\n+p. The initial motivation for BookKeeper comes from the namenode of HDFS. Namenodes have to log operations in a reliable fashion so that recovery is possible in the case of crashes. We have found the applications for BookKeeper extend far beyond HDFS, however. Essentially, any application that requires an append storage can replace their implementations with BookKeeper. BookKeeper has the advantage of scaling throughput with the number of servers. \n+\n+p. At a high level, a bookkeeper client receives entries from a client application and stores it to sets of bookies, and there are a few advantages in having such a service: \n+\n+* We can use hardware that is optimized for such a service. We currently believe that such a system has to be optimized only for disk I/O; \n+* We can have a pool of servers implementing such a log system, and shared among a number of servers; \n+* We can have a higher degree of replication with such a pool, which makes sense if the hardware necessary for it is cheaper compared to the one the application uses. \n+\n+\n+h1. In slightly more detail...\n+\n+p. BookKeeper implements highly available logs, and it has been designed with write-ahead logging in mind. Besides high availability due to the replicated nature of the service, it provides high throughput due to striping. As we write entries in a subset of bookies of an ensemble and rotate writes across available quorums, we are able to increase throughput with the number of servers for both reads and writes. Scalability is a property that is possible to achieve in this case due to the use of quorums. Other replication techniques, such as state-machine replication, do not enable such a property. \n+\n+p. An application first creates a ledger before writing to bookies through a local BookKeeper client instance. Upon creating a ledger, a BookKeeper client writes metadata about the ledger to ZooKeeper. Each ledger currently has a single writer. This writer has to execute a close ledger operation before any other client can read from it. If the writer of a ledger does not close a ledger properly because, for example, it has crashed before having the opportunity of closing the ledger, then the next client that tries to open a ledger executes a procedure to recover it. As closing a ledger consists essentially of writing the last entry written to a ledger to ZooKeeper, the recovery procedure simply finds the last entry written correctly and writes it to ZooKeeper. \n+\n+p. Note that currently this recovery procedure is executed automatically upon trying to open a ledger and no explicit action is necessary. Although two clients may try to recover a ledger concurrently, only one will succeed, the first one that is able to create the close znode for the ledger. \n+\n+h1. Bookkeeper elements and concepts\n+\n+p. BookKeeper uses four basic elements: \n+\n+*  _Ledger_ : A ledger is a sequence of entries, and each entry is a sequence of bytes. Entries are written sequentially to a ledger and at most once. Consequently, ledgers have an append-only semantics; \n+*  _BookKeeper client_ : A client runs along with a BookKeeper application, and it enables applications to execute operations on ledgers, such as creating a ledger and writing to it; \n+*  _Bookie_ : A bookie is a BookKeeper storage server. Bookies store the content of ledgers. For any given ledger L, we call an _ensemble_ the group of bookies storing the content of L. For performance, we store on each bookie of an ensemble only a fragment of a ledger. That is, we stripe when writing entries to a ledger such that each entry is written to sub-group of bookies of the ensemble. \n+*  _Metadata storage service_ : BookKeeper requires a metadata storage service to store information related to ledgers and available bookies. We currently use ZooKeeper for such a task. \n+\n+\n+h1. Bookkeeper initial design\n+\n+p. A set of bookies implements BookKeeper, and we use a quorum-based protocol to replicate data across the bookies. There are basically two operations to an existing ledger: read and append. Here is the complete API list (mode detail \"here\":bookkeeperProgrammer.html): \n+\n+* Create ledger: creates a new empty ledger; \n+* Open ledger: opens an existing ledger for reading; \n+* Add entry: adds a record to a ledger either synchronously or asynchronously; \n+* Read entries: reads a sequence of entries from a ledger either synchronously or asynchronously \n+\n+\n+p. There is only a single client that can write to a ledger. Once that ledger is closed or the client fails, no more entries can be added. (We take advantage of this behavior to provide our strong guarantees.) There will not be gaps in the ledger. Fingers get broken, people get roughed up or end up in prison when books are manipulated, so there is no deleting or changing of entries. \n+\n+!images/bk-overview.jpg!\n+p. A simple use of BooKeeper is to implement a write-ahead transaction log. A server maintains an in-memory data structure (with periodic snapshots for example) and logs changes to that structure before it applies the change. The application server creates a ledger at startup and store the ledger id and password in a well known place (ZooKeeper maybe). When it needs to make a change, the server adds an entry with the change information to a ledger and apply the change when BookKeeper adds the entry successfully. The server can even use asyncAddEntry to queue up many changes for high change throughput. BooKeeper meticulously logs the changes in order and call the completion functions in order. \n+\n+p. When the application server dies, a backup server will come online, get the last snapshot and then it will open the ledger of the old server and read all the entries from the time the snapshot was taken. (Since it doesn't know the last entry number it will use MAX_INTEGER). Once all the entries have been processed, it will close the ledger and start a new one for its use. \n+\n+p. A client library takes care of communicating with bookies and managing entry numbers. An entry has the following fields: \n+\n+|Field|Type|Description|\n+|Ledger number|long|The id of the ledger of this entry|\n+|Entry number|long|The id of this entry|\n+|last confirmed ( _LC_ )|long|id of the last recorded entry|\n+|data|byte[]|the entry data (supplied by application)|\n+|authentication code|byte[]|Message authentication code that includes all other fields of the entry|\n+\n+\n+p. The client library generates a ledger entry. None of the fields are modified by the bookies and only the first three fields are interpreted by the bookies. \n+\n+p. To add to a ledger, the client generates the entry above using the ledger number. The entry number will be one more than the last entry generated. The _LC_ field contains the last entry that has been successfully recorded by BookKeeper. If the client writes entries one at a time, _LC_ is the last entry id. But, if the client is using asyncAddEntry, there may be many entries in flight. An entry is considered recorded when both of the following conditions are met: \n+\n+* the entry has been accepted by a quorum of bookies \n+* all entries with a lower entry id have been accepted by a quorum of bookies \n+\n+\n+ _LC_ seems mysterious right now, but it is too early to explain how we use it; just smile and move on. \n+\n+p. Once all the other fields have been field in, the client generates an authentication code with all of the previous fields. The entry is then sent to a quorum of bookies to be recorded. Any failures will result in the entry being sent to a new quorum of bookies. \n+\n+p. To read, the client library initially contacts a bookie and starts requesting entries. If an entry is missing or invalid (a bad MAC for example), the client will make a request to a different bookie. By using quorum writes, as long as enough bookies are up we are guaranteed to eventually be able to read an entry. \n+\n+h1. Bookkeeper metadata management\n+\n+p. There are some meta data that needs to be made available to BookKeeper clients: \n+\n+* The available bookies; \n+* The list of ledgers; \n+* The list of bookies that have been used for a given ledger; \n+* The last entry of a ledger; \n+\n+\n+p. We maintain this information in ZooKeeper. Bookies use ephemeral nodes to indicate their availability. Clients use znodes to track ledger creation and deletion and also to know the end of the ledger and the bookies that were used to store the ledger. Bookies also watch the ledger list so that they can cleanup ledgers that get deleted. \n+\n+h1. Closing out ledgers\n+\n+p. The process of closing out the ledger and finding the last ledger is difficult due to the durability guarantees of BookKeeper: \n+\n+* If an entry has been successfully recorded, it must be readable. \n+* If an entry is read once, it must always be available to be read. \n+\n+\n+p. If the ledger was closed gracefully, ZooKeeper will have the last entry and everything will work well. But, if the BookKeeper client that was writing the ledger dies, there is some recovery that needs to take place. \n+\n+p. The problematic entries are the ones at the end of the ledger. There can be entries in flight when a BookKeeper client dies. If the entry only gets to one bookie, the entry should not be readable since the entry will disappear if that bookie fails. If the entry is only on one bookie, that doesn't mean that the entry has not been recorded successfully; the other bookies that recorded the entry might have failed. \n+\n+p. The trick to making everything work is to have a correct idea of a last entry. We do it in roughly three steps: \n+\n+# Find the entry with the highest last recorded entry, _LC_ ; \n+# Find the highest consecutively recorded entry, _LR_ ; \n+# Make sure that all entries between _LC_ and _LR_ are on a quorum of bookies; \n+"},{"sha":"7117e2a15b89d6162edd916349bdda51914ceb78","filename":"doc/bookkeeperProgrammer.textile","status":"added","additions":219,"deletions":0,"changes":219,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperProgrammer.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperProgrammer.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/bookkeeperProgrammer.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,219 @@\n+Title:        BookKeeper Getting Started Guide\n+Notice: Licensed under the Apache License, Version 2.0 (the \"License\");\n+        you may not use this file except in compliance with the License. You may\n+        obtain a copy of the License at \"http://www.apache.org/licenses/LICENSE-2.0\":http://www.apache.org/licenses/LICENSE-2.0.\n+        .        \n+        Unless required by applicable law or agreed to in writing,\n+        software distributed under the License is distributed on an \"AS IS\"\n+        BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+        implied. See the License for the specific language governing permissions\n+        and limitations under the License.\n+        .\n+\n+h1. Abstract\n+\n+This guide contains detailed information about using BookKeeper for logging. It discusses the basic operations BookKeeper supports, and how to create logs and perform basic read and write operations on these logs.\n+\n+h1.  Instantiating BookKeeper.\n+\n+p. The first step to use BookKeeper is to instantiate a BookKeeper object: \n+\n+ @org.apache.bookkeeper.BookKeeper @ \n+\n+p. There are three BookKeeper constructors: \n+\n+ @public BookKeeper(String servers) throws KeeperException, IOException @ \n+\n+p. where: \n+\n+*  @servers@ is a comma-separated list of ZooKeeper servers. \n+\n+\n+ @public BookKeeper(ZooKeeper zk) throws InterruptedException, KeeperException @ \n+\n+p. where: \n+\n+*  @zk@ is a ZooKeeper object. This constructor is useful when the application also using ZooKeeper and wants to have a single instance of ZooKeeper. \n+\n+\n+ @public BookKeeper(ZooKeeper zk, ClientSocketChannelFactory channelFactory) throws InterruptedException, KeeperException @ \n+\n+p. where: \n+\n+*  @zk@ is a ZooKeeper object. This constructor is useful when the application also using ZooKeeper and wants to have a single instance of ZooKeeper. \n+*  @channelFactory@ is a netty channel object ( @org.jboss.netty.channel.socket@ ). \n+\n+\n+h1.  Creating a ledger. \n+\n+p. Before writing entries to BookKeeper, it is necessary to create a ledger. With the current BookKeeper API, it is possible to create a ledger both synchronously or asynchronously. The following methods belong to @org.apache.bookkeeper.client.BookKeeper@ . \n+\n+ _Synchronous call:_ \n+\n+ @public LedgerHandle createLedger(int ensSize, int qSize, DigestType type,  byte passwd[]) throws KeeperException, InterruptedException, IOException, BKException @ \n+\n+p. where: \n+\n+*  @ensSize@ is the number of bookies (ensemble size); \n+*  @qSize@ is the write quorum size; \n+*  @type@ is the type of digest used with entries: either MAC or CRC32. \n+*  @passwd@ is a password that authorizes the client to write to the ledger being created. \n+\n+\n+p. All further operations on a ledger are invoked through the @LedgerHandle@ object returned. \n+\n+p. As a convenience, we provide a @createLedger@ with default parameters (3,2,VERIFIABLE), and the only two input parameters it requires are a digest type and a password. \n+\n+ _Asynchronous call:_ \n+\n+ @public void asyncCreateLedger(int ensSize, int qSize, DigestType type, byte passwd[], CreateCallback cb, Object ctx ) @ \n+\n+p. The parameters are the same of the synchronous version, with the exception of @cb@ and @ctx@ . @CreateCallback@ is an interface in @org.apache.bookkeeper.client.AsyncCallback@ , and a class implementing it has to implement a method called @createComplete@ that has the following signature: \n+\n+ @void createComplete(int rc, LedgerHandle lh, Object ctx); @ \n+\n+p. where: \n+\n+*  @rc@ is a return code (please refer to @org.apache.bookeeper.client.BKException@ for a list); \n+*  @lh@ is a @LedgerHandle@ object to manipulate a ledger; \n+*  @ctx@ is a control object for accountability purposes. It can be essentially any object the application is happy with. \n+\n+\n+p. The @ctx@ object passed as a parameter to the call to create a ledger is the one same returned in the callback. \n+\n+h1.  Adding entries to a ledger. \n+\n+p. Once we have a ledger handle @lh@ obtained through a call to create a ledger, we can start writing entries. As with creating ledgers, we can write both synchronously and asynchronously. The following methods belong to @org.apache.bookkeeper.client.LedgerHandle@ . \n+\n+ _Synchronous call:_ \n+\n+ @public long addEntry(byte[] data) throws InterruptedException @ \n+\n+p. where: \n+\n+*  @data@ is a byte array; \n+\n+\n+p. A call to @addEntry@ returns the status of the operation (please refer to @org.apache.bookeeper.client.BKDefs@ for a list); \n+\n+ _Asynchronous call:_ \n+\n+ @public void asyncAddEntry(byte[] data, AddCallback cb, Object ctx) @ \n+\n+p. It also takes a byte array as the sequence of bytes to be stored as an entry. Additionaly, it takes a callback object @cb@ and a control object @ctx@ . The callback object must implement the @AddCallback@ interface in @org.apache.bookkeeper.client.AsyncCallback@ , and a class implementing it has to implement a method called @addComplete@ that has the following signature: \n+\n+ @void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx); @ \n+\n+p. where: \n+\n+*  @rc@ is a return code (please refer to @org.apache.bookeeper.client.BKDefs@ for a list); \n+*  @lh@ is a @LedgerHandle@ object to manipulate a ledger; \n+*  @entryId@ is the identifier of entry associated with this request; \n+*  @ctx@ is control object used for accountability purposes. It can be any object the application is happy with. \n+\n+\n+h1.  Closing a ledger. \n+\n+p. Once a client is done writing, it closes the ledger. The following methods belong to @org.apache.bookkeeper.client.LedgerHandle@ . \n+\n+ _Synchronous close:_ \n+\n+ @public void close() throws InterruptedException @ \n+\n+p. It takes no input parameters. \n+\n+ _Asynchronous close:_ \n+\n+ @public void asyncClose(CloseCallback cb, Object ctx) throws InterruptedException @ \n+\n+p. It takes a callback object @cb@ and a control object @ctx@ . The callback object must implement the @CloseCallback@ interface in @org.apache.bookkeeper.client.AsyncCallback@ , and a class implementing it has to implement a method called @closeComplete@ that has the following signature: \n+\n+ @void closeComplete(int rc, LedgerHandle lh, Object ctx) @ \n+\n+p. where: \n+\n+*  @rc@ is a return code (please refer to @org.apache.bookeeper.client.BKDefs@ for a list); \n+*  @lh@ is a @LedgerHandle@ object to manipulate a ledger; \n+*  @ctx@ is control object used for accountability purposes. \n+\n+\n+h1.  Opening a ledger. \n+\n+p. To read from a ledger, a client must open it first. The following methods belong to @org.apache.bookkeeper.client.BookKeeper@ . \n+\n+ _Synchronous open:_ \n+\n+ @public LedgerHandle openLedger(long lId, DigestType type, byte passwd[]) throws InterruptedException, BKException @ \n+\n+*  @ledgerId@ is the ledger identifier; \n+*  @type@ is the type of digest used with entries: either MAC or CRC32. \n+*  @passwd@ is a password to access the ledger (used only in the case of @VERIFIABLE@ ledgers); \n+\n+\n+ _Asynchronous open:_ \n+\n+ @public void asyncOpenLedger(long lId, DigestType type, byte passwd[], OpenCallback cb, Object ctx) @ \n+\n+p. It also takes a a ledger identifier and a password. Additionaly, it takes a callback object  @cb@ and a control object @ctx@ . The callback object must implement the @OpenCallback@ interface in @org.apache.bookkeeper.client.AsyncCallback@ , and a class implementing it has to implement a method called @openComplete@ that has the following signature: \n+\n+ @public void openComplete(int rc, LedgerHandle lh, Object ctx) @ \n+\n+p. where: \n+\n+*  @rc@ is a return code (please refer to @org.apache.bookeeper.client.BKDefs@ for a list); \n+*  @lh@ is a @LedgerHandle@ object to manipulate a ledger; \n+*  @ctx@ is control object used for accountability purposes. \n+\n+\n+h1.  Reading from ledger \n+\n+p. Read calls may request one or more consecutive entries. The following methods belong to @org.apache.bookkeeper.client.LedgerHandle@ . \n+\n+ _Synchronous read:_ \n+\n+ @public Enumeration<LedgerEntry> readEntries(long firstEntry, long lastEntry) throws InterruptedException, BKException @ \n+\n+*  @firstEntry@ is the identifier of the first entry in the sequence of entries to read; \n+*  @lastEntry@ is the identifier of the last entry in the sequence of entries to read. \n+\n+\n+ _Asynchronous read:_ \n+\n+ @public void asyncReadEntries(long firstEntry, long lastEntry, ReadCallback cb, Object ctx) throws BKException, InterruptedException @ \n+\n+p. It also takes a first and a last entry identifiers. Additionaly, it takes a callback object  @cb@ and a control object @ctx@ . The callback object must implement the @ReadCallback@ interface in @org.apache.bookkeeper.client.AsyncCallback@ , and a class implementing it has to implement a method called @readComplete@ that has the following signature: \n+\n+ @void readComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx) @ \n+\n+p. where: \n+\n+*  @rc@ is a return code (please refer to @org.apache.bookeeper.client.BKDefs@ for a list); \n+*  @lh@ is a @LedgerHandle@ object to manipulate a ledger; \n+*  @seq@ is a @Enumeration<LedgerEntry>@ object to containing the list of entries requested; \n+*  @ctx@ is control object used for accountability purposes. \n+\n+\n+h1.  Deleting a ledger \n+\n+p. Once a client is done with a ledger and is sure that nobody will ever need to read from it again, they can delete the ledger. The following methods belong to @org.apache.bookkeeper.client.BookKeeper@ . \n+\n+ _Synchronous delete:_ \n+\n+ @public void deleteLedger(long lId) throws InterruptedException, BKException @ \n+\n+*  @lId@ is the ledger identifier; \n+\n+\n+ _Asynchronous delete:_ \n+\n+ @public void asyncDeleteLedger(long lId, DeleteCallback cb, Object ctx) @ \n+\n+p. It takes a ledger identifier. Additionally, it takes a callback object  @cb@ and a control object @ctx@ . The callback object must implement the @DeleteCallback@ interface in @org.apache.bookkeeper.client.AsyncCallback@ , and a class implementing it has to implement a method called @deleteComplete@ that has the following signature: \n+\n+ @void deleteComplete(int rc, Object ctx) @ \n+\n+p. where: \n+\n+*  @rc@ is a return code (please refer to @org.apache.bookeeper.client.BKDefs@ for a list); \n+*  @ctx@ is control object used for accountability purposes. \n+"},{"sha":"dc01dc87c99ecdd0c4826bf568ff4132a602d998","filename":"doc/bookkeeperStarted.textile","status":"added","additions":89,"deletions":0,"changes":89,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperStarted.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperStarted.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/bookkeeperStarted.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,89 @@\n+Title:        BookKeeper Getting Started Guide\n+Notice: Licensed under the Apache License, Version 2.0 (the \"License\");\n+        you may not use this file except in compliance with the License. You may\n+        obtain a copy of the License at \"http://www.apache.org/licenses/LICENSE-2.0\":http://www.apache.org/licenses/LICENSE-2.0.\n+        .        \n+        Unless required by applicable law or agreed to in writing,\n+        software distributed under the License is distributed on an \"AS IS\"\n+        BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+        implied. See the License for the specific language governing permissions\n+        and limitations under the License.\n+        .\n+\n+h1. Abstract\n+\n+This guide contains detailed information about using BookKeeper for logging. It discusses the basic operations BookKeeper supports, and how to create logs and perform basic read and write operations on these logs.\n+\n+h1. Getting Started: Setting up BookKeeper to write logs.\n+\n+p. This document contains information to get you started quickly with BookKeeper. It is aimed primarily at developers willing to try it out, and contains simple installation instructions for a simple BookKeeper installation and a simple programming example. For further programming detail, please refer to  \"BookKeeper Programmer's Guide\":bookkeeperProgrammer.html. \n+\n+h1. Pre-requisites\n+\n+p. See \"System Requirements\":bookkeeperConfig.html#bk_sysReqin the Admin guide.\n+\n+h1. Download\n+\n+p. BookKeeper is distributed along with ZooKeeper. To get a ZooKeeper distribution, download a recent  \"stable\":http://hadoop.apache.org/zookeeper/releases.htmlrelease from one of the Apache Download Mirrors.\n+\n+h1. LocalBookKeeper\n+\n+p. Under org.apache.bookkeeper.util, you'll find a java program called LocalBookKeeper.java that sets you up to run BookKeeper on a single machine. This is far from ideal from a performance perspective, but the program is useful for both test and educational purposes. \n+\n+h1. Setting up bookies\n+\n+p. If you're bold and you want more than just running things locally, then you'll need to run bookies in different servers. You'll need at least three bookies to start with. \n+\n+p. For each bookie, we need to execute a command like the following: \n+\n+ @java -cp .:./zookeeper-<version>-bookkeeper.jar:./zookeeper-<version>.jar\\ :lib/slf4j-api-1.6.1.jar:lib/slf4j-log4j12-1.6.1.jar:lib/log4j-1.2.15.jar -Dlog4j.configuration=log4j.properties\\ org.apache.bookkeeper.proto.BookieServer 3181 127.0.0.1:2181 /path_to_log_device/\\ /path_to_ledger_device/ @ \n+\n+p. \"/path_to_log_device/\" and \"/path_to_ledger_device/\" are different paths. Also, port 3181 is the port that a bookie listens on for connection requests from clients. 127.0.0.1:2181 is the hostname:port for the ZooKeeper server. In this example, the standalone ZooKeeper server is running locally on port 2181. If we had multiple ZooKeeper servers, this parameter would be a comma separated list of all the hostname:port values corresponding to them. \n+\n+h1. Setting up ZooKeeper\n+\n+p. ZooKeeper stores metadata on behalf of BookKeeper clients and bookies. To get a minimal ZooKeeper installation to work with BookKeeper, we can set up one server running in standalone mode. Once we have the server running, we need to create a few znodes: \n+\n+#  @/ledgers @ \n+#  @/ledgers/available @ \n+# For each bookie, we add one znode such that the name of the znode is the concatenation of the machine name and the port number that the bookie is listening on. For example, if a bookie is running on bookie.foo.com an is listening on port 3181, we add a znode  @/ledgers/available/bookie.foo.com:3181@ . \n+\n+\n+h1. Example\n+\n+p. In the following excerpt of code, we: \n+\n+# Create a ledger; \n+# Write to the ledger; \n+# Close the ledger; \n+# Open the same ledger for reading; \n+# Read from the ledger; \n+# Close the ledger again; \n+\n+bc.. \n+LedgerHandle lh = bkc.createLedger(ledgerPassword);\n+ledgerId = lh.getId();\n+ByteBuffer entry = ByteBuffer.allocate(4);\n+\n+for(int i = 0; i < 10; i++){\n+\tentry.putInt(i);\n+\tentry.position(0);\n+\tentries.add(entry.array());\t\t\t\t\n+\tlh.addEntry(entry.array());\n+}\n+lh.close();\n+lh = bkc.openLedger(ledgerId, ledgerPassword);\t\t\n+\t\t\t\n+Enumeration<LedgerEntry> ls = lh.readEntries(0, 9);\n+int i = 0;\n+while(ls.hasMoreElements()){\n+\tByteBuffer origbb = ByteBuffer.wrap(\n+\t\t\t\tentries.get(i++));\n+\tInteger origEntry = origbb.getInt();\n+\tByteBuffer result = ByteBuffer.wrap(\n+\t\t\t\tls.nextElement().getEntry());\n+\n+\tInteger retrEntry = result.getInt();\n+}\n+lh.close();\n+\t    "},{"sha":"a0c8feb8578e9999d5e70f2d8071959939f6538c","filename":"doc/bookkeeperStream.textile","status":"added","additions":124,"deletions":0,"changes":124,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperStream.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/bookkeeperStream.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/bookkeeperStream.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,124 @@\n+Title:        Streaming with BookKeeper\n+Notice: Licensed under the Apache License, Version 2.0 (the \"License\");\n+        you may not use this file except in compliance with the License. You may\n+        obtain a copy of the License at \"http://www.apache.org/licenses/LICENSE-2.0\":http://www.apache.org/licenses/LICENSE-2.0.\n+        .        \n+        Unless required by applicable law or agreed to in writing,\n+        software distributed under the License is distributed on an \"AS IS\"\n+        BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+        implied. See the License for the specific language governing permissions\n+        and limitations under the License.\n+        .\n+\n+h1. Abstract\n+\n+This guide contains detailed information about using how to stream bytes on top of BookKeeper. It essentially motivates and discusses the basic stream operations currently supported.\n+\n+h1. Summary\n+\n+p. When using the BookKeeper API, an application has to split the data to write into entries, each entry being a byte array. This is natural for many applications. For example, when using BookKeeper for write-ahead logging, an application typically wants to write the modifications corresponding to a command or a transaction. Some other applications, however, might not have a natural boundary for entries, and may prefer to write and read streams of bytes. This is exactly the purpose of the stream API we have implemented on top of BookKeeper. \n+\n+p. The stream API is implemented in the package @Streaming@ , and it contains two main classes: @LedgerOutputStream@ and  @LedgerInputStream@ . The class names are indicative of what they do. \n+\n+h1. Writing a stream of bytes\n+\n+p. Class @LedgerOutputStream@ implements two constructors and five public methods: \n+\n+ @public LedgerOutputStream(LedgerHandle lh) @ \n+\n+p. where: \n+\n+*  @lh@ is a ledger handle for a previously created and open ledger. \n+\n+\n+ @public LedgerOutputStream(LedgerHandle lh, int size) @ \n+\n+p. where: \n+\n+*  @lh@ is a ledger handle for a previously created and open ledger. \n+*  @size@ is the size of the byte buffer to store written bytes before flushing. \n+\n+\n+ _Closing a stream._ This call closes the stream by flushing the write buffer. \n+\n+ @public void close() @ \n+\n+p. which has no parameters. \n+\n+ _Flushing a stream._ This call essentially flushes the write buffer. \n+\n+ @public synchronized void flush() @ \n+\n+p. which has no parameters. \n+\n+ _Writing bytes._ There are three calls for writing bytes to a stream. \n+\n+ @public synchronized void write(byte[] b) @ \n+\n+p. where: \n+\n+*  @b@ is an array of bytes to write. \n+\n+\n+ @public synchronized void write(byte[] b, int off, int len) @ \n+\n+p. where: \n+\n+*  @b@ is an array of bytes to write. \n+*  @off@ is a buffer offset. \n+*  @len@ is the length to write. \n+\n+\n+ @public synchronized void write(int b) @ \n+\n+p. where: \n+\n+*  @b@ contains a byte to write. The method writes the least significant byte of the integer four bytes. \n+\n+\n+h1. Reading a stream of bytes\n+\n+p. Class @LedgerOutputStream@ implements two constructors and four public methods: \n+\n+ @public LedgerInputStream(LedgerHandle lh) throws BKException, InterruptedException @ \n+\n+p. where: \n+\n+*  @lh@ is a ledger handle for a previously created and open ledger. \n+\n+\n+ @public LedgerInputStream(LedgerHandle lh, int size) throws BKException, InterruptedException @ \n+\n+p. where: \n+\n+*  @lh@ is a ledger handle for a previously created and open ledger. \n+*  @size@ is the size of the byte buffer to store bytes that the application will eventually read. \n+\n+\n+ _Closing._ There is one call to close an input stream, but the call is currently empty and the application is responsible for closing the ledger handle. \n+\n+ @public void close() @ \n+\n+p. which has no parameters. \n+\n+ _Reading._ There are three calls to read from the stream. \n+\n+ @public synchronized int read() throws IOException @ \n+\n+p. which has no parameters. \n+\n+ @public synchronized int read(byte[] b) throws IOException @ \n+\n+p. where: \n+\n+*  @b@ is a byte array to write to. \n+\n+\n+ @public synchronized int read(byte[] b, int off, int len) throws IOException @ \n+\n+p. where: \n+\n+*  @b@ is a byte array to write to. \n+*  @off@ is an offset for byte array @b@ . \n+*  @len@ is the length in bytes to write to @b@ . \n+"},{"sha":"83c129ad2d976a8e8ef21204ef03424102db494b","filename":"doc/build.txt","status":"removed","additions":0,"deletions":146,"changes":146,"blob_url":"https://github.com/apache/bookkeeper/blob/267ed95a2ae2ef1a8f6a3166b217d86a28166391/doc/build.txt","raw_url":"https://github.com/apache/bookkeeper/raw/267ed95a2ae2ef1a8f6a3166b217d86a28166391/doc/build.txt","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/build.txt?ref=267ed95a2ae2ef1a8f6a3166b217d86a28166391","patch":"@@ -1,146 +0,0 @@\n-% Building Hedwig\n-% Yang Zhang\n-\n-Pre-requisites\n-==============\n-\n-For the core itself:\n-\n-- JDK 6: <http://java.sun.com/>.  Ensure `$JAVA_HOME` is correctly set.\n-- Maven 2: <http://maven.apache.org/>.\n-- Protocol Buffers 2.3.0: <http://protobuf.googlecode.com/>.\n-- Zookeeper 3.4.0: <http://hadoop.apache.org/zookeeper/>.  See below.\n-- Bookkeeper 3.4.0: <http://hadoop.apache.org/zookeeper/>.  See below.\n-\n-Hedwig has been tested on Windows XP, Linux 2.6, and OS X.\n-\n-For the deployment and distributed support scripts in `hw.bash`:\n-\n-- Ant: <http://ant.apache.org/>, if you want to build Zookeeper.\n-- Bash: <http://www.gnu.org/software/bash/>.\n-- Coreutils: <http://www.gnu.org/software/coreutils/>.\n-- Expect: <http://expect.nist.gov/>, if you want `unbuffer`.\n-- Findutils: <http://www.gnu.org/software/findutils/>.\n-- OpenSSH: <http://www.openssh.com/>.\n-- Python 2.6: <http://python.org/>.\n-\n-Protocol Buffers\n-----------------\n-\n-Hedwig requires the use of the Java runtime libraries of Protocol Buffers 2.3.0.\n-These libraries need to be installed into your local maven repository. (Maven allows\n-multiple versions to be installed.) To install protocol buffels to your local\n-repository, you have to download the tarball and follow the README.txt \n-instructions. Note that you must first install the C++ package which contains the\n-compiler (protoc) before you can build the java libraries. That will install the\n-library jar's in the local maven repository where Hedwig is currently configured\n-to point to.\n-\n-Zookeeper and Bookkeeper\n-------------------------\n-\n-Hedwig currently requires the version of Bookkeeper maintained in Apache's current\n-trunk SVN respository (version 3.4.0). This is not a released version yet but certain\n-features needed for BookKeeper are only available there.\n-\n-Hedwig also depends on ZK testing code for its own testing code.\n-\n-Since Hedwig is a Maven project, all these dependencies must be made available\n-as Maven artifacts.  However, neither ZK nor BK are currently Mavenized.\n-Hedwig provides some bash scripts to ease the installation of ZK, ZK tests, and\n-BK, all as Maven artifacts.\n-\n-Currently, we have included the necessary ZooKeeper and BookKeeper jars in the Hedwig\n-source itself in the $HEDWIG_DIR/server/lib directory. There is no need to retrieve\n-them directly from the Apache download site as they are non-released trunk versions.\n-\n-#Not relevant right now since we already have the ZK jars already in the Hedwig source.\n-To fetch and build ZK 3.4.0 (and its tests) in the current directory, run:\n-\n-  $HEDWIG_DIR/scripts/hw.bash get-zk\n-\n-#Not relevant right now, but when we start using the apache version of BK, to\n-build the local version of BK:\n-\n-  $HEDWIG_DIR/scripts/hw.bash get-bk\n-\n-The $HEDWIG_DIR/server/lib directory contains all of the the class and source jars for\n-ZK, ZK tests, and BK.  To install these, go to that directory and run the following\n-command to install them into your local maven repository:\n-\n-  $HEDWIG_DIR/scripts/hw.bash install-zk-bk\n-\n-Command-Line Instructions\n-=========================\n-\n-From the main Hedwig directory, run `mvn package`.  This will produce the\n-executable jars for both the client and server, as well as a server \"assembly\n-jar\" containing all dependencies as well for easier deployment.\n-\n-See the User's Guide for instructions on running and usage.\n-\n-Eclipse Instructions\n-====================\n-\n-To check out, build, and develop using Eclipse:\n-\n-1. Install the Subclipse plugin.  Update site:\n-   <http://subclipse.tigris.org/update_1.4.x>.\n-\n-2. Install the Maven plugin.  Update site:\n-   <http://m2eclipse.sonatype.org/update>.  From the list of packages available\n-   from this site, select everything under the \"Maven Integration\" category,\n-   and from the optional components select the ones with the word \"SCM\" in them.\n-\n-3. Go to Preferences > Team > SVN.  For the SVN interface, choose \"Pure Java\".\n-\n-4. Choose File > New > Project... > Maven > Checkout Maven Projects from SCM.\n-\n-5. For the SCM URL type, choose SVN.  For the URL, enter\n-   SVN URL.  Maven will automatically\n-   create a top-level Eclipse project for each of the 4 Maven modules\n-   (recommended).  If you want fewer top-level projects, uncheck the option of\n-   having a project for each module (under Advanced).\n-\n-6. Right-click on the `protocol` project and choose Run As > Maven\n-   generate-sources.  This will generate the Java and C++ code for Protocol\n-   Buffers.\n-\n-7. Refresh the workspace to pick up the generated code and add\n-   `hedwig/protocol/target/generated-sources/java` as a source folder.  (6 & 7\n-   should really be doable automatically, but I haven't figured out how.)\n-\n-You are now ready to run and debug the client and server code.  See the User's\n-Guide for instructions on running and usage.\n-\n-Utilities\n-=========\n-\n-Removing Conflicting Files in Jars\n-----------------------------------\n-\n-The Maven assembly plugin that produces the fat assembly jar may end up putting\n-into the jar files with the same conflicting paths from multiple dependencies.\n-This makes working with the files from certain tools (like `jar`) a bit jarring.\n-In our case, these files are not things like class files, but rather README and\n-LICENSE files, so we can safely remove conflicts by choosing an arbitrary winner.\n-To do so, run:\n-\n-  $HEDWIG_DIR/scripts/hw.bash strip-jar\n-\n-Adjusting Logging\n------------------\n-\n-The logging level is something that is baked into the jar in the\n-`log4j.properties` resource.  However, it would be wasteful to go through a\n-Maven build cycle to update and adjust this.  If you're working from a source\n-tree, it's also annoying to have to edit a source file to adjust the logging.\n-\n-We have a little script for tweaking the logging level.  After running\n-`strip-jar`, run:\n-\n-  $HEDWIG_DIR/scripts/hw.bash set-logging WARN\n-\n-To see what the current logging level is:\n-\n-  $HEDWIG_DIR/scripts/hw.bash get-logging"},{"sha":"e40d1a4bf8abb87ab45a08b2a217a578d7d28b31","filename":"doc/dev.txt","status":"removed","additions":0,"deletions":338,"changes":338,"blob_url":"https://github.com/apache/bookkeeper/blob/267ed95a2ae2ef1a8f6a3166b217d86a28166391/doc/dev.txt","raw_url":"https://github.com/apache/bookkeeper/raw/267ed95a2ae2ef1a8f6a3166b217d86a28166391/doc/dev.txt","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/dev.txt?ref=267ed95a2ae2ef1a8f6a3166b217d86a28166391","patch":"@@ -1,338 +0,0 @@\n-% Developer's Guide\n-\n-Style\n-=====\n-\n-We have provided an Eclipse Formatter file `formatter.xml` with all the\n-formatting conventions currently used in the project.  Highlights include no\n-tabs, 4-space indentation, and 120-char width.  Please respect this so as to\n-reduce the amount of formatting-related noise produced in commits.\n-\n-Static Analysis\n-===============\n-\n-We would like to use static analysis tools PMD and FindBugs to maintain code\n-quality.  However, we have not yet arrived at a consensus on what rules to\n-adhere to, and what to ignore.\n-\n-Netty Notes\n-===========\n-\n-The asynchronous network IO infrastructure that Hedwig uses is [Netty].  Here\n-are some notes on Netty's concurrency architecture and its filter pipeline\n-design.\n-\n-[Netty]: http://www.jboss.org/netty\n-\n-Concurrency Architecture\n-------------------------\n-\n-After calling `ServerBootstrap.bind()`, Netty starts a boss thread\n-(`NioServerSocketPipelineSink.Boss`) that just accepts new connections and\n-registers them with one of the workers from the `NioWorker` pool in round-robin\n-fashion (pool size defaults to CPU count).  Each worker runs its own select\n-loop over just the set of keys that have been registered with it.  Workers\n-start lazily on demand and run only so long as there are interested fd's/keys.\n-All selected events are handled in the same thread and sent up the pipeline\n-attached to the channel (this association is established by the boss as soon as\n-a new connection is accepted).\n-\n-All workers, and the boss, run via the executor thread pool; hence, the\n-executor must support at least two simultaneous threads.\n-\n-Handler Pipeline\n-----------------\n-\n-A pipeline implements the intercepting filter pattern.  A pipeline is a\n-sequence of handlers.  Whenever a packet is read from the wire, it travels up\n-the stream, stopping at each handler that can handle upstream events.\n-Vice-versa for writes.  Between each filter, control flows back through the\n-centralized pipeline, and a linked list of contexts keeps track of where we are\n-in the pipeline (one context object per handler).\n-\n-Distributed Performance Evaluation\n-==================================\n-\n-We've included some scripts to repeatedly run varying configurations of Hedwig\n-on a distributed testbed and collect the resulting data.  The experiments use\n-the `org.apache.hedwig.client.App` client application and are driven by\n-`scripts/hw.bash` (via the `app` command).\n-\n-Currently, we have two types of experiments: subscription benchmarks and\n-publishing benchmarks.\n-\n-Subscription Benchmarks\n------------------------\n-\n-The subscription benchmark compares synchronous and asynchronous subscriptions.\n-Because the synchronicity of subscriptions is a server configuration parameter,\n-the servers must be restarted to change this.  The benchmarks varies the\n-maximum number of outstanding subscription requests.\n-\n-To run the subscription benchmark with wilbur6 as the subscriber and wilbur1 as\n-its default hub:\n-\n-  hosts=wilbur6 scripts/hw.bash sub-exp wilbur1\n-\n-This produces log files into the `sub` directory, which may then be analyzed\n-using the analysis scripts.\n-\n-Publishing Benchmarks\n----------------------\n-\n-The publishing benchmark measures the throughput and latency of publishing\n-messages within a LAN and across a WAN.  It varies the following parameters:\n-\n-- maximum number of outstanding publish requests\n-- number of publishers\n-- number of (local) receivers\n-\n-We vary each dimension separately (and have default settings) to avoid a\n-combinatorial explosion in the number of configurations to explore.\n-\n-First, start a (standalone) instance:\n-\n-  scripts/hw.bash start-region '' $hwhost $zkhost $bk1host $bk2host $bk3host\n-\n-To run this over `$host1` through `$host3`, with the number of\n-publishers/subscribers varying linearly over this set:\n-\n-  npars=\"20 40 60 80 100\" scripts/hw.bash pub-exps \"$host1 $host2 $host3\" $hwhost $zkhost\n-\n-This will vary the number of outstanding publish requests as specified in\n-`npars`.\n-\n-You may also optionally run this experiment with a second subscribing region:\n-\n-  scripts/hw.bash start-zk-bks $zkhost $bk1host $bk2host $bk3host\n-  npars=\"...\" scripts/hw.bash pub-exps \"$host1 $host2 $host3\" $hwhost $zkhost $rrecv $rhwhost $rzkhost\n-\n-where the final three extra arguments specify the client receiver, Hedwig, and\n-Zookeeper hosts, in that order.\n-\n-This command will produce files into `./pub/`, which can then be process using\n-`analyze.py`.\n-\n-Analysis and Visualization\n-==========================\n-\n-`scripts/analyze.py` produces plots from the collected experimental data.  It\n-has just a few immediate dependencies. In the following, the\n-indentation signifies nested dependencies, like an upside-down tree:\n-\n-      component AAA that component AA requires\n-      component AAB that component AA requires\n-    component AA that component A requires\n-      component ABA that component AB requires\n-      component ABB that component AB requires\n-    component AB that component A requires\n-  component A that analysis tools depend on\n-      component BAA that component BA requires\n-      component BAB that component BA requires\n-    component BA that component B requires\n-      component BBA that component BB requires\n-      component BBB that component BB requires\n-    component BB that component B requires\n-  component B that analysis tools depend on\n-\n-The reason the tree is upside-down is so that you can treat this whole thing as\n-a chunk of bash script.\n-\n-[toast] is a utility that makes it a breeze to install all this software, but\n-you do need to make sure your environment is set up correctly (e.g.\n-`PKG_CONFIG_PATH` must point to `~/.toast/armed/lib/pkgconfig/`).\n-\n-Setup:\n-\n-  wget -O- http://toastball.net/toast/toast|perl -x - arm toast\n-\n-  toast arm \"http://www.python.org/ftp/python/2.6.2/Python-2.6.2.tar.bz2\"\n-\n-  toast arm numpy\n-\n-        toast arm libpng\n-\n-        toast arm pixman\n-\n-        toast arm freetype\n-\n-          toast arm 'ftp://xmlsoft.org/libxml2/libxml2-2.7.3.tar.gz'\n-\n-        toast arm fontconfig\n-\n-      toast arm cairo\n-\n-    toast arm pycairo\n-\n-  hg clone https://yang@bitbucket.org/yang/pycha/\n-  pycha/setup.bash -d -p $path_to_install_to\n-\n-  svn co https://assorted.svn.sourceforge.net/svnroot/assorted/python-commons/trunk/ python-commons/\n-  python-commons/setup.bash -d -p $path_to_install_to\n-\n-To analyze the publishing experiments, change to the `pub` data directory and\n-run:\n-\n-  scripts/analyze.py pub\n-\n-To analyze the subscription experiments, change to the `sub` data directory\n-and run:\n-\n-  scripts/analyze.py sub\n-\n-[toast]: http://toastball.net/toast/\n-\n-Debugging\n-=========\n-\n-You can attach an Eclipse debugger (or any debugger) to a Java process running\n-on a remote host, as long as it has been started with the appropriate JVM\n-flags.  (See the Building Hedwig document to set up your Eclipse environment.)\n-To launch something using `hw.bash` with debugger attachment enabled, prefix\n-the command with `attach=true`, e.g.:\n-\n-  attach=true scripts/hw.bash start-regions myregions.cfg\n-\n-Profiling\n-=========\n-\n-The scripts we have provided include ways for you to launch with YourKit\n-profiling enabled.\n-\n-To deploy YourKit onto a number of machines:\n-\n-  hosts=\"...\" scripts/hw.bash setup-yjp $path_to_yjp\n-\n-where the path points to the [YourKit Linux zip archive] (which is freely\n-available and doesn't require any license to use).\n-\n-Now when using the scripts to run distributed experiments, to profile anything\n-with YourKit, prefix the command with `use_yjp=true`.  E.g.:\n-\n-  use_yjp=true scripts/hw.bash start-regions regions.cfg\n-\n-Now you may start on your local machine the YourKit GUI and connect to the\n-hosts that you're interested in.\n-\n-Note that you may want to disable the default set of filters in YourKit.\n-\n-[YourKit Linux zip archive]: http://www.yourkit.com/download/yjp-8.0.15.zip\n-\n-Pseudocode\n-==========\n-\n-This summarizes the control flow through the system.\n-\n-  publishhandler\n-    topicmgr.getowner\n-      (maybe) claim the topic, calling back into persmgr.acquiredtopic\n-        read /hedwig/standalone/topics/TOPIC (which should initially be empty)\n-        for each line, parse as \"STARTSEQ\\tLEDGERID\" # TODO how is this written?\n-          ledger = bk.openledger(ledgerid)\n-          lastid = ledger.getlast\n-          if lastid > 0, lrs[startseq] = persmgr.ledger2lr[ledgerid] = new LedgerRange(ledger, ledgerid, startseq, startseq + lastid # TODO what are ledger ranges?\n-        create new ledger for topic\n-          # TODO read\n-          lr = new LedgerRange(ledger, ledgerid, lastid, -1)\n-          lrs[lastid] = lr\n-          persmgr.topic2ranges[topic] = lrs\n-    add region info to pub req and send that to persmgr.persistmessage\n-      entryid = persmgr.topic2ranges[topic].last.ledger.addentry(the pub'd data)\n-      update persmgr.topic2lastseq[topic]:\n-        .local = persmgr.ledger2lr[ledger id].startseq + entryid\n-        .regions = maxes of orig seq and incoming pub seq\n-\n-  subscribehandler\n-    topicmgr.getowner...\n-    delivmgr.startservingsubscription(topic, endpoint, ishubsubscriber)\n-      delivmgr.endpoint2sub[endpoint] = new subscriber(lastseq = persmgr.getcurrentseqidfortopic(topic).local)\n-      delivmgr.topic2ptr2subs[topic][ptr].add(sub)\n-      sub.delivernextmessage\n-        sub.curseq = persmgr.getseqidafterskipping(topic, sub.lastseq, skip = 1)\n-        msg = persmgr.scansinglemessage(topic, seq = sub.curseq)\n-          if persmgr.topic2lastseq[topic].local >= seq\n-            lr = persmgr.topic2ranges[topic].floor(seq)\n-            return lr.ledger.read(first = last = seq - lr.startseq)\n-        if failed, then retry in 1 s\n-        endpoint.send(msg)\n-        movedeliveryptr\n-          delivmgr.topic2ptr2subs[topic][sub.lastseq].remove(sub)\n-          delivmgr.topic2ptr2subs[topic][sub.curseq].add(sub)\n-        previd = sub.lastseq, sub.lastseq = sub.curseq\n-        sub.delivernextmessage...\n-\n-ReadAhead Cache\n-================\n-\n-The delivery manager class is responsible for pushing published messages from \n-the hubs to the subscribers. The most common case is that all subscribers are \n-connected and either caught up, or close to the tail end of the topic. In this \n-case, we don't want the delivery manager to be polling bookkeeper for any newly \n-arrived messages on the topic; new messages should just be pushed to the \n-delivery manager. However, there is also the uncommon case when a subscriber is \n-behind, and messages must be pulled from Bookkeeper.\n-\n-Since all publishes go through the hub, it is possible to cache the recently \n-published messages in the hub, and then the delivery manager won't have to make \n-the trip to bookkeeper to get the messages but instead get them from local \n-process memory.\n-\n-These ideas of push, pull, and caching are unified in the following way:\n-- A hub has a cache of messages\n-\n-- When the delivery manager wants to deliver a message, it asks the cache for \n-  it. There are 3 cases:\n-  - The message is available in the cache, in which case it is given to the \n-    delivery manager\n-  - The message is not present in the cache and the seq-id of the message is \n-    beyond the last message published on that topic (this happens if the \n-    subscriber is totally caught up for that topic). In this case, a stub is put \n-    in the cache in order to notify the delivery manager when that message does \n-    happen to be published.\n-  - The message is not in the cache but has been published to the topic. In this \n-    case, a stub is put in the cache, and a read is issued to bookkeeper.\n-\n-- Whenever a message is published, it is cached. If there is a stub already in \n-  the cache for that message, the delivery manager is notified. \n-\n-- Whenever a message is read from bookkeeper, it is cached. There must be a stub \n-  for that message (since reads to bookkeeper are issued only after putting a \n-  stub), so the delivery manager is notified. \n-\n-- The cache does readahead, i.e., if a message requested by the delivery manager \n-  is not in the cache, a stub is established not only for that message, but also \n-  for the next n messages where n is configurable (default 10). On a cache hit, \n-  we look ahead n/2 messages, and if that message is not present, we establish \n-  another n/2 stubs. In short, we always ensure that the next n stubs are always \n-  established.\n-\n-- Over time, the cache will grow in size. There are 2 pruning mechanisms:\n-  \n-  - Once all subscribers have consumed up to a particular seq-id, they notify \n-    the cache, and all messages up to that seq-id are pruned from the cache.\n-  - If the above pruning is not working (e.g., because some subscribers are \n-    down), the cache will eventually hit its size limit which is configurable  \n-    (default, half of maximum jvm heap size). At this point, messages are just \n-    pruned in FIFO order. We use the size of the blobs in the message for \n-    estimating the cache size. The assumption is that that size will dominate \n-    over fixed, object-level size overheads. \n-  - Stubs are not purged because according to the above simplification, they are \n-    of 0 size.\n-\n-Scalability Bottlenecks Down the Road\n-=====================================\n-\n-- Currently each topic subscription is served on a different channel. The number \n-  of channels will become a bottleneck at higher channels. We should switch to \n-  an architecture, where multiple topic subscriptions between the same client, \n-  hub pair should be served on the same channel. We can have commands to start, \n-  stop subscriptions sent all the way to the server (right now these are local).\n-- Publishes for a topic are serialized through a hub, to get ordering \n-  guarantees. Currently, all subscriptions to that topic are served from the \n-  same hub. If we start having large number of subscribers to heavy-volume \n-  topics, the outbound bandwidth at the hub, or the CPU at that hub might become \n-  the bottleneck. In that case, we can setup other regions through which the \n-  messages are routed (this hierarchical scheme) reduces bandwidth requirements \n-  at any single node. It should be possible to do this entirely through \n-  configuration.\n-"},{"sha":"c78231b705259c47fa938b70e391895206f1322e","filename":"doc/doc.textile","status":"added","additions":10,"deletions":0,"changes":10,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/doc.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/doc.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/doc.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,10 @@\n+In the documentation directory, you'll find:\n+\n+* @build.txt@: Building Hedwig, or how to set up Hedwig\n+* @user.txt@: User's Guide, or how to program against the Hedwig API and how to run it\n+* @dev.txt@: Developer's Guide, or Hedwig internals and hacking details\n+\n+These documents are all written in the \"Pandoc\":http://johnmacfarlane.net/pandoc/ dialect of \"Markdown\":http://daringfireball.net/projects/markdown/. This makes them readable as plain text files, but also capable of generating HTML or LaTeX documentation.\n+\n+Documents are wrapped at 80 chars and use 2-space indentation.\n+"},{"sha":"36f2c0bae97d7e3f270bf2740ecd9ddfe3444d4e","filename":"doc/doc.txt","status":"removed","additions":0,"deletions":17,"changes":17,"blob_url":"https://github.com/apache/bookkeeper/blob/267ed95a2ae2ef1a8f6a3166b217d86a28166391/doc/doc.txt","raw_url":"https://github.com/apache/bookkeeper/raw/267ed95a2ae2ef1a8f6a3166b217d86a28166391/doc/doc.txt","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/doc.txt?ref=267ed95a2ae2ef1a8f6a3166b217d86a28166391","patch":"@@ -1,17 +0,0 @@\n-% Meta-Documentation\n-\n-In the documentation directory, you'll find:\n-\n-- `build.txt`: Building Hedwig, or how to set up Hedwig\n-- `user.txt`: User's Guide, or how to program against the Hedwig API and how to\n-  run it\n-- `dev.txt`: Developer's Guide, or Hedwig internals and hacking details\n-\n-These documents are all written in the [Pandoc] dialect of [Markdown].  This\n-makes them readable as plain text files, but also capable of generating HTML or\n-LaTeX documentation.\n-\n-[Pandoc]: http://johnmacfarlane.net/pandoc/\n-[Markdown]: http://daringfireball.net/projects/markdown/\n-\n-Documents are wrapped at 80 chars and use 2-space indentation."},{"sha":"4a3d4546d0c122f308d02da9505e775663ada3ec","filename":"doc/hedwigBuild.textile","status":"added","additions":92,"deletions":0,"changes":92,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/hedwigBuild.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/hedwigBuild.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/hedwigBuild.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,92 @@\n+h1. Pre-requisites\n+\n+For the core itself:\n+\n+* JDK 6: \"http://java.sun.com/\":http://java.sun.com/. Ensure @$JAVA_HOME@ is correctly set.\n+* Maven 2: \"http://maven.apache.org/\":http://maven.apache.org/.\n+* Protocol Buffers 2.3.0: \"http://protobuf.googlecode.com/\":http://protobuf.googlecode.com/.\n+* Zookeeper 3.4.0: \"http://hadoop.apache.org/zookeeper/\":http://hadoop.apache.org/zookeeper/. See below.\n+* Bookkeeper 3.4.0: \"http://hadoop.apache.org/zookeeper/\":http://hadoop.apache.org/zookeeper/. See below.\n+\n+Hedwig has been tested on Windows XP, Linux 2.6, and OS X.\n+\n+For the deployment and distributed support scripts in @hw.bash@:\n+\n+* Ant: \"http://ant.apache.org/\":http://ant.apache.org/, if you want to build Zookeeper.\n+* Bash: \"http://www.gnu.org/software/bash/\":http://www.gnu.org/software/bash/.\n+* Coreutils: \"http://www.gnu.org/software/coreutils/\":http://www.gnu.org/software/coreutils/.\n+* Expect: \"http://expect.nist.gov/\":http://expect.nist.gov/, if you want @unbuffer@.\n+* Findutils: \"http://www.gnu.org/software/findutils/\":http://www.gnu.org/software/findutils/.\n+* OpenSSH: \"http://www.openssh.com/\":http://www.openssh.com/.\n+* Python 2.6: \"http://python.org/\":http://python.org/.\n+\n+h2. Protocol Buffers\n+\n+Hedwig requires the use of the Java runtime libraries of Protocol Buffers 2.3.0. These libraries need to be installed into your local maven repository. (Maven allows multiple versions to be installed.) To install protocol buffels to your local repository, you have to download the tarball and follow the README.txt instructions. Note that you must first install the C++ package which contains the compiler (protoc) before you can build the java libraries. That will install the library jar's in the local maven repository where Hedwig is currently configured to point to.\n+\n+h2. Zookeeper and Bookkeeper\n+\n+Hedwig currently requires the version of Bookkeeper maintained in Apache's current trunk SVN respository (version 3.4.0). This is not a released version yet but certain features needed for BookKeeper are only available there.\n+\n+Hedwig also depends on ZK testing code for its own testing code.\n+\n+Since Hedwig is a Maven project, all these dependencies must be made available as Maven artifacts. However, neither ZK nor BK are currently Mavenized. Hedwig provides some bash scripts to ease the installation of ZK, ZK tests, and BK, all as Maven artifacts.\n+\n+Currently, we have included the necessary ZooKeeper and BookKeeper jars in the Hedwig source itself in the $HEDWIG&#95;DIR/server/lib directory. There is no need to retrieve them directly from the Apache download site as they are non-released trunk versions.\n+\n+h1. Not relevant right now since we already have the ZK jars already in the Hedwig source.\n+\n+To fetch and build ZK 3.4.0 (and its tests) in the current directory, run:\n+\n+$HEDWIG&#95;DIR/scripts/hw.bash get-zk\n+\n+h1. Not relevant right now, but when we start using the apache version of BK, to\n+\n+build the local version of BK:\n+\n+$HEDWIG&#95;DIR/scripts/hw.bash get-bk\n+\n+The $HEDWIG&#95;DIR/server/lib directory contains all of the the class and source jars for ZK, ZK tests, and BK. To install these, go to that directory and run the following command to install them into your local maven repository:\n+\n+$HEDWIG&#95;DIR/scripts/hw.bash install-zk-bk\n+\n+h1. Command-Line Instructions\n+\n+From the main Hedwig directory, run @mvn package@. This will produce the executable jars for both the client and server, as well as a server &quot;assembly jar&quot; containing all dependencies as well for easier deployment.\n+\n+See the User's Guide for instructions on running and usage.\n+\n+h1. Eclipse Instructions\n+\n+To check out, build, and develop using Eclipse:\n+\n+# Install the Subclipse plugin. Update site: \"http://subclipse.tigris.org/update_1.4.x\":http://subclipse.tigris.org/update_1.4.x.\n+# Install the Maven plugin. Update site: \"http://m2eclipse.sonatype.org/update\":http://m2eclipse.sonatype.org/update. From the list of packages available from this site, select everything under the &quot;Maven Integration&quot; category, and from the optional components select the ones with the word &quot;SCM&quot; in them.\n+# Go to Preferences &gt; Team &gt; SVN. For the SVN interface, choose &quot;Pure Java&quot;.\n+# Choose File &gt; New &gt; Project... &gt; Maven &gt; Checkout Maven Projects from SCM.\n+# For the SCM URL type, choose SVN. For the URL, enter SVN URL. Maven will automatically create a top-level Eclipse project for each of the 4 Maven modules (recommended). If you want fewer top-level projects, uncheck the option of having a project for each module (under Advanced).\n+# Right-click on the @protocol@ project and choose Run As &gt; Maven generate-sources. This will generate the Java and C++ code for Protocol Buffers.\n+# Refresh the workspace to pick up the generated code and add @hedwig/protocol/target/generated-sources/java@ as a source folder. (6 &amp; 7 should really be doable automatically, but I haven't figured out how.)\n+\n+You are now ready to run and debug the client and server code. See the User's Guide for instructions on running and usage.\n+\n+h1. Utilities\n+\n+h2. Removing Conflicting Files in Jars\n+\n+The Maven assembly plugin that produces the fat assembly jar may end up putting into the jar files with the same conflicting paths from multiple dependencies. This makes working with the files from certain tools (like @jar@) a bit jarring. In our case, these files are not things like class files, but rather README and LICENSE files, so we can safely remove conflicts by choosing an arbitrary winner. To do so, run:\n+\n+$HEDWIG&#95;DIR/scripts/hw.bash strip-jar\n+\n+h2. Adjusting Logging\n+\n+The logging level is something that is baked into the jar in the @log4j.properties@ resource. However, it would be wasteful to go through a Maven build cycle to update and adjust this. If you're working from a source tree, it's also annoying to have to edit a source file to adjust the logging.\n+\n+We have a little script for tweaking the logging level. After running @strip-jar@, run:\n+\n+$HEDWIG&#95;DIR/scripts/hw.bash set-logging WARN\n+\n+To see what the current logging level is:\n+\n+$HEDWIG&#95;DIR/scripts/hw.bash get-logging\n+"},{"sha":"fdb7b8751d5a5b73a2c2889d4686fff20bf211e8","filename":"doc/hedwigDesign.textile","status":"added","additions":178,"deletions":0,"changes":178,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/hedwigDesign.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/hedwigDesign.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/hedwigDesign.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,178 @@\n+h1. Style\n+\n+We have provided an Eclipse Formatter file @formatter.xml@ with all the formatting conventions currently used in the project. Highlights include no tabs, 4-space indentation, and 120-char width. Please respect this so as to reduce the amount of formatting-related noise produced in commits.\n+\n+h1. Static Analysis\n+\n+We would like to use static analysis tools PMD and FindBugs to maintain code quality. However, we have not yet arrived at a consensus on what rules to adhere to, and what to ignore.\n+\n+h1. Netty Notes\n+\n+The asynchronous network IO infrastructure that Hedwig uses is \"Netty\":http://www.jboss.org/netty. Here are some notes on Netty's concurrency architecture and its filter pipeline design.\n+\n+h2. Concurrency Architecture\n+\n+After calling @ServerBootstrap.bind()@, Netty starts a boss thread (@NioServerSocketPipelineSink.Boss@) that just accepts new connections and registers them with one of the workers from the @NioWorker@ pool in round-robin fashion (pool size defaults to CPU count). Each worker runs its own select loop over just the set of keys that have been registered with it. Workers start lazily on demand and run only so long as there are interested fd's/keys. All selected events are handled in the same thread and sent up the pipeline attached to the channel (this association is established by the boss as soon as a new connection is accepted).\n+\n+All workers, and the boss, run via the executor thread pool; hence, the executor must support at least two simultaneous threads.\n+\n+h2. Handler Pipeline\n+\n+A pipeline implements the intercepting filter pattern. A pipeline is a sequence of handlers. Whenever a packet is read from the wire, it travels up the stream, stopping at each handler that can handle upstream events. Vice-versa for writes. Between each filter, control flows back through the centralized pipeline, and a linked list of contexts keeps track of where we are in the pipeline (one context object per handler).\n+\n+h1. Distributed Performance Evaluation\n+\n+We've included some scripts to repeatedly run varying configurations of Hedwig on a distributed testbed and collect the resulting data. The experiments use the @org.apache.hedwig.client.App@ client application and are driven by @scripts/hw.bash@ (via the @app@ command).\n+\n+Currently, we have two types of experiments: subscription benchmarks and publishing benchmarks.\n+\n+h2. Subscription Benchmarks\n+\n+The subscription benchmark compares synchronous and asynchronous subscriptions. Because the synchronicity of subscriptions is a server configuration parameter, the servers must be restarted to change this. The benchmarks varies the maximum number of outstanding subscription requests.\n+\n+To run the subscription benchmark with wilbur6 as the subscriber and wilbur1 as its default hub:\n+\n+hosts=wilbur6 scripts/hw.bash sub-exp wilbur1\n+\n+This produces log files into the @sub@ directory, which may then be analyzed using the analysis scripts.\n+\n+h2. Publishing Benchmarks\n+\n+The publishing benchmark measures the throughput and latency of publishing messages within a LAN and across a WAN. It varies the following parameters:\n+\n+* maximum number of outstanding publish requests\n+* number of publishers\n+* number of (local) receivers\n+\n+We vary each dimension separately (and have default settings) to avoid a combinatorial explosion in the number of configurations to explore.\n+\n+First, start a (standalone) instance:\n+\n+scripts/hw.bash start-region '' $hwhost $zkhost $bk1host $bk2host $bk3host\n+\n+To run this over @$host1@ through @$host3@, with the number of publishers/subscribers varying linearly over this set:\n+\n+npars=&quot;20 40 60 80 100&quot; scripts/hw.bash pub-exps &quot;$host1 $host2 $host3&quot; $hwhost $zkhost\n+\n+This will vary the number of outstanding publish requests as specified in @npars@.\n+\n+You may also optionally run this experiment with a second subscribing region:\n+\n+scripts/hw.bash start-zk-bks $zkhost $bk1host $bk2host <span class=\"math\">bk3host npars=&quot;...&quot; scripts/hw.bash pub-exps &quot;</math>host1 $host2 $host3&quot; $hwhost $zkhost $rrecv $rhwhost $rzkhost\n+\n+where the final three extra arguments specify the client receiver, Hedwig, and Zookeeper hosts, in that order.\n+\n+This command will produce files into @./pub/@, which can then be process using @analyze.py@.\n+\n+h1. Analysis and Visualization\n+\n+@scripts/analyze.py@ produces plots from the collected experimental data. It has just a few immediate dependencies. In the following, the indentation signifies nested dependencies, like an upside-down tree:\n+\n+bc.   component AAA that component AA requires\n+  component AAB that component AA requires\n+component AA that component A requires\n+  component ABA that component AB requires\n+  component ABB that component AB requires\n+component AB that component A requires\n+\n+\n+component A that analysis tools depend on component BAA that component BA requires component BAB that component BA requires component BA that component B requires component BBA that component BB requires component BBB that component BB requires component BB that component B requires component B that analysis tools depend on\n+\n+The reason the tree is upside-down is so that you can treat this whole thing as a chunk of bash script.\n+\n+\"toast\":http://toastball.net/toast/ is a utility that makes it a breeze to install all this software, but you do need to make sure your environment is set up correctly (e.g. @PKG_CONFIG_PATH@ must point to @~/.toast/armed/lib/pkgconfig/@).\n+\n+Setup:\n+\n+wget -O- http://toastball.net/toast/toast&#124;perl -x - arm toast\n+\n+toast arm &quot;http://www.python.org/ftp/python/2.6.2/Python-2.6.2.tar.bz2&quot;\n+\n+toast arm numpy\n+\n+<pre>\n+    toast arm libpng\n+\n+    toast arm pixman\n+\n+    toast arm freetype\n+\n+      toast arm 'ftp://xmlsoft.org/libxml2/libxml2-2.7.3.tar.gz'\n+\n+    toast arm fontconfig\n+\n+  toast arm cairo\n+\n+toast arm pycairo\n+</pre>\n+\n+hg clone https://yang&#64;bitbucket.org/yang/pycha/ pycha/setup.bash -d -p $path&#95;to&#95;install&#95;to\n+\n+svn co https://assorted.svn.sourceforge.net/svnroot/assorted/python-commons/trunk/ python-commons/ python-commons/setup.bash -d -p $path&#95;to&#95;install&#95;to\n+\n+To analyze the publishing experiments, change to the @pub@ data directory and run:\n+\n+scripts/analyze.py pub\n+\n+To analyze the subscription experiments, change to the @sub@ data directory and run:\n+\n+scripts/analyze.py sub\n+\n+h1. Debugging\n+\n+You can attach an Eclipse debugger (or any debugger) to a Java process running on a remote host, as long as it has been started with the appropriate JVM flags. (See the Building Hedwig document to set up your Eclipse environment.) To launch something using @hw.bash@ with debugger attachment enabled, prefix the command with @attach=true@, e.g.:\n+\n+attach=true scripts/hw.bash start-regions myregions.cfg\n+\n+h1. Profiling\n+\n+The scripts we have provided include ways for you to launch with YourKit profiling enabled.\n+\n+To deploy YourKit onto a number of machines:\n+\n+hosts=&quot;...&quot; scripts/hw.bash setup-yjp $path&#95;to&#95;yjp\n+\n+where the path points to the \"YourKit Linux zip archive\":http://www.yourkit.com/download/yjp-8.0.15.zip (which is freely available and doesn't require any license to use).\n+\n+Now when using the scripts to run distributed experiments, to profile anything with YourKit, prefix the command with @use_yjp=true@. E.g.:\n+\n+use&#95;yjp=true scripts/hw.bash start-regions regions.cfg\n+\n+Now you may start on your local machine the YourKit GUI and connect to the hosts that you're interested in.\n+\n+Note that you may want to disable the default set of filters in YourKit.\n+\n+h1. Pseudocode\n+\n+This summarizes the control flow through the system.\n+\n+publishhandler topicmgr.getowner (maybe) claim the topic, calling back into persmgr.acquiredtopic read /hedwig/standalone/topics/TOPIC (which should initially be empty) for each line, parse as &quot;STARTSEQ&quot; # TODO how is this written? ledger = bk.openledger(ledgerid) lastid = ledger.getlast if lastid &gt; 0, lrs[startseq] = persmgr.ledger2lr[ledgerid] = new LedgerRange(ledger, ledgerid, startseq, startseq + lastid # TODO what are ledger ranges? create new ledger for topic # TODO read lr = new LedgerRange(ledger, ledgerid, lastid, -1) lrs[lastid] = lr persmgr.topic2ranges[topic] = lrs add region info to pub req and send that to persmgr.persistmessage entryid = persmgr.topic2ranges[topic].last.ledger.addentry(the pub'd data) update persmgr.topic2lastseq[topic]: .local = persmgr.ledger2lr[ledger id].startseq + entryid .regions = maxes of orig seq and incoming pub seq\n+\n+subscribehandler topicmgr.getowner... delivmgr.startservingsubscription(topic, endpoint, ishubsubscriber) delivmgr.endpoint2sub[endpoint] = new subscriber(lastseq = persmgr.getcurrentseqidfortopic(topic).local) delivmgr.topic2ptr2subs[topic][ptr].add(sub) sub.delivernextmessage sub.curseq = persmgr.getseqidafterskipping(topic, sub.lastseq, skip = 1) msg = persmgr.scansinglemessage(topic, seq = sub.curseq) if persmgr.topic2lastseq[topic].local &gt;= seq lr = persmgr.topic2ranges[topic].floor(seq) return lr.ledger.read(first = last = seq - lr.startseq) if failed, then retry in 1 s endpoint.send(msg) movedeliveryptr delivmgr.topic2ptr2subs[topic][sub.lastseq].remove(sub) delivmgr.topic2ptr2subs[topic][sub.curseq].add(sub) previd = sub.lastseq, sub.lastseq = sub.curseq sub.delivernextmessage...\n+\n+h1. ReadAhead Cache\n+\n+The delivery manager class is responsible for pushing published messages from the hubs to the subscribers. The most common case is that all subscribers are connected and either caught up, or close to the tail end of the topic. In this case, we don't want the delivery manager to be polling bookkeeper for any newly arrived messages on the topic; new messages should just be pushed to the delivery manager. However, there is also the uncommon case when a subscriber is behind, and messages must be pulled from Bookkeeper.\n+\n+Since all publishes go through the hub, it is possible to cache the recently published messages in the hub, and then the delivery manager won't have to make the trip to bookkeeper to get the messages but instead get them from local process memory.\n+\n+These ideas of push, pull, and caching are unified in the following way: - A hub has a cache of messages\n+\n+* When the delivery manager wants to deliver a message, it asks the cache for it. There are 3 cases:\n+* The message is available in the cache, in which case it is given to the delivery manager\n+* The message is not present in the cache and the seq-id of the message is beyond the last message published on that topic (this happens if the subscriber is totally caught up for that topic). In this case, a stub is put in the cache in order to notify the delivery manager when that message does happen to be published.\n+* The message is not in the cache but has been published to the topic. In this case, a stub is put in the cache, and a read is issued to bookkeeper.\n+* Whenever a message is published, it is cached. If there is a stub already in the cache for that message, the delivery manager is notified.\n+* Whenever a message is read from bookkeeper, it is cached. There must be a stub for that message (since reads to bookkeeper are issued only after putting a stub), so the delivery manager is notified.\n+* The cache does readahead, i.e., if a message requested by the delivery manager is not in the cache, a stub is established not only for that message, but also for the next n messages where n is configurable (default 10). On a cache hit, we look ahead n/2 messages, and if that message is not present, we establish another n/2 stubs. In short, we always ensure that the next n stubs are always established.\n+* Over time, the cache will grow in size. There are 2 pruning mechanisms:\n+* Once all subscribers have consumed up to a particular seq-id, they notify the cache, and all messages up to that seq-id are pruned from the cache.\n+* If the above pruning is not working (e.g., because some subscribers are down), the cache will eventually hit its size limit which is configurable\n+ (default, half of maximum jvm heap size). At this point, messages are just pruned in FIFO order. We use the size of the blobs in the message for estimating the cache size. The assumption is that that size will dominate over fixed, object-level size overheads.\n+* Stubs are not purged because according to the above simplification, they are of 0 size.\n+\n+h1. Scalability Bottlenecks Down the Road\n+\n+* Currently each topic subscription is served on a different channel. The number of channels will become a bottleneck at higher channels. We should switch to an architecture, where multiple topic subscriptions between the same client, hub pair should be served on the same channel. We can have commands to start, stop subscriptions sent all the way to the server (right now these are local).\n+* Publishes for a topic are serialized through a hub, to get ordering guarantees. Currently, all subscriptions to that topic are served from the same hub. If we start having large number of subscribers to heavy-volume topics, the outbound bandwidth at the hub, or the CPU at that hub might become the bottleneck. In that case, we can setup other regions through which the messages are routed (this hierarchical scheme) reduces bandwidth requirements at any single node. It should be possible to do this entirely through configuration.\n+"},{"sha":"532e9570dfae48981e545e7d90942db7184fbe27","filename":"doc/hedwigUser.textile","status":"added","additions":157,"deletions":0,"changes":157,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/hedwigUser.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/hedwigUser.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/hedwigUser.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,157 @@\n+h1. Design\n+\n+In Hedwig, clients publish messages associated with a topic, and they subscribe to a topic to receive all messages published with that topic. Clients are associated with (publish to and subscribe from) a Hedwig _instance_ (also referred to as a _region_), which consists of a number of servers called _hubs_. The hubs partition up topic ownership among themselves, and all publishes and subscribes to a topic must be done to its owning hub. When a client doesn't know the owning hub, it tries a default hub, which may redirect the client.\n+\n+Running a Hedwig instance requires a Zookeeper server and at least three Bookkeeper servers.\n+\n+An instance is designed to run within a datacenter. For wide-area messaging across datacenters, specify in the server configuration the set of default servers for each of the other instances. Dissemination among instances currently takes place over an all-to-all topology. Local subscriptions cause the hub to subscribe to all other regions on this topic, so that the local region receives all updates to it. Future work includes allowing the user to overlay alternative topologies.\n+\n+Because all messages on a topic go through a single hub per region, all messages within a region are ordered. This means that, for a given topic, messages are delivered in the same order to all subscribers within a region, and messages from any particular region are delivered in the same order to all subscribers globally, but messages from different regions may be delivered in different orders to different regions. Providing global ordering is prohibitively expensive in the wide area. However, in Hedwig clients such as PNUTS, the lack of global ordering is not a problem, as PNUTS serializes all updates to a table row at a single designated master for that row.\n+\n+Topics are independent; Hedwig provides no ordering across different topics.\n+\n+Version vectors are associated with each topic and serve as the identifiers for each message. Vectors consist of one component per region. A component value is the region's local sequence number on the topic, and is incremented each time a hub persists a message (published either locally or remotely) to BK.\n+\n+TODO: More on how version vectors are to be used, and on maintaining vector-maxes.\n+\n+h1. Entry Points\n+\n+The main class for running the server is @org.apache.hedwig.server.netty.PubSubServer@. It takes a single argument, which is a \"Commons Configuration\":http://commons.apache.org/configuration/ file. Currently, for configuration, the source is the documentation. See @org.apache.hedwig.server.conf.ServerConfiguration@ for server configuration parameters.\n+\n+The client is a library intended to be consumed by user applications. It takes a Commons Configuration object, for which the source/documentation is in @org.apache.hedwig.client.conf.ClientConfiguration@.\n+\n+We have provided a simple client application, @org.apache.hedwig.client.App@, that can drive a number of benchmarks. This also takes a single configuration file argument, which is fed to the client library.\n+\n+We've provided a number of scripts to faciliate running servers and clients in a variety of configurations, including over distributed hosts. These are all consolidated in @scripts/hw.bash@. Although the snippets in this documentation run the script from the hedwig main directory, you can run it from any location. Apologies in advance for these being bash scripts; time permitting, a more robust and maintainable support/tooling infrastructure would be ideal.\n+\n+h1. Deployment\n+\n+When ssh-ing into a new host, you are requested to verify and accept the host key. In order to automatically accept the host keys for many new hosts (dangerous), use:\n+\n+hosts=&quot;$host1 $host2 ...&quot; scripts/hw.bash warmup\n+\n+The @hosts@ variable is set here to the list of hosts that you would like to warm up.\n+\n+To set up JDK6 on some hosts, use:\n+\n+hosts=&quot;...&quot; scripts/hw.bash setup-java $path&#95;to&#95;modified&#95;jdk6\n+\n+The argument must point to a JDK6 binary self-extracting executable, but with the @more@ command that displays the License agreement replaced with @cat@. Unfortunately, this step must be performed manually. This script will extract the JDK directly into the home directory and update @$PATH@ in @~/.bashrc@ (in an idempotent fashion).\n+\n+Because the current implementation uses a single socket per subscription, the Hedwig launching scripts all require a high @ulimit@ on the number of open file descriptors. Non-root users can only use up to the limit specified in @/etc/security/limits.conf@; to raise this to 1024^2, run:\n+\n+hosts=&quot;...&quot; scripts/hw.bash setup-limits\n+\n+This uses @ssh@ so that you need to enter your password for @sudo@ just once.\n+\n+For most of the commands presented in the next section, you may prefix the command with:\n+\n+push&#95;jar=true ...\n+\n+to first push the assembly jar (assumed to be available in @server/target/@) to all hosts.\n+\n+h1. Running Servers\n+\n+To start three BK bookies on ports 3181-3183 on localhost (directories must all exist):\n+\n+scripts/hw.bash bk 3181 $bk1&#95;journal&#95;dir $bk1&#95;ledger&#95;dir &amp; scripts/hw.bash bk 3182 $bk2&#95;journal&#95;dir $bk2&#95;ledger&#95;dir &amp; scripts/hw.bash bk 3183 $bk3&#95;journal&#95;dir $bk3&#95;ledger&#95;dir &amp;\n+\n+To start a ZK on port 2181 (directory must exist):\n+\n+scripts/hw.bash zk 2181 /path/for/zk/ &amp;\n+\n+To register the BKs with the ZK (so that Hedwig knows where to find the bookies):\n+\n+scripts/hw.bash setup-bk localhost:2181 @hostname@:3181 @hostname@:3182 @hostname@:3183\n+\n+Everything up to this point may be done using a single command over a set of hosts, with ZK on port 9877 and BK on port 9878. The following function takes 2 arguments. The first is the ZK host. The second is a string list of BK hosts:\n+\n+scripts/hw.bash start-zk-bks <span class=\"math\">zkhost &quot;</math>bk1host $bk2host $bk3host ...&quot;\n+\n+Note that the hosts may be SSH profile aliases in your @~/.ssh/config@; the script will parse this file and look up their hostnames where necessary. This applies for the hosts specified in the other commands.\n+\n+Also, the scripts use the @bk-journal@ and @bk-ledger@ functions in @hw.bash@ to determine where to place the BK journal and ledger, given a hostname.\n+\n+To start a Hedwig server locally:\n+\n+scripts/hw.bash hw server.conf &amp;\n+\n+To start Hedwig servers on some hosts &quot;$hw1host $hw2host $hw3host ...&quot; on port 9876, using $zkhost as the ZK server:\n+\n+scripts/hw.bash start-hw '' &quot;$hw1host $hw2host $hw3host ...&quot; $zkhost\n+\n+Above, the first empty string argument is the list of default servers to each of the other regions. You may run multiple connected instances of Hedwig this way.\n+E.g., to start three regions each with a single Hedwig hub that talk to each other, and using the hw.bash default server ports of 9875 (non-SSL) and 9876 (SSL):\n+\n+scripts/hw.bash start-hw &quot;$hw2host:9875:9876 <span class=\"math\">hw3host:9875:9876&quot; &quot;</math>hw1host&quot; <span class=\"math\">zk1host scripts/hw.bash start-hw &quot;</math>hw1host:9875:9876 <span class=\"math\">hw3host:9875:9876&quot; &quot;</math>hw2host&quot; <span class=\"math\">zk2host scripts/hw.bash start-hw &quot;</math>hw1host:9875:9876 <span class=\"math\">hw2host:9875:9876&quot; &quot;</math>hw3host&quot; $zk3host\n+\n+Everything up to this point may be done using a single command over a set of hosts:\n+\n+scripts/hw.bash start-region '' &quot;$hw1host $hw2host $hw3host ...&quot; <span class=\"math\">zkhost &quot;</math>bk1host $bk2host $bk3host ...&quot;\n+\n+The first three arguments are the same as for @start-hw@.\n+\n+You may start multiple regions as well:\n+\n+scripts/hw.bash start-regions regions.cfg\n+\n+&quot;regions.cfg&quot; is a list of all regions, one per line, with each region having the following format:\n+\n+region=<Region name>, hub=<list of hub servers>, default=<single hub server>, zk=<single ZK server>, bk=<list of BK servers>\n+\n+This will create all of the regions with an all-to-all topology. Each region is connected to the default hub server of every other region. The &quot;, &quot; delimiter is used to separate out the different parts of a region along with the hard-coded parameter names. There also needs to be a newline after the last region line. Here is an example file specifying three regions:\n+\n+region=wilbur, hub=wilbur90 wilbur91, default=wilbur90, zk=wilbur93, bk=wilbur93 wilbur94 wilbur95 region=re1, hub=sherpa7 sherpa8, default=sherpa7, zk=sherpa9, bk=sherpa9 sherpa10 sherpa11 region=peanuts, hub=peanuts1 peanuts2, default=peanuts2, zk=peanuts3, bk=peanuts3 peanuts4 peanuts5\n+\n+h1. Running the Client\n+\n+To run the test client:\n+\n+JAVAFLAGS=&quot;...&quot; scripts/hw.bash hwc $conf&#95;path\n+\n+where @$conf_path@ is a client configuration file.\n+\n+To run the test client on some other hosts:\n+\n+hosts=&quot;...&quot; JAVAFLAGS=&quot;...&quot; scripts/hw.bash app $hwhost\n+\n+This will generate a simple configuration file assuming $hwhost is listening on the default SSL and non-SSL ports which are specified as global variables in hw.bash. Currently these are 9875 for non-SSL and 9876 for SSL.\n+\n+Client usage is currently documented in the source. To run a subscription benchmark, set @JAVAFLAGS@ to:\n+\n+-Dmode=sub -Dcount=10000 -Dnpar=100 -Dstart=5 -Dnwarmups=30\n+\n+This will first create 30 warm-up subscriptions to topics &quot;warmup-5&quot; through &quot;warmup-34&quot;, then 10,000 benchmarked subscriptions to topics &quot;topic-5&quot; through &quot;topic-10,004&quot;. It will have a pipeline depth of 100 requests, meaning that there will be at most 100 outstanding (unresponded) messages in flight at any moment.\n+\n+To run a publishing benchmark, set @JAVAFLAGS@ to:\n+\n+-Dmode=pub -Dcount=10000 -Dnpar=100 -Dstart=5\n+\n+This will publish 10,000 messages to topic &quot;topic-5&quot;, with a pipeline depth of 100 requests.\n+\n+At the end, the programs will print throughput and latency information.\n+\n+h1. Utilities\n+\n+To kill all the user's Java processes running on some machines, use:\n+\n+hosts=&quot;...&quot; scripts/hw.bash dkill\n+\n+To check if any processes are running and are using ports of interest (esp. 9876-9878):\n+\n+hosts=&quot;...&quot; scripts/hw.bash dstatus\n+\n+Add an argument to @dstatus@ (may be anything) to get a more detailed listing.\n+\n+To check if there's anything consuming the CPU on some machines:\n+\n+hosts=&quot;...&quot; scripts/hw.bash tops\n+\n+To run an arbitrary command on multiple hosts in parallel:\n+\n+hosts=&quot;...&quot; scripts/hw.bash parssh $command\n+\n+To do this in sequence:\n+\n+hosts=&quot;...&quot; xargs= scripts/hw.bash parssh $command\n+"},{"sha":"a3138da00416f1cab613b7775e0ef0a07d04ab25","filename":"doc/index.textile","status":"added","additions":32,"deletions":0,"changes":32,"blob_url":"https://github.com/apache/bookkeeper/blob/d20cb28943e80c1d39cca5916f459651e29dd573/doc/index.textile","raw_url":"https://github.com/apache/bookkeeper/raw/d20cb28943e80c1d39cca5916f459651e29dd573/doc/index.textile","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/index.textile?ref=d20cb28943e80c1d39cca5916f459651e29dd573","patch":"@@ -0,0 +1,32 @@\n+Title:     BookKeeper Documentation\n+Notice:    Licensed to the Apache Software Foundation (ASF) under one\n+           or more contributor license agreements.  See the NOTICE file\n+           distributed with this work for additional information\n+           regarding copyright ownership.  The ASF licenses this file\n+           to you under the Apache License, Version 2.0 (the\n+           \"License\"); you may not use this file except in compliance\n+           with the License.  You may obtain a copy of the License at\n+           .\n+             http://www.apache.org/licenses/LICENSE-2.0\n+           .\n+           Unless required by applicable law or agreed to in writing,\n+           software distributed under the License is distributed on an\n+           \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+           KIND, either express or implied.  See the License for the\n+           specific language governing permissions and limitations\n+           under the License.\n+\n+h1. Apache BookKeeper documentation\n+\n+* \"Overview\":./bookkeeperOverview.html\n+* \"Getting started\":./bookkeeperStarted.html\n+* \"Programmer's Guide\":./bookkeeperProgrammer.html\n+* \"Admin Guide\":./bookkeeperConfig.html\n+* \"Using BookKeeper stream library\":./bookkeeperStream.html\n+\n+h1. Apache Hedwig documentation\n+\n+* \"Building Hedwig, or how to set up Hedwig\":./hedwigBuild.html\n+* \"User's Guide, or how to program against the Hedwig API and how to run it\":./hedwigUser.html\n+* \"Developer's Guide, or Hedwig internals and hacking details\":./hedwigDesign.html\n+"},{"sha":"242e97695ec1f629aab7216aae980da0a153d7df","filename":"doc/user.txt","status":"removed","additions":0,"deletions":252,"changes":252,"blob_url":"https://github.com/apache/bookkeeper/blob/267ed95a2ae2ef1a8f6a3166b217d86a28166391/doc/user.txt","raw_url":"https://github.com/apache/bookkeeper/raw/267ed95a2ae2ef1a8f6a3166b217d86a28166391/doc/user.txt","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/doc/user.txt?ref=267ed95a2ae2ef1a8f6a3166b217d86a28166391","patch":"@@ -1,252 +0,0 @@\n-% User's Guide\n-% Yang Zhang\n-\n-Design\n-======\n-\n-In Hedwig, clients publish messages associated with a topic, and they subscribe\n-to a topic to receive all messages published with that topic.  Clients are\n-associated with (publish to and subscribe from) a Hedwig _instance_ (also\n-referred to as a _region_), which consists of a number of servers called\n-_hubs_.  The hubs partition up topic ownership among themselves, and all\n-publishes and subscribes to a topic must be done to its owning hub.  When a\n-client doesn't know the owning hub, it tries a default hub, which may redirect\n-the client.\n-\n-Running a Hedwig instance requires a Zookeeper server and at least three\n-Bookkeeper servers.\n-\n-An instance is designed to run within a datacenter.  For wide-area messaging\n-across datacenters, specify in the server configuration the set of default\n-servers for each of the other instances.  Dissemination among instances\n-currently takes place over an all-to-all topology.  Local subscriptions cause\n-the hub to subscribe to all other regions on this topic, so that the local\n-region receives all updates to it.  Future work includes allowing the user to\n-overlay alternative topologies.\n-\n-Because all messages on a topic go through a single hub per region, all\n-messages within a region are ordered.  This means that, for a given topic,\n-messages are delivered in the same order to all subscribers within a region,\n-and messages from any particular region are delivered in the same order to all\n-subscribers globally, but messages from different regions may be delivered in\n-different orders to different regions.  Providing global ordering is\n-prohibitively expensive in the wide area.  However, in Hedwig clients such as\n-PNUTS, the lack of global ordering is not a problem, as PNUTS serializes all\n-updates to a table row at a single designated master for that row.\n-\n-Topics are independent; Hedwig provides no ordering across different topics.\n-\n-Version vectors are associated with each topic and serve as the identifiers for\n-each message.  Vectors consist of one component per region.  A component value\n-is the region's local sequence number on the topic, and is incremented each\n-time a hub persists a message (published either locally or remotely) to BK.\n-\n-TODO: More on how version vectors are to be used, and on maintaining\n-vector-maxes.\n-\n-Entry Points\n-============\n-\n-The main class for running the server is\n-`org.apache.hedwig.server.netty.PubSubServer`.  It takes a single argument,\n-which is a [Commons Configuration] file.  Currently, for configuration, the\n-source is the documentation.  See\n-`org.apache.hedwig.server.conf.ServerConfiguration` for server configuration\n-parameters.\n-\n-The client is a library intended to be consumed by user applications.  It takes\n-a Commons Configuration object, for which the source/documentation is in\n-`org.apache.hedwig.client.conf.ClientConfiguration`.\n-\n-We have provided a simple client application, `org.apache.hedwig.client.App`,\n-that can drive a number of benchmarks.  This also takes a single configuration\n-file argument, which is fed to the client library.\n-\n-We've provided a number of scripts to faciliate running servers and clients\n-in a variety of configurations, including over distributed hosts.  These are\n-all consolidated in `scripts/hw.bash`.  Although the snippets in this\n-documentation run the script from the hedwig main directory, you can run it\n-from any location.  Apologies in advance for these being bash scripts; time\n-permitting, a more robust and maintainable support/tooling infrastructure would\n-be ideal.\n-\n-[Commons Configuration]: http://commons.apache.org/configuration/\n-\n-Deployment\n-==========\n-\n-When ssh-ing into a new host, you are requested to verify and accept the host\n-key.  In order to automatically accept the host keys for many new hosts\n-(dangerous), use:\n-\n-  hosts=\"$host1 $host2 ...\" scripts/hw.bash warmup\n-\n-The `hosts` variable is set here to the list of hosts that you would like to\n-warm up.\n-\n-To set up JDK6 on some hosts, use:\n-\n-  hosts=\"...\" scripts/hw.bash setup-java $path_to_modified_jdk6\n-\n-The argument must point to a JDK6 binary self-extracting executable, but with\n-the `more` command that displays the License agreement replaced with\n-`cat`.  Unfortunately, this step must be performed manually.  This script will\n-extract the JDK directly into the home directory and update `$PATH` in\n-`~/.bashrc` (in an idempotent fashion).\n-\n-Because the current implementation uses a single socket per subscription, the\n-Hedwig launching scripts all require a high `ulimit` on the number of open file\n-descriptors.  Non-root users can only use up to the limit specified in\n-`/etc/security/limits.conf`; to raise this to 1024^2, run:\n-\n-  hosts=\"...\" scripts/hw.bash setup-limits\n-\n-This uses `ssh` so that you need to enter your password for `sudo` just\n-once.\n-\n-For most of the commands presented in the next section, you may prefix the\n-command with:\n-\n-  push_jar=true ...\n-\n-to first push the assembly jar (assumed to be available in `server/target/`) to\n-all hosts.\n-\n-Running Servers\n-===============\n-\n-To start three BK bookies on ports 3181-3183 on localhost (directories must all\n-exist):\n-\n-  scripts/hw.bash bk 3181 $bk1_journal_dir $bk1_ledger_dir &\n-  scripts/hw.bash bk 3182 $bk2_journal_dir $bk2_ledger_dir &\n-  scripts/hw.bash bk 3183 $bk3_journal_dir $bk3_ledger_dir &\n-\n-To start a ZK on port 2181 (directory must exist):\n-\n-  scripts/hw.bash zk 2181 /path/for/zk/ &\n-\n-To register the BKs with the ZK (so that Hedwig knows where to find the\n-bookies):\n-\n-  scripts/hw.bash setup-bk localhost:2181 `hostname`:3181 `hostname`:3182 `hostname`:3183\n-\n-Everything up to this point may be done using a single command over a set of\n-hosts, with ZK on port 9877 and BK on port 9878. The following function takes\n-2 arguments.  The first is the ZK host.  The second is a string list of BK hosts:\n-\n-  scripts/hw.bash start-zk-bks $zkhost \"$bk1host $bk2host $bk3host ...\"\n-\n-Note that the hosts may be SSH profile aliases in your `~/.ssh/config`; the\n-script will parse this file and look up their hostnames where necessary.  This\n-applies for the hosts specified in the other commands.\n-\n-Also, the scripts use the `bk-journal` and `bk-ledger` functions in `hw.bash`\n-to determine where to place the BK journal and ledger, given a hostname.\n-\n-To start a Hedwig server locally:\n-\n-  scripts/hw.bash hw server.conf &\n-\n-To start Hedwig servers on some hosts \"$hw1host $hw2host $hw3host ...\" on port 9876, \n-using $zkhost as the ZK server:\n-\n-  scripts/hw.bash start-hw '' \"$hw1host $hw2host $hw3host ...\" $zkhost\n-\n-Above, the first empty string argument is the list of default servers to each \n-of the other regions.  You may run multiple connected instances of Hedwig this way.  \n-E.g., to start three regions each with a single Hedwig hub that talk to each other,\n-and using the hw.bash default server ports of 9875 (non-SSL) and 9876 (SSL):\n-\n-  scripts/hw.bash start-hw \"$hw2host:9875:9876 $hw3host:9875:9876\" \"$hw1host\" $zk1host\n-  scripts/hw.bash start-hw \"$hw1host:9875:9876 $hw3host:9875:9876\" \"$hw2host\" $zk2host\n-  scripts/hw.bash start-hw \"$hw1host:9875:9876 $hw2host:9875:9876\" \"$hw3host\" $zk3host\n-\n-Everything up to this point may be done using a single command over a set of\n-hosts:\n-\n-  scripts/hw.bash start-region '' \"$hw1host $hw2host $hw3host ...\" $zkhost \"$bk1host $bk2host $bk3host ...\"\n-\n-The first three arguments are the same as for `start-hw`.\n-\n-You may start multiple regions as well:\n-\n-  scripts/hw.bash start-regions regions.cfg\n-\n-\"regions.cfg\" is a list of all regions, one per line, with each region having \n-the following format:\n-\n-  region=<Region name>, hub=<list of hub servers>, default=<single hub server>, zk=<single ZK server>, bk=<list of BK servers>\n-\n-This will create all of the regions with an all-to-all topology. Each region \n-is connected to the default hub server of every other region. The \", \" delimiter\n-is used to separate out the different parts of a region along with the hard-coded \n-parameter names. There also needs to be a newline after the last region line.\n-Here is an example file specifying three regions:\n-\n-  region=wilbur, hub=wilbur90 wilbur91, default=wilbur90, zk=wilbur93, bk=wilbur93 wilbur94 wilbur95\n-  region=re1, hub=sherpa7 sherpa8, default=sherpa7, zk=sherpa9, bk=sherpa9 sherpa10 sherpa11\n-  region=peanuts, hub=peanuts1 peanuts2, default=peanuts2, zk=peanuts3, bk=peanuts3 peanuts4 peanuts5\n-\n-Running the Client\n-==================\n-\n-To run the test client:\n-\n-  JAVAFLAGS=\"...\" scripts/hw.bash hwc $conf_path\n-\n-where `$conf_path` is a client configuration file.\n-\n-To run the test client on some other hosts:\n-\n-  hosts=\"...\" JAVAFLAGS=\"...\" scripts/hw.bash app $hwhost\n-\n-This will generate a simple configuration file assuming $hwhost is listening on\n-the default SSL and non-SSL ports which are specified as global variables in hw.bash.\n-Currently these are 9875 for non-SSL and 9876 for SSL.\n-\n-Client usage is currently documented in the source.  To run a subscription\n-benchmark, set `JAVAFLAGS` to:\n-\n-  -Dmode=sub -Dcount=10000 -Dnpar=100 -Dstart=5 -Dnwarmups=30\n-\n-This will first create 30 warm-up subscriptions to topics \"warmup-5\" through\n-\"warmup-34\", then 10,000 benchmarked subscriptions to topics \"topic-5\" through\n-\"topic-10,004\".  It will have a pipeline depth of 100 requests, meaning that\n-there will be at most 100 outstanding (unresponded) messages in flight at any\n-moment.\n-\n-To run a publishing benchmark, set `JAVAFLAGS` to:\n-\n-  -Dmode=pub -Dcount=10000 -Dnpar=100 -Dstart=5\n-\n-This will publish 10,000 messages to topic \"topic-5\", with a pipeline depth of\n-100 requests.\n-\n-At the end, the programs will print throughput and latency information.\n-\n-Utilities\n-=========\n-\n-To kill all the user's Java processes running on some machines, use:\n-\n-  hosts=\"...\" scripts/hw.bash dkill\n-\n-To check if any processes are running and are using ports of interest (esp.\n-9876-9878):\n-\n-  hosts=\"...\" scripts/hw.bash dstatus\n-\n-Add an argument to `dstatus` (may be anything) to get a more detailed listing.\n-\n-To check if there's anything consuming the CPU on some machines:\n-\n-  hosts=\"...\" scripts/hw.bash tops\n-\n-To run an arbitrary command on multiple hosts in parallel:\n-\n-  hosts=\"...\" scripts/hw.bash parssh $command\n-\n-To do this in sequence:\n-\n-  hosts=\"...\" xargs= scripts/hw.bash parssh $command"}]}

