{"sha":"d32010f5fcc6a040a56dc8b983cc14d107cff2df","node_id":"MDY6Q29tbWl0MTU3NTk1NjpkMzIwMTBmNWZjYzZhMDQwYTU2ZGM4Yjk4M2NjMTRkMTA3Y2ZmMmRm","commit":{"author":{"name":"Matteo Merli","email":"mmerli@apache.org","date":"2016-05-04T13:55:46Z"},"committer":{"name":"Matteo Merli","email":"mmerli@apache.org","date":"2016-05-04T13:55:46Z"},"message":"BOOKKEEPER-926: Compacted entries are not properly synced before updating index\n\nâ€¦ting index\n\nAuthor: Matteo Merli <mmerli@apache.org>\n\nReviewers: Guo Sijie <sijie@apache.org>\n\nCloses #41 from merlimat/bk-926","tree":{"sha":"1b0df4cbd6745d5a1d03589caa1a254739a0da3d","url":"https://api.github.com/repos/apache/bookkeeper/git/trees/1b0df4cbd6745d5a1d03589caa1a254739a0da3d"},"url":"https://api.github.com/repos/apache/bookkeeper/git/commits/d32010f5fcc6a040a56dc8b983cc14d107cff2df","comment_count":0,"verification":{"verified":false,"reason":"unsigned","signature":null,"payload":null}},"url":"https://api.github.com/repos/apache/bookkeeper/commits/d32010f5fcc6a040a56dc8b983cc14d107cff2df","html_url":"https://github.com/apache/bookkeeper/commit/d32010f5fcc6a040a56dc8b983cc14d107cff2df","comments_url":"https://api.github.com/repos/apache/bookkeeper/commits/d32010f5fcc6a040a56dc8b983cc14d107cff2df/comments","author":{"login":"merlimat","id":62500,"node_id":"MDQ6VXNlcjYyNTAw","avatar_url":"https://avatars.githubusercontent.com/u/62500?v=4","gravatar_id":"","url":"https://api.github.com/users/merlimat","html_url":"https://github.com/merlimat","followers_url":"https://api.github.com/users/merlimat/followers","following_url":"https://api.github.com/users/merlimat/following{/other_user}","gists_url":"https://api.github.com/users/merlimat/gists{/gist_id}","starred_url":"https://api.github.com/users/merlimat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/merlimat/subscriptions","organizations_url":"https://api.github.com/users/merlimat/orgs","repos_url":"https://api.github.com/users/merlimat/repos","events_url":"https://api.github.com/users/merlimat/events{/privacy}","received_events_url":"https://api.github.com/users/merlimat/received_events","type":"User","site_admin":false},"committer":{"login":"merlimat","id":62500,"node_id":"MDQ6VXNlcjYyNTAw","avatar_url":"https://avatars.githubusercontent.com/u/62500?v=4","gravatar_id":"","url":"https://api.github.com/users/merlimat","html_url":"https://github.com/merlimat","followers_url":"https://api.github.com/users/merlimat/followers","following_url":"https://api.github.com/users/merlimat/following{/other_user}","gists_url":"https://api.github.com/users/merlimat/gists{/gist_id}","starred_url":"https://api.github.com/users/merlimat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/merlimat/subscriptions","organizations_url":"https://api.github.com/users/merlimat/orgs","repos_url":"https://api.github.com/users/merlimat/repos","events_url":"https://api.github.com/users/merlimat/events{/privacy}","received_events_url":"https://api.github.com/users/merlimat/received_events","type":"User","site_admin":false},"parents":[{"sha":"f8e0331f14a933ad2ed3d933c5eea927020b4967","url":"https://api.github.com/repos/apache/bookkeeper/commits/f8e0331f14a933ad2ed3d933c5eea927020b4967","html_url":"https://github.com/apache/bookkeeper/commit/f8e0331f14a933ad2ed3d933c5eea927020b4967"}],"stats":{"total":159,"additions":91,"deletions":68},"files":[{"sha":"e5ee8d753ae0943e83dd0850c06514788c85f3dd","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","status":"modified","additions":29,"deletions":67,"changes":96,"blob_url":"https://github.com/apache/bookkeeper/blob/d32010f5fcc6a040a56dc8b983cc14d107cff2df/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","raw_url":"https://github.com/apache/bookkeeper/raw/d32010f5fcc6a040a56dc8b983cc14d107cff2df/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java?ref=d32010f5fcc6a040a56dc8b983cc14d107cff2df","patch":"@@ -126,7 +126,7 @@ void acquire(int permits) {\n     /**\n      * A scanner wrapper to check whether a ledger is alive in an entry log file\n      */\n-    class CompactionScannerFactory implements EntryLogger.EntryLogListener {\n+    class CompactionScannerFactory {\n         List<EntryLocation> offsets = new ArrayList<EntryLocation>();\n \n         EntryLogScanner newScanner(final EntryLogMetadata meta) {\n@@ -141,66 +141,38 @@ public boolean accept(long ledgerId) {\n                 }\n \n                 @Override\n-                public void process(final long ledgerId, long offset, ByteBuffer entry)\n-                        throws IOException {\n+                public void process(final long ledgerId, long offset, ByteBuffer entry) throws IOException {\n                     throttler.acquire(entry.remaining());\n-                    synchronized (CompactionScannerFactory.this) {\n-                        if (offsets.size() > maxOutstandingRequests) {\n-                            waitEntrylogFlushed();\n-                        }\n-                        entry.getLong(); // discard ledger id, we already have it\n-                        long entryId = entry.getLong();\n-                        entry.rewind();\n-\n-                        long newoffset = entryLogger.addEntry(ledgerId, entry);\n-                        offsets.add(new EntryLocation(ledgerId, entryId, newoffset));\n+\n+                    if (offsets.size() > maxOutstandingRequests) {\n+                        flush();\n                     }\n+                    entry.getLong(); // discard ledger id, we already have it\n+                    long entryId = entry.getLong();\n+                    entry.rewind();\n+\n+                    long newoffset = entryLogger.addEntry(ledgerId, entry);\n+                    offsets.add(new EntryLocation(ledgerId, entryId, newoffset));\n+\n                 }\n             };\n         }\n \n-        final Object flushLock = new Object();\n-\n-        @Override\n-        public void onRotateEntryLog() {\n-            synchronized (flushLock) {\n-                flushLock.notifyAll();\n+        void flush() throws IOException {\n+            if (offsets.isEmpty()) {\n+                LOG.debug(\"Skipping entry log flushing, as there are no offset!\");\n+                return;\n             }\n-        }\n \n-        synchronized private void waitEntrylogFlushed() throws IOException {\n+            // Before updating the index, we want to wait until all the compacted entries are flushed into the\n+            // entryLog\n             try {\n-                if (offsets.size() <= 0) {\n-                    LOG.debug(\"Skipping entry log flushing, as there is no offset!\");\n-                    return;\n-                }\n-\n-                EntryLocation lastOffset = offsets.get(offsets.size()-1);\n-                long lastOffsetLogId = EntryLogger.logIdForOffset(lastOffset.location);\n-                while (lastOffsetLogId < entryLogger.getLeastUnflushedLogId() && running) {\n-                    synchronized (flushLock) {\n-                        flushLock.wait(1000);\n-                    }\n+                entryLogger.flush();\n \n-                    lastOffset = offsets.get(offsets.size()-1);\n-                    lastOffsetLogId = EntryLogger.logIdForOffset(lastOffset.location);\n-                }\n-                if (lastOffsetLogId >= entryLogger.getLeastUnflushedLogId() && !running) {\n-                    throw new IOException(\"Shutdown before flushed\");\n-                }\n-            } catch (InterruptedException ie) {\n-                Thread.currentThread().interrupt();\n-                throw new IOException(\"Interrupted waiting for flush\", ie);\n+                ledgerStorage.updateEntriesLocations(offsets);\n+            } finally {\n+                offsets.clear();\n             }\n-\n-            ledgerStorage.updateEntriesLocations(offsets);\n-            offsets.clear();\n-        }\n-\n-        synchronized void flush() throws IOException {\n-            waitEntrylogFlushed();\n-\n-            ledgerStorage.flushEntriesLocationsIndex();\n         }\n     }\n \n@@ -227,7 +199,6 @@ public GarbageCollectorThread(ServerConfiguration conf,\n         this.compactionRateByEntries  = conf.getCompactionRateByEntries();\n         this.compactionRateByBytes = conf.getCompactionRateByBytes();\n         this.scannerFactory = new CompactionScannerFactory();\n-        entryLogger.addListener(this.scannerFactory);\n \n         this.garbageCleaner = new GarbageCollector.GarbageCleaner() {\n             @Override\n@@ -456,19 +427,22 @@ public int compare(EntryLogMetadata m1, EntryLogMetadata m2) {\n         List<EntryLogMetadata> logsToCompact = new ArrayList<EntryLogMetadata>();\n         logsToCompact.addAll(entryLogMetaMap.values());\n         Collections.sort(logsToCompact, sizeComparator);\n-        List<Long> toRemove = new ArrayList<Long>();\n \n         for (EntryLogMetadata meta : logsToCompact) {\n             if (meta.getUsage() >= threshold) {\n                 break;\n             }\n \n             if (LOG.isDebugEnabled()) {\n-                LOG.debug(\"Compacting entry log {} below threshold {}.\", meta.getEntryLogId(), threshold);\n+                LOG.debug(\"Compacting entry log {} below threshold {}\", meta.getEntryLogId(), threshold);\n             }\n             try {\n                 compactEntryLog(scannerFactory, meta);\n-                toRemove.add(meta.getEntryLogId());\n+                scannerFactory.flush();\n+\n+                LOG.info(\"Removing entry log {} after compaction\", meta.getEntryLogId());\n+                removeEntryLog(meta.getEntryLogId());\n+\n             } catch (LedgerDirsManager.NoWritableLedgerDirException nwlde) {\n                 LOG.warn(\"No writable ledger directory available, aborting compaction\", nwlde);\n                 break;\n@@ -483,18 +457,6 @@ public int compare(EntryLogMetadata m1, EntryLogMetadata m2) {\n                 return;\n             }\n         }\n-        try {\n-            // compaction finished, flush any outstanding offsets\n-            scannerFactory.flush();\n-        } catch (IOException ioe) {\n-            LOG.error(\"Cannot flush compacted entries, skip removal\", ioe);\n-            return;\n-        }\n-\n-        // offsets have been flushed, its now safe to remove the old entrylogs\n-        for (Long l : toRemove) {\n-            removeEntryLog(l);\n-        }\n     }\n \n     /**\n@@ -545,7 +507,7 @@ protected void compactEntryLog(CompactionScannerFactory scannerFactory,\n             return;\n         }\n \n-        LOG.info(\"Compacting entry log : {}\", entryLogMeta.getEntryLogId());\n+        LOG.info(\"Compacting entry log : {} - Usage: {} %\", entryLogMeta.getEntryLogId(), entryLogMeta.getUsage());\n \n         try {\n             entryLogger.scanEntryLog(entryLogMeta.getEntryLogId(),"},{"sha":"5d384baf26d586e76f45ed21d36c09e3157440be","filename":"bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","status":"modified","additions":62,"deletions":1,"changes":63,"blob_url":"https://github.com/apache/bookkeeper/blob/d32010f5fcc6a040a56dc8b983cc14d107cff2df/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","raw_url":"https://github.com/apache/bookkeeper/raw/d32010f5fcc6a040a56dc8b983cc14d107cff2df/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java?ref=d32010f5fcc6a040a56dc8b983cc14d107cff2df","patch":"@@ -33,6 +33,8 @@\n import java.util.Collection;\n \n import org.apache.bookkeeper.client.BookKeeper.DigestType;\n+import org.apache.bookkeeper.bookie.EntryLogger.EntryLogScanner;\n+import org.apache.bookkeeper.bookie.GarbageCollectorThread.CompactionScannerFactory;\n import org.apache.bookkeeper.client.LedgerEntry;\n import org.apache.bookkeeper.client.LedgerHandle;\n import org.apache.bookkeeper.client.LedgerMetadata;\n@@ -49,7 +51,7 @@\n import org.apache.bookkeeper.util.TestUtils;\n import org.apache.bookkeeper.versioning.Version;\n import org.apache.zookeeper.AsyncCallback;\n-\n+import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -114,6 +116,7 @@ public void setUp() throws Exception {\n         baseConf.setEntryLogSizeLimit(numEntries * ENTRY_SIZE);\n         // Disable skip list for compaction\n         baseConf.setGcWaitTime(gcWaitTime);\n+        baseConf.setFlushInterval(100);\n         baseConf.setMinorCompactionThreshold(minorCompactionThreshold);\n         baseConf.setMajorCompactionThreshold(majorCompactionThreshold);\n         baseConf.setMinorCompactionInterval(minorCompactionInterval);\n@@ -631,4 +634,62 @@ public void checkpointComplete(Checkpoint checkPoint, boolean compact)\n         storage.gcThread.resumeMinorGC();\n         storage.gcThread.resumeMajorGC();\n     }\n+\n+    @Test(timeout = 60000)\n+    public void testCompactionWithEntryLogRollover() throws Exception {\n+        // Disable bookie gc during this test\n+        baseConf.setGcWaitTime(60000);\n+        baseConf.setMinorCompactionInterval(0);\n+        baseConf.setMajorCompactionInterval(0);\n+        restartBookies();\n+\n+        // prepare data\n+        LedgerHandle[] lhs = prepareData(3, false);\n+\n+        for (LedgerHandle lh : lhs) {\n+            lh.close();\n+        }\n+\n+        // remove ledger2 and ledger3\n+        bkc.deleteLedger(lhs[1].getId());\n+        bkc.deleteLedger(lhs[2].getId());\n+        LOG.info(\"Finished deleting the ledgers contains most entries.\");\n+\n+        InterleavedLedgerStorage ledgerStorage = (InterleavedLedgerStorage) bs.get(0).getBookie().ledgerStorage;\n+        GarbageCollectorThread garbageCollectorThread = ledgerStorage.gcThread;\n+        CompactionScannerFactory compactionScannerFactory = garbageCollectorThread.scannerFactory;\n+        long entryLogId = 0;\n+        EntryLogger entryLogger = ledgerStorage.entryLogger;\n+\n+        LOG.info(\"Before compaction -- Least unflushed log id: {}\", entryLogger.getLeastUnflushedLogId());\n+\n+        // Compact entryLog 0\n+        EntryLogScanner scanner = compactionScannerFactory.newScanner(entryLogger.getEntryLogMetadata(entryLogId));\n+\n+        entryLogger.scanEntryLog(entryLogId, scanner);\n+\n+        long entryLogIdAfterCompaction = entryLogger.getLeastUnflushedLogId();\n+        LOG.info(\"After compaction -- Least unflushed log id: {}\", entryLogIdAfterCompaction);\n+\n+        // Add more entries to trigger entrylog roll over\n+        LedgerHandle[] lhs2 = prepareData(3, false);\n+\n+        for (LedgerHandle lh : lhs2) {\n+            lh.close();\n+        }\n+\n+        // Wait for entry logger to move forward\n+        while (entryLogger.getLeastUnflushedLogId() <= entryLogIdAfterCompaction) {\n+            Thread.sleep(100);\n+        }\n+\n+        long entryLogIdBeforeFlushing = entryLogger.getLeastUnflushedLogId();\n+        LOG.info(\"Added more data -- Least unflushed log id: {}\", entryLogIdBeforeFlushing);\n+\n+        Assert.assertTrue(entryLogIdAfterCompaction < entryLogIdBeforeFlushing);\n+\n+        // Wait for entries to be flushed on entry logs and update index\n+        // This operation should succeed even if the entry log rolls over after the last entry was compacted\n+        compactionScannerFactory.flush();\n+    }\n }"}]}

