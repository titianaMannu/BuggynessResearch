{"sha":"e8a1dec3fbea9b2f8c59c32e2f189d60c1864688","node_id":"MDY6Q29tbWl0MTU3NTk1NjplOGExZGVjM2ZiZWE5YjJmOGM1OWMzMmUyZjE4OWQ2MGMxODY0Njg4","commit":{"author":{"name":"Ivan Brendan Kelly","email":"ivank@apache.org","date":"2012-03-14T16:11:54Z"},"committer":{"name":"Ivan Brendan Kelly","email":"ivank@apache.org","date":"2012-03-14T16:11:54Z"},"message":"BOOKKEEPER-185: Remove bookkeeper-server dependency on hadoop-common (ivank)\n\ngit-svn-id: https://svn.apache.org/repos/asf/zookeeper/bookkeeper/trunk@1300616 13f79535-47bb-0310-9956-ffa450edef68","tree":{"sha":"c38647e41c831501b27f6cc7e93e44b2340bc569","url":"https://api.github.com/repos/apache/bookkeeper/git/trees/c38647e41c831501b27f6cc7e93e44b2340bc569"},"url":"https://api.github.com/repos/apache/bookkeeper/git/commits/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688","comment_count":0,"verification":{"verified":false,"reason":"unsigned","signature":null,"payload":null}},"url":"https://api.github.com/repos/apache/bookkeeper/commits/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688","html_url":"https://github.com/apache/bookkeeper/commit/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688","comments_url":"https://api.github.com/repos/apache/bookkeeper/commits/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688/comments","author":{"login":"ivankelly","id":54955,"node_id":"MDQ6VXNlcjU0OTU1","avatar_url":"https://avatars.githubusercontent.com/u/54955?v=4","gravatar_id":"","url":"https://api.github.com/users/ivankelly","html_url":"https://github.com/ivankelly","followers_url":"https://api.github.com/users/ivankelly/followers","following_url":"https://api.github.com/users/ivankelly/following{/other_user}","gists_url":"https://api.github.com/users/ivankelly/gists{/gist_id}","starred_url":"https://api.github.com/users/ivankelly/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ivankelly/subscriptions","organizations_url":"https://api.github.com/users/ivankelly/orgs","repos_url":"https://api.github.com/users/ivankelly/repos","events_url":"https://api.github.com/users/ivankelly/events{/privacy}","received_events_url":"https://api.github.com/users/ivankelly/received_events","type":"User","site_admin":false},"committer":{"login":"ivankelly","id":54955,"node_id":"MDQ6VXNlcjU0OTU1","avatar_url":"https://avatars.githubusercontent.com/u/54955?v=4","gravatar_id":"","url":"https://api.github.com/users/ivankelly","html_url":"https://github.com/ivankelly","followers_url":"https://api.github.com/users/ivankelly/followers","following_url":"https://api.github.com/users/ivankelly/following{/other_user}","gists_url":"https://api.github.com/users/ivankelly/gists{/gist_id}","starred_url":"https://api.github.com/users/ivankelly/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ivankelly/subscriptions","organizations_url":"https://api.github.com/users/ivankelly/orgs","repos_url":"https://api.github.com/users/ivankelly/repos","events_url":"https://api.github.com/users/ivankelly/events{/privacy}","received_events_url":"https://api.github.com/users/ivankelly/received_events","type":"User","site_admin":false},"parents":[{"sha":"fd5bb2bce8585ef49ba01d853241a4ee36d487e2","url":"https://api.github.com/repos/apache/bookkeeper/commits/fd5bb2bce8585ef49ba01d853241a4ee36d487e2","html_url":"https://github.com/apache/bookkeeper/commit/fd5bb2bce8585ef49ba01d853241a4ee36d487e2"}],"stats":{"total":848,"additions":668,"deletions":180},"files":[{"sha":"fe80e40643b9aa2e9f3d3964f6c5fe828c3bff50","filename":"CHANGES.txt","status":"modified","additions":2,"deletions":0,"changes":2,"blob_url":"https://github.com/apache/bookkeeper/blob/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688/CHANGES.txt","raw_url":"https://github.com/apache/bookkeeper/raw/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688/CHANGES.txt","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/CHANGES.txt?ref=e8a1dec3fbea9b2f8c59c32e2f189d60c1864688","patch":"@@ -52,6 +52,8 @@ Trunk (unreleased changes)\n \n         BOOKKEEPER-180: bookie server doesn't quit when running out of disk space (sijie via ivank)\n \n+        BOOKKEEPER-185: Remove bookkeeper-server dependency on hadoop-common (ivank)\n+\n       hedwig-server/\n       \n         BOOKKEEPER-140: Hub server doesn't subscribe remote region correctly when a region is down. (Sijie Gou via ivank)"},{"sha":"8c2a1fb8944157130eb9bd80278b32220bb7ddfc","filename":"bookkeeper-server/pom.xml","status":"modified","additions":0,"deletions":179,"changes":179,"blob_url":"https://github.com/apache/bookkeeper/blob/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688/bookkeeper-server/pom.xml","raw_url":"https://github.com/apache/bookkeeper/raw/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688/bookkeeper-server/pom.xml","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/pom.xml?ref=e8a1dec3fbea9b2f8c59c32e2f189d60c1864688","patch":"@@ -81,185 +81,6 @@\n       <artifactId>commons-io</artifactId>\n       <version>2.1</version>\n     </dependency>\n-    <dependency>\n-      <groupId>org.apache.hadoop</groupId>\n-      <artifactId>hadoop-common</artifactId>\n-      <version>0.23.1</version>\n-      <exclusions>\n-\t<exclusion>\n-\t  <groupId>com.google.guava</groupId>\n-\t  <artifactId>guava</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>com.google.guava</groupId>\n-\t  <artifactId>guava</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.apache.commons</groupId>\n-\t  <artifactId>commons-math</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>xmlenc</groupId>\n-\t  <artifactId>xmlenc</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>commons-httpclient</groupId>\n-\t  <artifactId>commons-httpclient</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>commons-codec</groupId>\n-\t  <artifactId>commons-codec</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>commons-net</groupId>\n-\t  <artifactId>commons-net</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>javax.servlet</groupId>\n-\t  <artifactId>servlet-api</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.mortbay.jetty</groupId>\n-\t  <artifactId>jetty</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.mortbay.jetty</groupId>\n-\t  <artifactId>jetty-util</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>asm</groupId>\n-\t  <artifactId>asm</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>com.sun.jersey</groupId>\n-\t  <artifactId>jersey-core</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>com.sun.jersey</groupId>\n-\t  <artifactId>jersey-json</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>com.sun.jersey</groupId>\n-\t  <artifactId>jersey-server</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>tomcat</groupId>\n-\t  <artifactId>jasper-compiler</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>tomcat</groupId>\n-\t  <artifactId>jasper-runtime</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>javax.servlet.jsp</groupId>\n-\t  <artifactId>jsp-api</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>commons-el</groupId>\n-\t  <artifactId>commons-el</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>commons-logging</groupId>\n-\t  <artifactId>commons-logging</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>commons-logging</groupId>\n-\t  <artifactId>commons-logging-api</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>log4j</groupId>\n-\t  <artifactId>log4j</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>net.java.dev.jets3t</groupId>\n-\t  <artifactId>jets3t</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.apache.mina</groupId>\n-\t  <artifactId>mina-core</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.apache.ftpserver</groupId>\n-\t  <artifactId>ftplet-api</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.apache.ftpserver</groupId>\n-\t  <artifactId>ftpserver-core</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.apache.ftpserver</groupId>\n-\t  <artifactId>ftpserver-deprecated</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>junit</groupId>\n-\t  <artifactId>junit</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>commons-lang</groupId>\n-\t  <artifactId>commons-lang</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>commons-collections</groupId>\n-\t  <artifactId>commons-collections</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>commons-configuration</groupId>\n-\t  <artifactId>commons-configuration</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>hsqldb</groupId>\n-\t  <artifactId>hsqldb</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.slf4j</groupId>\n-\t  <artifactId>slf4j-api</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.slf4j</groupId>\n-\t  <artifactId>slf4j-log4j12</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.eclipse.jdt</groupId>\n-\t  <artifactId>core</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>oro</groupId>\n-\t  <artifactId>oro</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.codehaus.jackson</groupId>\n-\t  <artifactId>jackson-mapper-asl</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.aspectj</groupId>\n-\t  <artifactId>aspectjrt</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.apache.avro</groupId>\n-\t  <artifactId>avro</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.apache.avro</groupId>\n-\t  <artifactId>avro-ipc</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>net.sf.kosmosfs</groupId>\n-\t  <artifactId>kfs</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>com.google.protobuf</groupId>\n-\t  <artifactId>protobuf-java</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>org.apache.hadoop</groupId>\n-\t  <artifactId>hadoop-auth</artifactId>\n-\t</exclusion>\n-\t<exclusion>\n-\t  <groupId>com.googlecode.json-simple</groupId>\n-\t  <artifactId>json-simple</artifactId>\n-\t</exclusion>\n-      </exclusions>\n-    </dependency>\n     <!--\n \tAnnoying dependency we need to include because\n \tzookeeper uses log4j and so we transatively do, but"},{"sha":"e7370554e2ef1cfb01f5bbe10d42ad832f917d8e","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/FileSystemUpgrade.java","status":"modified","additions":1,"deletions":1,"changes":2,"blob_url":"https://github.com/apache/bookkeeper/blob/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/FileSystemUpgrade.java","raw_url":"https://github.com/apache/bookkeeper/raw/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/FileSystemUpgrade.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/FileSystemUpgrade.java?ref=e8a1dec3fbea9b2f8c59c32e2f189d60c1864688","patch":"@@ -21,7 +21,7 @@\n \n package org.apache.bookkeeper.bookie;\n \n-import org.apache.hadoop.fs.HardLink;\n+import org.apache.bookkeeper.util.HardLink;\n \n import org.apache.commons.io.FileUtils;\n import org.apache.commons.cli.BasicParser;"},{"sha":"6e6c3e781e7920a07f332ed950c269ee3b6fefd1","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/util/HardLink.java","status":"added","additions":665,"deletions":0,"changes":665,"blob_url":"https://github.com/apache/bookkeeper/blob/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688/bookkeeper-server/src/main/java/org/apache/bookkeeper/util/HardLink.java","raw_url":"https://github.com/apache/bookkeeper/raw/e8a1dec3fbea9b2f8c59c32e2f189d60c1864688/bookkeeper-server/src/main/java/org/apache/bookkeeper/util/HardLink.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/util/HardLink.java?ref=e8a1dec3fbea9b2f8c59c32e2f189d60c1864688","patch":"@@ -0,0 +1,665 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+/* Copied wholesale from hadoop-common 0.23.1\n+package org.apache.hadoop.fs;\n+*/\n+package org.apache.bookkeeper.util;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+\n+/**\n+ * Class for creating hardlinks.\n+ * Supports Unix/Linux, WinXP/2003/Vista via Cygwin, and Mac OS X.\n+ * \n+ * The HardLink class was formerly a static inner class of FSUtil,\n+ * and the methods provided were blatantly non-thread-safe.\n+ * To enable volume-parallel Update snapshots, we now provide static \n+ * threadsafe methods that allocate new buffer string arrays\n+ * upon each call.  We also provide an API to hardlink all files in a\n+ * directory with a single command, which is up to 128 times more \n+ * efficient - and minimizes the impact of the extra buffer creations.\n+ */\n+public class HardLink { \n+\n+  public enum OSType {\n+    OS_TYPE_UNIX,\n+    OS_TYPE_WINXP,\n+    OS_TYPE_SOLARIS,\n+    OS_TYPE_MAC\n+  }\n+  \n+  public static OSType osType;\n+  private static HardLinkCommandGetter getHardLinkCommand;\n+  \n+  public final LinkStats linkStats; //not static\n+  \n+  //initialize the command \"getters\" statically, so can use their \n+  //methods without instantiating the HardLink object\n+  static { \n+    osType = getOSType();\n+    if (osType == OSType.OS_TYPE_WINXP) {\n+      // Windows\n+      getHardLinkCommand = new HardLinkCGWin();\n+    } else {\n+      // Unix\n+      getHardLinkCommand = new HardLinkCGUnix();\n+      //override getLinkCountCommand for the particular Unix variant\n+      //Linux is already set as the default - {\"stat\",\"-c%h\", null}\n+      if (osType == OSType.OS_TYPE_MAC) {\n+        String[] linkCountCmdTemplate = {\"stat\",\"-f%l\", null};\n+        HardLinkCGUnix.setLinkCountCmdTemplate(linkCountCmdTemplate);\n+      } else if (osType == OSType.OS_TYPE_SOLARIS) {\n+        String[] linkCountCmdTemplate = {\"ls\",\"-l\", null};\n+        HardLinkCGUnix.setLinkCountCmdTemplate(linkCountCmdTemplate);        \n+      }\n+    }\n+  }\n+\n+  public HardLink() {\n+    linkStats = new LinkStats();\n+  }\n+  \n+  static private OSType getOSType() {\n+    String osName = System.getProperty(\"os.name\");\n+    if (osName.contains(\"Windows\") &&\n+            (osName.contains(\"XP\") \n+            || osName.contains(\"2003\") \n+            || osName.contains(\"Vista\")\n+            || osName.contains(\"Windows_7\")\n+            || osName.contains(\"Windows 7\") \n+            || osName.contains(\"Windows7\"))) {\n+      return OSType.OS_TYPE_WINXP;\n+    }\n+    else if (osName.contains(\"SunOS\") \n+            || osName.contains(\"Solaris\")) {\n+       return OSType.OS_TYPE_SOLARIS;\n+    }\n+    else if (osName.contains(\"Mac\")) {\n+       return OSType.OS_TYPE_MAC;\n+    }\n+    else {\n+      return OSType.OS_TYPE_UNIX;\n+    }\n+  }\n+  \n+  /**\n+   * This abstract class bridges the OS-dependent implementations of the \n+   * needed functionality for creating hardlinks and querying link counts.\n+   * The particular implementation class is chosen during \n+   * static initialization phase of the HardLink class.\n+   * The \"getter\" methods construct shell command strings for various purposes.\n+   */\n+  private static abstract class HardLinkCommandGetter {\n+\n+    /**\n+     * Get the command string needed to hardlink a bunch of files from\n+     * a single source directory into a target directory.  The source directory\n+     * is not specified here, but the command will be executed using the source\n+     * directory as the \"current working directory\" of the shell invocation.\n+     * \n+     * @param fileBaseNames - array of path-less file names, relative\n+     *            to the source directory\n+     * @param linkDir - target directory where the hardlinks will be put\n+     * @return - an array of Strings suitable for use as a single shell command\n+     *            with {@link Runtime.exec()}\n+     * @throws IOException - if any of the file or path names misbehave\n+     */\n+    abstract String[] linkMult(String[] fileBaseNames, File linkDir) \n+                          throws IOException;\n+    \n+    /**\n+     * Get the command string needed to hardlink a single file\n+     */\n+    abstract String[] linkOne(File file, File linkName) throws IOException;\n+    \n+    /**\n+     * Get the command string to query the hardlink count of a file\n+     */\n+    abstract String[] linkCount(File file) throws IOException;\n+    \n+    /**\n+     * Calculate the total string length of the shell command\n+     * resulting from execution of linkMult, plus the length of the\n+     * source directory name (which will also be provided to the shell)\n+     * \n+     * @param fileDir - source directory, parent of fileBaseNames\n+     * @param fileBaseNames - array of path-less file names, relative\n+     *            to the source directory\n+     * @param linkDir - target directory where the hardlinks will be put\n+     * @return - total data length (must not exceed maxAllowedCmdArgLength)\n+     * @throws IOException\n+     */\n+    abstract int getLinkMultArgLength(\n+                     File fileDir, String[] fileBaseNames, File linkDir) \n+                     throws IOException;\n+    \n+    /**\n+     * Get the maximum allowed string length of a shell command on this OS,\n+     * which is just the documented minimum guaranteed supported command\n+     * length - aprx. 32KB for Unix, and 8KB for Windows.\n+     */\n+    abstract int getMaxAllowedCmdArgLength(); \n+  }\n+  \n+  /**\n+   * Implementation of HardLinkCommandGetter class for Unix\n+   */\n+  static class HardLinkCGUnix extends HardLinkCommandGetter {\n+    private static String[] hardLinkCommand = {\"ln\", null, null};\n+    private static String[] hardLinkMultPrefix = {\"ln\"};\n+    private static String[] hardLinkMultSuffix = {null};\n+    private static String[] getLinkCountCommand = {\"stat\",\"-c%h\", null};\n+    //Unix guarantees at least 32K bytes cmd length.\n+    //Subtract another 64b to allow for Java 'exec' overhead\n+    private static final int maxAllowedCmdArgLength = 32*1024 - 65;\n+    \n+    private static synchronized \n+    void setLinkCountCmdTemplate(String[] template) {\n+      //May update this for specific unix variants, \n+      //after static initialization phase\n+      getLinkCountCommand = template;\n+    }\n+    \n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkOne(java.io.File, java.io.File)\n+     */\n+    @Override\n+    String[] linkOne(File file, File linkName) \n+    throws IOException {\n+      String[] buf = new String[hardLinkCommand.length];\n+      System.arraycopy(hardLinkCommand, 0, buf, 0, hardLinkCommand.length);\n+      //unix wants argument order: \"ln <existing> <new>\"\n+      buf[1] = makeShellPath(file); \n+      buf[2] = makeShellPath(linkName);\n+      return buf;\n+    }\n+    \n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkMult(java.lang.String[], java.io.File)\n+     */\n+    @Override\n+    String[] linkMult(String[] fileBaseNames, File linkDir) \n+    throws IOException {\n+      String[] buf = new String[fileBaseNames.length \n+                                + hardLinkMultPrefix.length \n+                                + hardLinkMultSuffix.length];\n+      int mark=0;\n+      System.arraycopy(hardLinkMultPrefix, 0, buf, mark, \n+                       hardLinkMultPrefix.length);\n+      mark += hardLinkMultPrefix.length;\n+      System.arraycopy(fileBaseNames, 0, buf, mark, fileBaseNames.length);\n+      mark += fileBaseNames.length;\n+      buf[mark] = makeShellPath(linkDir);\n+      return buf;\n+    }\n+    \n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkCount(java.io.File)\n+     */\n+    @Override\n+    String[] linkCount(File file) \n+    throws IOException {\n+      String[] buf = new String[getLinkCountCommand.length];\n+      System.arraycopy(getLinkCountCommand, 0, buf, 0, \n+                       getLinkCountCommand.length);\n+      buf[getLinkCountCommand.length - 1] = makeShellPath(file);\n+      return buf;\n+    }\n+    \n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#getLinkMultArgLength(java.io.File, java.lang.String[], java.io.File)\n+     */\n+    @Override\n+    int getLinkMultArgLength(File fileDir, String[] fileBaseNames, File linkDir) \n+    throws IOException{\n+      int sum = 0;\n+      for (String x : fileBaseNames) {\n+        // add 1 to account for terminal null or delimiter space\n+        sum += 1 + ((x == null) ? 0 : x.length());\n+      }\n+      sum += 2 + makeShellPath(fileDir).length()\n+             + makeShellPath(linkDir).length();\n+      //add the fixed overhead of the hardLinkMult prefix and suffix\n+      sum += 3; //length(\"ln\") + 1\n+      return sum;\n+    }\n+    \n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#getMaxAllowedCmdArgLength()\n+     */\n+    @Override\n+    int getMaxAllowedCmdArgLength() {\n+      return maxAllowedCmdArgLength;\n+    }\n+  }\n+  \n+  \n+  /**\n+   * Implementation of HardLinkCommandGetter class for Windows\n+   * \n+   * Note that the linkCount shell command for Windows is actually\n+   * a Cygwin shell command, and depends on ${cygwin}/bin\n+   * being in the Windows PATH environment variable, so\n+   * stat.exe can be found.\n+   */\n+  static class HardLinkCGWin extends HardLinkCommandGetter {\n+    //The Windows command getter impl class and its member fields are\n+    //package-private (\"default\") access instead of \"private\" to assist \n+    //unit testing (sort of) on non-Win servers\n+\n+    static String[] hardLinkCommand = {\n+                        \"fsutil\",\"hardlink\",\"create\", null, null};\n+    static String[] hardLinkMultPrefix = {\n+                        \"cmd\",\"/q\",\"/c\",\"for\", \"%f\", \"in\", \"(\"};\n+    static String   hardLinkMultDir = \"\\\\%f\";\n+    static String[] hardLinkMultSuffix = {\n+                        \")\", \"do\", \"fsutil\", \"hardlink\", \"create\", null, \n+                        \"%f\", \"1>NUL\"};\n+    static String[] getLinkCountCommand = {\"stat\",\"-c%h\", null};\n+    //Windows guarantees only 8K - 1 bytes cmd length.\n+    //Subtract another 64b to allow for Java 'exec' overhead\n+    static final int maxAllowedCmdArgLength = 8*1024 - 65;\n+\n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkOne(java.io.File, java.io.File)\n+     */\n+    @Override\n+    String[] linkOne(File file, File linkName) \n+    throws IOException {\n+      String[] buf = new String[hardLinkCommand.length];\n+      System.arraycopy(hardLinkCommand, 0, buf, 0, hardLinkCommand.length);\n+      //windows wants argument order: \"create <new> <existing>\"\n+      buf[4] = file.getCanonicalPath(); \n+      buf[3] = linkName.getCanonicalPath();\n+      return buf;\n+    }\n+    \n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkMult(java.lang.String[], java.io.File)\n+     */\n+    @Override\n+    String[] linkMult(String[] fileBaseNames, File linkDir) \n+    throws IOException {\n+      String[] buf = new String[fileBaseNames.length \n+                                + hardLinkMultPrefix.length \n+                                + hardLinkMultSuffix.length];\n+      String td = linkDir.getCanonicalPath() + hardLinkMultDir;\n+      int mark=0;\n+      System.arraycopy(hardLinkMultPrefix, 0, buf, mark, \n+                       hardLinkMultPrefix.length);\n+      mark += hardLinkMultPrefix.length;\n+      System.arraycopy(fileBaseNames, 0, buf, mark, fileBaseNames.length);\n+      mark += fileBaseNames.length;\n+      System.arraycopy(hardLinkMultSuffix, 0, buf, mark, \n+                       hardLinkMultSuffix.length);\n+      mark += hardLinkMultSuffix.length;\n+      buf[mark - 3] = td;\n+      return buf;\n+    }\n+    \n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkCount(java.io.File)\n+     */\n+    @Override\n+    String[] linkCount(File file) \n+    throws IOException {\n+      String[] buf = new String[getLinkCountCommand.length];\n+      System.arraycopy(getLinkCountCommand, 0, buf, 0, \n+                       getLinkCountCommand.length);\n+      //The linkCount command is actually a Cygwin shell command,\n+      //not a Windows shell command, so we should use \"makeShellPath()\"\n+      //instead of \"getCanonicalPath()\".  However, that causes another\n+      //shell exec to \"cygpath.exe\", and \"stat.exe\" actually can handle\n+      //DOS-style paths (it just prints a couple hundred bytes of warning\n+      //to stderr), so we use the more efficient \"getCanonicalPath()\".\n+      buf[getLinkCountCommand.length - 1] = file.getCanonicalPath();\n+      return buf;\n+    }\n+    \n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#getLinkMultArgLength(java.io.File, java.lang.String[], java.io.File)\n+     */\n+    @Override\n+    int getLinkMultArgLength(File fileDir, String[] fileBaseNames, File linkDir) \n+    throws IOException {\n+      int sum = 0;\n+      for (String x : fileBaseNames) {\n+        // add 1 to account for terminal null or delimiter space\n+        sum += 1 + ((x == null) ? 0 : x.length());\n+      }\n+      sum += 2 + fileDir.getCanonicalPath().length() +\n+               linkDir.getCanonicalPath().length();\n+      //add the fixed overhead of the hardLinkMult command \n+      //(prefix, suffix, and Dir suffix)\n+      sum += (\"cmd.exe /q /c for %f in ( ) do \"\n+              + \"fsutil hardlink create \\\\%f %f 1>NUL \").length();\n+      return sum;\n+    }\n+    \n+    /*\n+     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#getMaxAllowedCmdArgLength()\n+     */\n+    @Override\n+    int getMaxAllowedCmdArgLength() {\n+      return maxAllowedCmdArgLength;\n+    }\n+  }\n+  \n+  \n+  /**\n+   * Calculate the nominal length of all contributors to the total \n+   * commandstring length, including fixed overhead of the OS-dependent \n+   * command.  It's protected rather than private, to assist unit testing,\n+   * but real clients are not expected to need it -- see the way \n+   * createHardLinkMult() uses it internally so the user doesn't need to worry\n+   * about it.\n+   * \n+   * @param fileDir - source directory, parent of fileBaseNames\n+   * @param fileBaseNames - array of path-less file names, relative\n+   *            to the source directory\n+   * @param linkDir - target directory where the hardlinks will be put\n+   * @return - total data length (must not exceed maxAllowedCmdArgLength)\n+   * @throws IOException\n+   */\n+  protected static int getLinkMultArgLength(\n+          File fileDir, String[] fileBaseNames, File linkDir) \n+  throws IOException {\n+    return getHardLinkCommand.getLinkMultArgLength(fileDir, \n+          fileBaseNames, linkDir);\n+  }\n+  \n+  /**\n+   * Return this private value for use by unit tests.\n+   * Shell commands are not allowed to have a total string length\n+   * exceeding this size.\n+   */\n+  protected static int getMaxAllowedCmdArgLength() {\n+    return getHardLinkCommand.getMaxAllowedCmdArgLength();\n+  }\n+  \n+  /*\n+   * ****************************************************\n+   * Complexity is above.  User-visible functionality is below\n+   * ****************************************************\n+   */\n+\n+  /**\n+   * Creates a hardlink \n+   * @param file - existing source file\n+   * @param linkName - desired target link file\n+   */\n+  public static void createHardLink(File file, File linkName) \n+  throws IOException {\n+    if (file == null) {\n+      throw new IOException(\n+          \"invalid arguments to createHardLink: source file is null\");\n+    }\n+    if (linkName == null) {\n+      throw new IOException(\n+          \"invalid arguments to createHardLink: link name is null\");\n+    }\n+\t  // construct and execute shell command\n+    String[] hardLinkCommand = getHardLinkCommand.linkOne(file, linkName);\n+    Process process = Runtime.getRuntime().exec(hardLinkCommand);\n+    try {\n+      if (process.waitFor() != 0) {\n+        String errMsg = new BufferedReader(new InputStreamReader(\n+            process.getInputStream())).readLine();\n+        if (errMsg == null)  errMsg = \"\";\n+        String inpMsg = new BufferedReader(new InputStreamReader(\n+            process.getErrorStream())).readLine();\n+        if (inpMsg == null)  inpMsg = \"\";\n+        throw new IOException(errMsg + inpMsg);\n+      }\n+    } catch (InterruptedException e) {\n+      throw new IOException(e);\n+    } finally {\n+      process.destroy();\n+    }\n+  }\n+\n+  /**\n+   * Creates hardlinks from multiple existing files within one parent\n+   * directory, into one target directory.\n+   * @param parentDir - directory containing source files\n+   * @param fileBaseNames - list of path-less file names, as returned by \n+   *                        parentDir.list()\n+   * @param linkDir - where the hardlinks should be put.  It must already exist.\n+   * \n+   * If the list of files is too long (overflows maxAllowedCmdArgLength),\n+   * we will automatically split it into multiple invocations of the\n+   * underlying method.\n+   */\n+  public static void createHardLinkMult(File parentDir, String[] fileBaseNames, \n+      File linkDir) throws IOException {\n+    //This is the public method all non-test clients are expected to use.\n+    //Normal case - allow up to maxAllowedCmdArgLength characters in the cmd\n+    createHardLinkMult(parentDir, fileBaseNames, linkDir, \n+                       getHardLinkCommand.getMaxAllowedCmdArgLength());\n+  }\n+\n+  /*\n+   * Implements {@link createHardLinkMult} with added variable  \"maxLength\",\n+   * to ease unit testing of the auto-splitting feature for long lists.\n+   * Likewise why it returns \"callCount\", the number of sub-arrays that\n+   * the file list had to be split into.\n+   * Non-test clients are expected to call the public method instead.\n+   */\n+  protected static int createHardLinkMult(File parentDir, \n+      String[] fileBaseNames, File linkDir, int maxLength) \n+  throws IOException {\n+    if (parentDir == null) {\n+      throw new IOException(\n+          \"invalid arguments to createHardLinkMult: parent directory is null\");\n+    }\n+    if (linkDir == null) {\n+      throw new IOException(\n+          \"invalid arguments to createHardLinkMult: link directory is null\");\n+    }\n+    if (fileBaseNames == null) {\n+      throw new IOException(\n+          \"invalid arguments to createHardLinkMult: \"\n+          + \"filename list can be empty but not null\");\n+    }\n+    if (fileBaseNames.length == 0) {\n+      //the OS cmds can't handle empty list of filenames, \n+      //but it's legal, so just return.\n+      return 0; \n+    }\n+    if (!linkDir.exists()) {\n+      throw new FileNotFoundException(linkDir + \" not found.\");\n+    }\n+\n+    //if the list is too long, split into multiple invocations\n+    int callCount = 0;\n+    if (getLinkMultArgLength(parentDir, fileBaseNames, linkDir) > maxLength\n+          && fileBaseNames.length > 1) {\n+      String[] list1 = Arrays.copyOf(fileBaseNames, fileBaseNames.length/2);\n+      callCount += createHardLinkMult(parentDir, list1, linkDir, maxLength);\n+      String[] list2 = Arrays.copyOfRange(fileBaseNames, fileBaseNames.length/2,\n+          fileBaseNames.length);\n+      callCount += createHardLinkMult(parentDir, list2, linkDir, maxLength);  \n+      return callCount;\n+    } else {\n+      callCount = 1;\n+    }\n+    \n+    // construct and execute shell command\n+    String[] hardLinkCommand = getHardLinkCommand.linkMult(fileBaseNames, \n+        linkDir);\n+    Process process = Runtime.getRuntime().exec(hardLinkCommand, null, \n+        parentDir);\n+    try {\n+      if (process.waitFor() != 0) {\n+        String errMsg = new BufferedReader(new InputStreamReader(\n+            process.getInputStream())).readLine();\n+        if (errMsg == null)  errMsg = \"\";\n+        String inpMsg = new BufferedReader(new InputStreamReader(\n+            process.getErrorStream())).readLine();\n+        if (inpMsg == null)  inpMsg = \"\";\n+        throw new IOException(errMsg + inpMsg);\n+      }\n+    } catch (InterruptedException e) {\n+      throw new IOException(e);\n+    } finally {\n+      process.destroy();\n+    }\n+    return callCount;\n+  }\n+\n+   /**\n+   * Retrieves the number of links to the specified file.\n+   */\n+  public static int getLinkCount(File fileName) throws IOException {\n+    if (fileName == null) {\n+      throw new IOException(\n+          \"invalid argument to getLinkCount: file name is null\");\n+    }\n+    if (!fileName.exists()) {\n+      throw new FileNotFoundException(fileName + \" not found.\");\n+    }\n+\n+    // construct and execute shell command\n+    String[] cmd = getHardLinkCommand.linkCount(fileName);\n+    String inpMsg = null;\n+    String errMsg = null;\n+    int exitValue = -1;\n+    BufferedReader in = null;\n+    BufferedReader err = null;\n+\n+    Process process = Runtime.getRuntime().exec(cmd);\n+    try {\n+      exitValue = process.waitFor();\n+      in = new BufferedReader(new InputStreamReader(\n+                                  process.getInputStream()));\n+      inpMsg = in.readLine();\n+      err = new BufferedReader(new InputStreamReader(\n+                                   process.getErrorStream()));\n+      errMsg = err.readLine();\n+      if (inpMsg == null || exitValue != 0) {\n+        throw createIOException(fileName, inpMsg, errMsg, exitValue, null);\n+      }\n+      if (osType == OSType.OS_TYPE_SOLARIS) {\n+        String[] result = inpMsg.split(\"\\\\s+\");\n+        return Integer.parseInt(result[1]);\n+      } else {\n+        return Integer.parseInt(inpMsg);\n+      }\n+    } catch (NumberFormatException e) {\n+      throw createIOException(fileName, inpMsg, errMsg, exitValue, e);\n+    } catch (InterruptedException e) {\n+      throw createIOException(fileName, inpMsg, errMsg, exitValue, e);\n+    } finally {\n+      process.destroy();\n+      if (in != null) in.close();\n+      if (err != null) err.close();\n+    }\n+  }\n+  \n+  /* Create an IOException for failing to get link count. */\n+  private static IOException createIOException(File f, String message,\n+      String error, int exitvalue, Exception cause) {\n+    \n+    final String winErrMsg = \"; Windows errors in getLinkCount are often due \"\n+         + \"to Cygwin misconfiguration\";\n+\n+    final String s = \"Failed to get link count on file \" + f\n+        + \": message=\" + message\n+        + \"; error=\" + error\n+        + ((osType == OSType.OS_TYPE_WINXP) ? winErrMsg : \"\")\n+        + \"; exit value=\" + exitvalue;\n+    return (cause == null) ? new IOException(s) : new IOException(s, cause);\n+  }\n+  \n+  \n+  /**\n+   * HardLink statistics counters and methods.\n+   * Not multi-thread safe, obviously.\n+   * Init is called during HardLink instantiation, above.\n+   * \n+   * These are intended for use by knowledgeable clients, not internally, \n+   * because many of the internal methods are static and can't update these\n+   * per-instance counters.\n+   */\n+  public static class LinkStats {\n+    public int countDirs = 0; \n+    public int countSingleLinks = 0; \n+    public int countMultLinks = 0; \n+    public int countFilesMultLinks = 0; \n+    public int countEmptyDirs = 0; \n+    public int countPhysicalFileCopies = 0;\n+  \n+    public void clear() {\n+      countDirs = 0; \n+      countSingleLinks = 0; \n+      countMultLinks = 0; \n+      countFilesMultLinks = 0; \n+      countEmptyDirs = 0; \n+      countPhysicalFileCopies = 0;\n+    }\n+    \n+    public String report() {\n+      return \"HardLinkStats: \" + countDirs + \" Directories, including \" \n+      + countEmptyDirs + \" Empty Directories, \" \n+      + countSingleLinks \n+      + \" single Link operations, \" + countMultLinks \n+      + \" multi-Link operations, linking \" + countFilesMultLinks \n+      + \" files, total \" + (countSingleLinks + countFilesMultLinks) \n+      + \" linkable files.  Also physically copied \" \n+      + countPhysicalFileCopies + \" other files.\";\n+    }\n+  }\n+\n+  /**\n+   * Convert a os-native filename to a path that works for the shell.\n+   * @param filename The filename to convert\n+   * @return The unix pathname\n+   * @throws IOException on windows, there can be problems with the subprocess\n+   */\n+  public static String makeShellPath(File file) throws IOException {\n+    String filename = file.getCanonicalPath();\n+    if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n+      BufferedReader r = null;\n+      try {\n+        ProcessBuilder pb = new ProcessBuilder(\"cygpath\", \"-u\", filename);\n+        Process p = pb.start();\n+        int err = p.waitFor();\n+        if (err != 0) {\n+            throw new IOException(\"Couldn't resolve path \"\n+                                  + filename + \"(\" + err + \")\");\n+        }\n+        r = new BufferedReader(new InputStreamReader(p.getInputStream()));\n+        return r.readLine();\n+      } catch (InterruptedException ie) {\n+        throw new IOException(\"Couldn't resolve path \" + filename, ie);\n+      } finally {\n+        if (r != null) {\n+          r.close();\n+        }\n+      }\n+    } else {\n+      return filename;\n+    }\n+  }\n+}\n+"}]}

