{"sha":"e911559291b6ae9aa310a695fb4174796921ac81","node_id":"MDY6Q29tbWl0MTU3NTk1NjplOTExNTU5MjkxYjZhZTlhYTMxMGE2OTVmYjQxNzQ3OTY5MjFhYzgx","commit":{"author":{"name":"Ivan Brendan Kelly","email":"ivank@apache.org","date":"2012-03-08T11:13:23Z"},"committer":{"name":"Ivan Brendan Kelly","email":"ivank@apache.org","date":"2012-03-08T11:13:23Z"},"message":"BOOKKEEPER-160: bookie server needs to do compaction over entry log files to reclaim disk space (sijie via ivank)\n\ngit-svn-id: https://svn.apache.org/repos/asf/zookeeper/bookkeeper/trunk@1298357 13f79535-47bb-0310-9956-ffa450edef68","tree":{"sha":"0c4e8295bf23fed6a611de9656735774832176a4","url":"https://api.github.com/repos/apache/bookkeeper/git/trees/0c4e8295bf23fed6a611de9656735774832176a4"},"url":"https://api.github.com/repos/apache/bookkeeper/git/commits/e911559291b6ae9aa310a695fb4174796921ac81","comment_count":0,"verification":{"verified":false,"reason":"unsigned","signature":null,"payload":null}},"url":"https://api.github.com/repos/apache/bookkeeper/commits/e911559291b6ae9aa310a695fb4174796921ac81","html_url":"https://github.com/apache/bookkeeper/commit/e911559291b6ae9aa310a695fb4174796921ac81","comments_url":"https://api.github.com/repos/apache/bookkeeper/commits/e911559291b6ae9aa310a695fb4174796921ac81/comments","author":{"login":"ivankelly","id":54955,"node_id":"MDQ6VXNlcjU0OTU1","avatar_url":"https://avatars.githubusercontent.com/u/54955?v=4","gravatar_id":"","url":"https://api.github.com/users/ivankelly","html_url":"https://github.com/ivankelly","followers_url":"https://api.github.com/users/ivankelly/followers","following_url":"https://api.github.com/users/ivankelly/following{/other_user}","gists_url":"https://api.github.com/users/ivankelly/gists{/gist_id}","starred_url":"https://api.github.com/users/ivankelly/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ivankelly/subscriptions","organizations_url":"https://api.github.com/users/ivankelly/orgs","repos_url":"https://api.github.com/users/ivankelly/repos","events_url":"https://api.github.com/users/ivankelly/events{/privacy}","received_events_url":"https://api.github.com/users/ivankelly/received_events","type":"User","site_admin":false},"committer":{"login":"ivankelly","id":54955,"node_id":"MDQ6VXNlcjU0OTU1","avatar_url":"https://avatars.githubusercontent.com/u/54955?v=4","gravatar_id":"","url":"https://api.github.com/users/ivankelly","html_url":"https://github.com/ivankelly","followers_url":"https://api.github.com/users/ivankelly/followers","following_url":"https://api.github.com/users/ivankelly/following{/other_user}","gists_url":"https://api.github.com/users/ivankelly/gists{/gist_id}","starred_url":"https://api.github.com/users/ivankelly/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ivankelly/subscriptions","organizations_url":"https://api.github.com/users/ivankelly/orgs","repos_url":"https://api.github.com/users/ivankelly/repos","events_url":"https://api.github.com/users/ivankelly/events{/privacy}","received_events_url":"https://api.github.com/users/ivankelly/received_events","type":"User","site_admin":false},"parents":[{"sha":"431554da9e2598c73edfbaf671e87cfd9ae94716","url":"https://api.github.com/repos/apache/bookkeeper/commits/431554da9e2598c73edfbaf671e87cfd9ae94716","html_url":"https://github.com/apache/bookkeeper/commit/431554da9e2598c73edfbaf671e87cfd9ae94716"}],"stats":{"total":1319,"additions":1123,"deletions":196},"files":[{"sha":"64e7d517e14cde311e0e7904065a1fd3a648679e","filename":"CHANGES.txt","status":"modified","additions":2,"deletions":0,"changes":2,"blob_url":"https://github.com/apache/bookkeeper/blob/e911559291b6ae9aa310a695fb4174796921ac81/CHANGES.txt","raw_url":"https://github.com/apache/bookkeeper/raw/e911559291b6ae9aa310a695fb4174796921ac81/CHANGES.txt","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/CHANGES.txt?ref=e911559291b6ae9aa310a695fb4174796921ac81","patch":"@@ -76,6 +76,8 @@ Trunk (unreleased changes)\n \n         BOOKKEEPER-178: Delay ledger directory creation until the ledger index file was created (sijie via ivank)\n \n+        BOOKKEEPER-160: bookie server needs to do compaction over entry log files to reclaim disk space (sijie via ivank)\n+\n       hedwig-server/\n \n         BOOKKEEPER-77: Add a console client for hedwig (Sijie Guo via ivank)"},{"sha":"b206b7e333e47fb3590ffa94262e56bb10d220f6","filename":"bookkeeper-server/conf/bk_server.conf","status":"modified","additions":22,"deletions":0,"changes":22,"blob_url":"https://github.com/apache/bookkeeper/blob/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/conf/bk_server.conf","raw_url":"https://github.com/apache/bookkeeper/raw/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/conf/bk_server.conf","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/conf/bk_server.conf?ref=e911559291b6ae9aa310a695fb4174796921ac81","patch":"@@ -52,6 +52,28 @@ ledgerDirectories=/tmp/bk-data\n # A new entry log file will be created when the old one reaches the file size limitation\n # logSizeLimit=2147483648\n \n+# Threshold of minor compaction\n+# For those entry log files whose remaining size percentage reaches below\n+# this threshold will be compacted in a minor compaction.\n+# If it is set to less than zero, the minor compaction is disabled.\n+# minorCompactionThreshold=0.2\n+\n+# Interval to run minor compaction, in seconds\n+# If it is set to less than zero, the minor compaction is disabled. \n+# minorCompactionInterval=3600\n+\n+# Threshold of major compaction\n+# For those entry log files whose remaining size percentage reaches below\n+# this threshold will be compacted in a major compaction.\n+# Those entry log files whose remaining size percentage is still\n+# higher than the threshold will never be compacted.\n+# If it is set to less than zero, the minor compaction is disabled.\n+# majorCompactionThreshold=0.8\n+\n+# Interval to run major compaction, in seconds\n+# If it is set to less than zero, the major compaction is disabled. \n+# majorCompactionInterval=86400 \n+\n # Max file size of journal file, in mega bytes\n # A new journal file will be created when the old one reaches the file size limitation\n #"},{"sha":"a3c24de9c34eadf8042b762fedb89489d0cfe514","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","status":"modified","additions":45,"deletions":3,"changes":48,"blob_url":"https://github.com/apache/bookkeeper/blob/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","raw_url":"https://github.com/apache/bookkeeper/raw/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java?ref=e911559291b6ae9aa310a695fb4174796921ac81","patch":"@@ -136,6 +136,11 @@ public long getEntry() {\n \n     EntryLogger entryLogger;\n     LedgerCache ledgerCache;\n+    // This is the thread that garbage collects the entry logs that do not\n+    // contain any active ledgers in them; and compacts the entry logs that\n+    // has lower remaining percentage to reclaim disk space.\n+    final GarbageCollectorThread gcThread;\n+\n     /**\n      * SyncThread is a background thread which flushes ledger index pages periodically.\n      * Also it takes responsibility of garbage collecting journal files.\n@@ -263,6 +268,28 @@ void shutdown() throws InterruptedException {\n         }\n     }\n \n+    /**\n+     * Scanner used to do entry log compaction\n+     */\n+    class EntryLogCompactionScanner implements EntryLogger.EntryLogScanner {\n+        @Override\n+        public boolean accept(long ledgerId) {\n+            // bookie has no knowledge about which ledger is deleted\n+            // so just accept all ledgers.\n+            return true;\n+        }\n+\n+        @Override\n+        public void process(long ledgerId, ByteBuffer buffer)\n+            throws IOException {\n+            try {\n+                Bookie.this.addEntryByLedgerId(ledgerId, buffer);\n+            } catch (BookieException be) {\n+                throw new IOException(be);\n+            }\n+        }\n+    }\n+\n     public Bookie(ServerConfiguration conf) \n             throws IOException, KeeperException, InterruptedException, BookieException {\n         super(\"Bookie-\" + conf.getBookiePort());\n@@ -283,9 +310,11 @@ public Bookie(ServerConfiguration conf)\n         ledgerManager = LedgerManagerFactory.newLedgerManager(conf, this.zk);\n \n         syncThread = new SyncThread(conf);\n-        entryLogger = new EntryLogger(conf, this);\n+        entryLogger = new EntryLogger(conf);\n         ledgerCache = new LedgerCache(conf, ledgerManager);\n-\n+        gcThread = new GarbageCollectorThread(conf, this.zk, ledgerCache, entryLogger,\n+                                              new EntryLogCompactionScanner());\n+        // replay journals\n         readJournal();\n     }\n \n@@ -390,7 +419,7 @@ synchronized public void start() {\n         LOG.debug(\"I'm starting a bookie with journal directory \" + journalDirectory.getName());\n         super.start();\n         syncThread.start();\n-        entryLogger.start();\n+        gcThread.start();\n         // set running here.\n         // since bookie server use running as a flag to tell bookie server whether it is alive\n         // if setting it in bookie thread, the watcher might run before bookie thread.\n@@ -907,6 +936,9 @@ public synchronized void shutdown() throws InterruptedException {\n         if (!running) { // avoid shutdown twice\n             return;\n         }\n+        // shut down gc thread, which depends on zookeeper client\n+        // also compaction will write entries again to entry log file\n+        gcThread.shutdown();\n         // Shutdown the ZK client\n         if(zk != null) zk.close();\n         this.interrupt();\n@@ -957,6 +989,16 @@ public void writeComplete(int rc, long ledgerId,\n         return l;\n     }\n \n+    protected void addEntryByLedgerId(long ledgerId, ByteBuffer entry)\n+        throws IOException, BookieException {\n+        LedgerDescriptor handle = getHandle(ledgerId);\n+        try {\n+            handle.addEntry(entry);\n+        } finally {\n+            putHandle(handle);\n+        }\n+    }\n+\n     /**\n      * Add an entry to a ledger as specified by handle. \n      */"},{"sha":"82f01e8e006bb053714fbeb144e56140ccead27a","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java","status":"modified","additions":239,"deletions":180,"changes":419,"blob_url":"https://github.com/apache/bookkeeper/blob/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java","raw_url":"https://github.com/apache/bookkeeper/raw/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java?ref=e911559291b6ae9aa310a695fb4174796921ac81","patch":"@@ -36,13 +36,13 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.List;\n+import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentMap;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+\n import org.apache.bookkeeper.conf.ServerConfiguration;\n-import org.apache.bookkeeper.meta.LedgerManager;\n \n /**\n  * This class manages the writing of the bookkeeper entries. All the new\n@@ -54,9 +54,6 @@\n public class EntryLogger {\n     private static final Logger LOG = LoggerFactory.getLogger(EntryLogger.class);\n     private File dirs[];\n-    // This is a handle to the Bookie parent instance. We need this to get\n-    // access to the LedgerCache as well as the ZooKeeper client handle.\n-    private final Bookie bookie;\n \n     private long logId;\n     /**\n@@ -74,24 +71,102 @@\n     // this indicates that a write has happened since the last flush\n     private volatile boolean somethingWritten = false;\n \n-    // Maps entry log files to the set of ledgers that comprise the file.\n-    private ConcurrentMap<Long, ConcurrentHashMap<Long, Boolean>> entryLogs2LedgersMap = new ConcurrentHashMap<Long, ConcurrentHashMap<Long, Boolean>>();\n-    // This is the thread that garbage collects the entry logs that do not\n-    // contain any active ledgers in them.\n-    GarbageCollectorThread gcThread = new GarbageCollectorThread();\n-    // This is how often we want to run the Garbage Collector Thread (in milliseconds).\n-    final long gcWaitTime;\n+    final static long MB = 1024 * 1024;\n+\n+    /**\n+     * Records the total size, remaining size and the set of ledgers that comprise a entry log.\n+     */\n+    static class EntryLogMetadata {\n+        long entryLogId;\n+        long totalSize;\n+        long remainingSize;\n+        ConcurrentHashMap<Long, Long> ledgersMap;\n+\n+        public EntryLogMetadata(long logId) {\n+            this.entryLogId = logId;\n+\n+            totalSize = remainingSize = 0;\n+            ledgersMap = new ConcurrentHashMap<Long, Long>();\n+        }\n+\n+        public void addLedgerSize(long ledgerId, long size) {\n+            totalSize += size;\n+            remainingSize += size;\n+            Long ledgerSize = ledgersMap.get(ledgerId);\n+            if (null == ledgerSize) {\n+                ledgerSize = 0L;\n+            }\n+            ledgerSize += size;\n+            ledgersMap.put(ledgerId, ledgerSize);\n+        }\n+\n+        public void removeLedger(long ledgerId) {\n+            Long size = ledgersMap.remove(ledgerId);\n+            if (null == size) {\n+                return;\n+            }\n+            remainingSize -= size;\n+        }\n+\n+        public boolean containsLedger(long ledgerId) {\n+            return ledgersMap.containsKey(ledgerId);\n+        }\n+\n+        public double getUsage() {\n+            if (totalSize == 0L) {\n+                return 0.0f;\n+            }\n+            return (double)remainingSize / totalSize;\n+        }\n+\n+        public boolean isEmpty() {\n+            return ledgersMap.isEmpty();\n+        }\n+\n+        @Override\n+        public String toString() {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(\"{ totalSize = \").append(totalSize).append(\", remainingSize = \")\n+              .append(remainingSize).append(\", ledgersMap = \").append(ledgersMap).append(\" }\");\n+            return sb.toString();\n+        }\n+    }\n+\n+    /**\n+     * Scan entries in a entry log file.\n+     */\n+    static interface EntryLogScanner {\n+        /**\n+         * Tests whether or not the entries belongs to the specified ledger\n+         * should be processed.\n+         *\n+         * @param ledgerId\n+         *          Ledger ID.\n+         * @return true if and only the entries of the ledger should be scanned.\n+         */\n+        public boolean accept(long ledgerId);\n+\n+        /**\n+         * Process an entry.\n+         *\n+         * @param ledgerId\n+         *          Ledger ID.\n+         * @param entry\n+         *          Entry ByteBuffer\n+         * @throws IOException\n+         */\n+        public void process(long ledgerId, ByteBuffer entry) throws IOException;\n+    }\n \n     /**\n      * Create an EntryLogger that stores it's log files in the given\n      * directories\n      */\n-    public EntryLogger(ServerConfiguration conf, Bookie bookie) throws IOException {\n+    public EntryLogger(ServerConfiguration conf) throws IOException {\n         this.dirs = conf.getLedgerDirs();\n-        this.bookie = bookie;\n         // log size limit\n         this.logSizeLimit = conf.getEntryLogSizeLimit();\n-        this.gcWaitTime = conf.getGcWaitTime();\n+\n         // Initialize the entry log header buffer. This cannot be a static object\n         // since in our unit tests, we run multiple Bookies and thus EntryLoggers\n         // within the same JVM. All of these Bookie instances access this header\n@@ -108,118 +183,11 @@ public EntryLogger(ServerConfiguration conf, Bookie bookie) throws IOException {\n         createLogId(logId);\n     }\n \n-    public void start() {\n-        // Start the Garbage Collector thread to prune unneeded entry logs.\n-        gcThread.start();\n-    }\n-\n     /**\n      * Maps entry log files to open channels.\n      */\n     private ConcurrentHashMap<Long, BufferedChannel> channels = new ConcurrentHashMap<Long, BufferedChannel>();\n \n-    /**\n-     * This is the garbage collector thread that runs in the background to\n-     * remove any entry log files that no longer contains any active ledger.\n-     */\n-    class GarbageCollectorThread extends Thread {\n-        volatile boolean running = true;\n-\n-        public GarbageCollectorThread() {\n-            super(\"GarbageCollectorThread\");\n-        }\n-\n-        @Override\n-        public void run() {\n-            while (running) {\n-                synchronized (this) {\n-                    try {\n-                        wait(gcWaitTime);\n-                    } catch (InterruptedException e) {\n-                        Thread.currentThread().interrupt();\n-                        continue;\n-                    }\n-                }\n-                // Extract all of the ledger ID's that comprise all of the entry logs\n-                // (except for the current new one which is still being written to).\n-                try {\n-                    extractLedgersFromEntryLogs();\n-                } catch (IOException ie) {\n-                    LOG.warn(\"Exception when extracting ledgers from entry logs : \", ie);\n-                }\n-\n-                // Initialization check. No need to run any logic if we are still starting up.\n-                if (bookie == null ||\n-                    bookie.zk == null || bookie.ledgerCache == null) {\n-                    continue;\n-                }\n-\n-                // gc inactive/deleted ledgers\n-                doGcLedgers();\n-\n-                // gc entry logs\n-                doGcEntryLogs();\n-            }\n-        }\n-\n-        /**\n-         * Do garbage collection ledger index files\n-         */\n-        private void doGcLedgers() {\n-            bookie.ledgerCache.activeLedgerManager.garbageCollectLedgers(\n-            new LedgerManager.GarbageCollector() {\n-                @Override\n-                public void gc(long ledgerId) {\n-                    try {\n-                        bookie.ledgerCache.deleteLedger(ledgerId);\n-                    } catch (IOException e) {\n-                        LOG.error(\"Exception when deleting the ledger index file on the Bookie: \", e);\n-                    }\n-                }\n-            });\n-        }\n-\n-        /**\n-         * Garbage collect those entry loggers which are not associated with any active ledgers\n-         */\n-        private void doGcEntryLogs() {\n-            // Loop through all of the entry logs and remove the non-active ledgers.\n-            for (Long entryLogId : entryLogs2LedgersMap.keySet()) {\n-                ConcurrentHashMap<Long, Boolean> entryLogLedgers = entryLogs2LedgersMap.get(entryLogId);\n-                for (Long entryLogLedger : entryLogLedgers.keySet()) {\n-                    // Remove the entry log ledger from the set if it isn't active.\n-                    if (!bookie.ledgerCache.activeLedgerManager.containsActiveLedger(entryLogLedger)) {\n-                        entryLogLedgers.remove(entryLogLedger);\n-                    }\n-                }\n-                if (entryLogLedgers.isEmpty()) {\n-                    // This means the entry log is not associated with any active ledgers anymore.\n-                    // We can remove this entry log file now.\n-                    LOG.info(\"Deleting entryLogId \" + entryLogId + \" as it has no active ledgers!\");\n-                    BufferedChannel bc = channels.remove(entryLogId);\n-                    if (null != bc) {\n-                        // close its underlying file channel, so it could be deleted really\n-                        try {\n-                            bc.getFileChannel().close();\n-                        } catch (IOException ie) {\n-                            LOG.warn(\"Exception while closing garbage collected entryLog file : \", ie);\n-                        }\n-                    }\n-                    File entryLogFile;\n-                    try {\n-                        entryLogFile = findFile(entryLogId);\n-                    } catch (FileNotFoundException e) {\n-                        LOG.error(\"Trying to delete an entryLog file that could not be found: \"\n-                                + entryLogId + \".log\");\n-                        continue;\n-                    }\n-                    entryLogFile.delete();\n-                    entryLogs2LedgersMap.remove(entryLogId);\n-                }\n-            }\n-        }\n-    }\n-\n     /**\n      * Creates a new log file with the given id.\n      */\n@@ -238,6 +206,34 @@ private void createLogId(long logId) throws IOException {\n         }\n     }\n \n+    /**\n+     * Remove entry log.\n+     *\n+     * @param entryLogId\n+     *          Entry Log File Id\n+     */\n+    protected boolean removeEntryLog(long entryLogId) {\n+        BufferedChannel bc = channels.remove(entryLogId);\n+        if (null != bc) {\n+            // close its underlying file channel, so it could be deleted really\n+            try {\n+                bc.getFileChannel().close();\n+            } catch (IOException ie) {\n+                LOG.warn(\"Exception while closing garbage collected entryLog file : \", ie);\n+            }\n+        }\n+        File entryLogFile;\n+        try {\n+            entryLogFile = findFile(entryLogId);\n+        } catch (FileNotFoundException e) {\n+            LOG.error(\"Trying to delete an entryLog file that could not be found: \"\n+                    + entryLogId + \".log\");\n+            return false;\n+        }\n+        entryLogFile.delete();\n+        return true;\n+    }\n+\n     /**\n      * writes the given id to the \"lastId\" file in the given directory.\n      */\n@@ -326,7 +322,7 @@ synchronized long addEntry(long ledger, ByteBuffer entry) throws IOException {\n         sizeBuff.flip();\n         int entrySize = sizeBuff.getInt();\n         // entrySize does not include the ledgerId\n-        if (entrySize > 1024*1024) {\n+        if (entrySize > MB) {\n             LOG.error(\"Sanity check failed for entry size of \" + entrySize + \" at location \" + pos + \" in \" + entryLogId);\n \n         }\n@@ -389,85 +385,148 @@ synchronized public boolean testAndClearSomethingWritten() {\n         }\n     }\n \n+    /**\n+     * A scanner used to extract entry log meta from entry log files.\n+     */\n+    class ExtractionScanner implements EntryLogScanner {\n+        EntryLogMetadata meta;\n+\n+        public ExtractionScanner(EntryLogMetadata meta) {\n+            this.meta = meta;\n+        }\n+\n+        @Override\n+        public boolean accept(long ledgerId) {\n+            return true;\n+        }\n+        @Override\n+        public void process(long ledgerId, ByteBuffer entry) {\n+            // add new entry size of a ledger to entry log meta\n+            meta.addLedgerSize(ledgerId, entry.limit() + 4);\n+        }\n+    }\n+\n     /**\n      * Method to read in all of the entry logs (those that we haven't done so yet),\n      * and find the set of ledger ID's that make up each entry log file.\n+     *\n+     * @param entryLogMetaMap\n+     *          Existing EntryLogs to Meta\n+     * @throws IOException\n      */\n-    private void extractLedgersFromEntryLogs() throws IOException {\n+    protected Map<Long, EntryLogMetadata> extractMetaFromEntryLogs(Map<Long, EntryLogMetadata> entryLogMetaMap) throws IOException {\n         // Extract it for every entry log except for the current one.\n         // Entry Log ID's are just a long value that starts at 0 and increments\n         // by 1 when the log fills up and we roll to a new one.\n-        ByteBuffer sizeBuff = ByteBuffer.allocate(4);\n-        BufferedChannel bc;\n         long curLogId = logId;\n         for (long entryLogId = 0; entryLogId < curLogId; entryLogId++) {\n             // Comb the current entry log file if it has not already been extracted.\n-            if (entryLogs2LedgersMap.containsKey(entryLogId)) {\n-                continue;\n-            }\n-            LOG.info(\"Extracting the ledgers from entryLogId: \" + entryLogId);\n-            // Get the BufferedChannel for the current entry log file\n-            try {\n-                bc = getChannelForLogId(entryLogId);\n-            } catch (FileNotFoundException e) {\n-                // If we can't find the entry log file, just log a warning message and continue.\n-                // This could be a deleted/garbage collected entry log.\n-                LOG.warn(\"Entry Log file not found in log directories: \" + entryLogId + \".log\");\n+            if (entryLogMetaMap.containsKey(entryLogId)) {\n                 continue;\n             }\n-            // Start the read position in the current entry log file to be after\n-            // the header where all of the ledger entries are.\n-            long pos = LOGFILE_HEADER_SIZE;\n-            ConcurrentHashMap<Long, Boolean> entryLogLedgers = new ConcurrentHashMap<Long, Boolean>();\n-            // Read through the entry log file and extract the ledger ID's.\n+            LOG.info(\"Extracting entry log meta from entryLogId: \" + entryLogId);\n+            EntryLogMetadata entryLogMeta = new EntryLogMetadata(entryLogId);\n+            ExtractionScanner scanner = new ExtractionScanner(entryLogMeta);\n+            // Read through the entry log file and extract the entry log meta\n             try {\n-                while (true) {\n-                    // Check if we've finished reading the entry log file.\n-                    if (pos >= bc.size()) {\n-                        break;\n-                    }\n-                    if (bc.read(sizeBuff, pos) != sizeBuff.capacity()) {\n-                        throw new IOException(\"Short read from entrylog \" + entryLogId);\n-                    }\n-                    pos += 4;\n-                    sizeBuff.flip();\n-                    int entrySize = sizeBuff.getInt();\n-                    if (entrySize > 1024 * 1024) {\n-                        LOG.error(\"Sanity check failed for entry size of \" + entrySize + \" at location \" + pos + \" in \"\n-                                + entryLogId);\n-                    }\n-                    byte data[] = new byte[entrySize];\n-                    ByteBuffer buff = ByteBuffer.wrap(data);\n-                    int rc = bc.read(buff, pos);\n-                    if (rc != data.length) {\n-                        throw new IOException(\"Short read for entryLog \" + entryLogId + \"@\" + pos + \"(\" + rc + \"!=\"\n-                                + data.length + \")\");\n-                    }\n-                    buff.flip();\n-                    long ledgerId = buff.getLong();\n-                    entryLogLedgers.put(ledgerId, true);\n-                    // Advance position to the next entry and clear sizeBuff.\n-                    pos += entrySize;\n-                    sizeBuff.clear();\n-                }\n+                scanEntryLog(entryLogId, scanner);\n+                LOG.info(\"Retrieved entry log meta data entryLogId: \" + entryLogId + \", meta: \" + entryLogMeta);\n+                entryLogMetaMap.put(entryLogId, entryLogMeta);\n             } catch(IOException e) {\n-              LOG.info(\"Premature exception when processing \" + entryLogId + \n+              LOG.warn(\"Premature exception when processing \" + entryLogId +\n                        \"recovery will take care of the problem\", e);\n             }\n-            LOG.info(\"Retrieved all ledgers that comprise entryLogId: \" + entryLogId + \", values: \" + entryLogLedgers);\n-            entryLogs2LedgersMap.put(entryLogId, entryLogLedgers);\n+\n         }\n+        return entryLogMetaMap;\n+    }\n+\n+    protected EntryLogMetadata extractMetaFromEntryLog(long entryLogId) {\n+        EntryLogMetadata entryLogMeta = new EntryLogMetadata(entryLogId);\n+        ExtractionScanner scanner = new ExtractionScanner(entryLogMeta);\n+        // Read through the entry log file and extract the entry log meta\n+        try {\n+            scanEntryLog(entryLogId, scanner);\n+            LOG.info(\"Retrieved entry log meta data entryLogId: \" + entryLogId + \", meta: \" + entryLogMeta);\n+        } catch(IOException e) {\n+          LOG.warn(\"Premature exception when processing \" + entryLogId +\n+                   \"recovery will take care of the problem\", e);\n+        }\n+        return entryLogMeta;\n     }\n \n     /**\n-     * Shutdown method to gracefully stop all threads spawned in this class and exit.\n+     * Scan entry log\n      *\n-     * @throws InterruptedException if there is an exception stopping threads.\n+     * @param entryLogId\n+     *          Entry Log Id\n+     * @param scanner\n+     *          Entry Log Scanner\n+     * @throws IOException\n+     */\n+    protected void scanEntryLog(long entryLogId, EntryLogScanner scanner) throws IOException {\n+        ByteBuffer sizeBuff = ByteBuffer.allocate(4);\n+        ByteBuffer lidBuff = ByteBuffer.allocate(8);\n+        BufferedChannel bc;\n+        // Get the BufferedChannel for the current entry log file\n+        try {\n+            bc = getChannelForLogId(entryLogId);\n+        } catch (IOException e) {\n+            LOG.warn(\"Failed to get channel to scan entry log: \" + entryLogId + \".log\");\n+            throw e;\n+        }\n+        // Start the read position in the current entry log file to be after\n+        // the header where all of the ledger entries are.\n+        long pos = LOGFILE_HEADER_SIZE;\n+        // Read through the entry log file and extract the ledger ID's.\n+        while (true) {\n+            // Check if we've finished reading the entry log file.\n+            if (pos >= bc.size()) {\n+                break;\n+            }\n+            if (bc.read(sizeBuff, pos) != sizeBuff.capacity()) {\n+                throw new IOException(\"Short read for entry size from entrylog \" + entryLogId);\n+            }\n+            pos += 4;\n+            sizeBuff.flip();\n+            int entrySize = sizeBuff.getInt();\n+            if (entrySize > MB) {\n+                LOG.warn(\"Found large size entry of \" + entrySize + \" at location \" + pos + \" in \"\n+                        + entryLogId);\n+            }\n+            sizeBuff.clear();\n+            // try to read ledger id first\n+            if (bc.read(lidBuff, pos) != lidBuff.capacity()) {\n+                throw new IOException(\"Short read for ledger id from entrylog \" + entryLogId);\n+            }\n+            lidBuff.flip();\n+            long lid = lidBuff.getLong();\n+            lidBuff.clear();\n+            if (!scanner.accept(lid)) {\n+                // skip this entry\n+                pos += entrySize;\n+                continue;\n+            }\n+            // read the entry\n+            byte data[] = new byte[entrySize];\n+            ByteBuffer buff = ByteBuffer.wrap(data);\n+            int rc = bc.read(buff, pos);\n+            if (rc != data.length) {\n+                throw new IOException(\"Short read for ledger entry from entryLog \" + entryLogId\n+                                    + \"@\" + pos + \"(\" + rc + \"!=\" + data.length + \")\");\n+            }\n+            buff.flip();\n+            // process the entry\n+            scanner.process(lid, buff);\n+            // Advance position to the next entry\n+            pos += entrySize;\n+        }\n+    }\n+\n+    /**\n+     * Shutdown method to gracefully stop entry logger.\n      */\n-    public void shutdown() throws InterruptedException {\n-        gcThread.running = false;\n-        gcThread.interrupt();\n-        gcThread.join();\n+    public void shutdown() {\n         // since logChannel is buffered channel, do flush when shutting down\n         try {\n             flush();"},{"sha":"c514f6d193a62d4010d6a18566f62c612a16c1b6","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","status":"added","additions":378,"deletions":0,"changes":378,"blob_url":"https://github.com/apache/bookkeeper/blob/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","raw_url":"https://github.com/apache/bookkeeper/raw/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java?ref=e911559291b6ae9aa310a695fb4174796921ac81","patch":"@@ -0,0 +1,378 @@\n+/*\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ */\n+\n+package org.apache.bookkeeper.bookie;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Comparator;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Map;\n+import java.util.List;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.bookkeeper.bookie.EntryLogger.EntryLogMetadata;\n+import org.apache.bookkeeper.bookie.EntryLogger.EntryLogScanner;\n+import org.apache.bookkeeper.conf.ServerConfiguration;\n+import org.apache.bookkeeper.meta.LedgerManager;\n+import org.apache.zookeeper.ZooKeeper;\n+\n+/**\n+ * This is the garbage collector thread that runs in the background to\n+ * remove any entry log files that no longer contains any active ledger.\n+ */\n+public class GarbageCollectorThread extends Thread {\n+    private static final Logger LOG = LoggerFactory.getLogger(GarbageCollectorThread.class);\n+\n+    private static final int SECOND = 1000;\n+\n+    // Maps entry log files to the set of ledgers that comprise the file and the size usage per ledger\n+    private Map<Long, EntryLogMetadata> entryLogMetaMap = new ConcurrentHashMap<Long, EntryLogMetadata>();\n+\n+    // This is how often we want to run the Garbage Collector Thread (in milliseconds).\n+    final long gcWaitTime;\n+\n+    // Compaction parameters\n+    boolean enableMinorCompaction = false;\n+    final double minorCompactionThreshold;\n+    final long minorCompactionInterval;\n+\n+    boolean enableMajorCompaction = false;\n+    final double majorCompactionThreshold;\n+    final long majorCompactionInterval;\n+\n+    long lastMinorCompactionTime;\n+    long lastMajorCompactionTime;\n+\n+    // Entry Logger Handle\n+    final EntryLogger entryLogger;\n+    final EntryLogScanner scanner;\n+\n+    // Ledger Cache Handle\n+    final LedgerCache ledgerCache;\n+\n+    // ZooKeeper Client\n+    final ZooKeeper zk;\n+\n+    // flag to ensure gc thread will not be interrupted during compaction\n+    // to reduce the risk getting entry log corrupted\n+    final AtomicBoolean compacting = new AtomicBoolean(false);\n+\n+    volatile boolean running = true;\n+\n+    /**\n+     * A scanner wrapper to check whether a ledger is alive in an entry log file\n+     */\n+    class CompactionScanner implements EntryLogScanner {\n+        EntryLogMetadata meta;\n+\n+        public CompactionScanner(EntryLogMetadata meta) {\n+            this.meta = meta;\n+        }\n+\n+        @Override\n+        public boolean accept(long ledgerId) {\n+            return meta.containsLedger(ledgerId) && scanner.accept(ledgerId);\n+        }\n+\n+        @Override\n+        public void process(long ledgerId, ByteBuffer entry) throws IOException {\n+            scanner.process(ledgerId, entry);\n+        }\n+    }\n+\n+\n+    /**\n+     * Create a garbage collector thread.\n+     *\n+     * @param conf\n+     *          Server Configuration Object.\n+     * @throws IOException\n+     */\n+    public GarbageCollectorThread(ServerConfiguration conf,\n+                                  ZooKeeper zookeeper,\n+                                  LedgerCache ledgerCache,\n+                                  EntryLogger entryLogger,\n+                                  EntryLogScanner scanner)\n+        throws IOException {\n+        super(\"GarbageCollectorThread\");\n+\n+        this.zk = zookeeper;\n+        this.ledgerCache = ledgerCache;\n+        this.entryLogger = entryLogger;\n+        this.scanner = scanner;\n+\n+        this.gcWaitTime = conf.getGcWaitTime();\n+        // compaction parameters\n+        minorCompactionThreshold = conf.getMinorCompactionThreshold();\n+        minorCompactionInterval = conf.getMinorCompactionInterval() * SECOND;\n+        majorCompactionThreshold = conf.getMajorCompactionThreshold();\n+        majorCompactionInterval = conf.getMajorCompactionInterval() * SECOND;\n+\n+        if (minorCompactionInterval > 0 && minorCompactionThreshold > 0) {\n+            if (minorCompactionThreshold > 1.0f) {\n+                throw new IOException(\"Invalid minor compaction threshold \"\n+                                    + minorCompactionThreshold);\n+            }\n+            if (minorCompactionInterval <= gcWaitTime) {\n+                throw new IOException(\"Too short minor compaction interval : \"\n+                                    + minorCompactionInterval);\n+            }\n+            enableMinorCompaction = true;\n+        }\n+\n+        if (majorCompactionInterval > 0 && majorCompactionThreshold > 0) {\n+            if (majorCompactionThreshold > 1.0f) {\n+                throw new IOException(\"Invalid major compaction threshold \"\n+                                    + majorCompactionThreshold);\n+            }\n+            if (majorCompactionInterval <= gcWaitTime) {\n+                throw new IOException(\"Too short major compaction interval : \"\n+                                    + majorCompactionInterval);\n+            }\n+            enableMajorCompaction = true;\n+        }\n+\n+        if (enableMinorCompaction && enableMajorCompaction) {\n+            if (minorCompactionInterval >= majorCompactionInterval ||\n+                minorCompactionThreshold >= majorCompactionThreshold) {\n+                throw new IOException(\"Invalid minor/major compaction settings : minor (\"\n+                                    + minorCompactionThreshold + \", \" + minorCompactionInterval\n+                                    + \"), major (\" + majorCompactionThreshold + \", \"\n+                                    + majorCompactionInterval + \")\");\n+            }\n+        }\n+\n+        LOG.info(\"Minor Compaction : enabled=\" + enableMinorCompaction + \", threshold=\"\n+               + minorCompactionThreshold + \", interval=\" + minorCompactionInterval);\n+        LOG.info(\"Major Compaction : enabled=\" + enableMajorCompaction + \", threshold=\"\n+               + majorCompactionThreshold + \", interval=\" + majorCompactionInterval);\n+\n+        lastMinorCompactionTime = lastMajorCompactionTime = System.currentTimeMillis();\n+    }\n+\n+    @Override\n+    public void run() {\n+        while (running) {\n+            synchronized (this) {\n+                try {\n+                    wait(gcWaitTime);\n+                } catch (InterruptedException e) {\n+                    Thread.currentThread().interrupt();\n+                    continue;\n+                }\n+            }\n+\n+            // Dependency check.\n+            if (null == zk) {\n+                continue;\n+            }\n+\n+            // Extract all of the ledger ID's that comprise all of the entry logs\n+            // (except for the current new one which is still being written to).\n+            try {\n+                entryLogMetaMap = entryLogger.extractMetaFromEntryLogs(entryLogMetaMap);\n+            } catch (IOException ie) {\n+                LOG.warn(\"Exception when extracting entry log meta from entry logs : \", ie);\n+            }\n+\n+            // gc inactive/deleted ledgers\n+            doGcLedgers();\n+\n+            // gc entry logs\n+            doGcEntryLogs();\n+\n+            long curTime = System.currentTimeMillis();\n+            if (enableMajorCompaction &&\n+                curTime - lastMajorCompactionTime > majorCompactionInterval) {\n+                // enter major compaction\n+                LOG.info(\"Enter major compaction\");\n+                doCompactEntryLogs(majorCompactionThreshold);\n+                lastMajorCompactionTime = System.currentTimeMillis();\n+                // also move minor compaction time\n+                lastMinorCompactionTime = lastMajorCompactionTime;\n+                continue;\n+            }\n+\n+            if (enableMinorCompaction &&\n+                curTime - lastMinorCompactionTime > minorCompactionInterval) {\n+                // enter minor compaction\n+                LOG.info(\"Enter minor compaction\");\n+                doCompactEntryLogs(minorCompactionThreshold);\n+                lastMinorCompactionTime = System.currentTimeMillis();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Do garbage collection ledger index files\n+     */\n+    private void doGcLedgers() {\n+        ledgerCache.activeLedgerManager.garbageCollectLedgers(\n+        new LedgerManager.GarbageCollector() {\n+            @Override\n+            public void gc(long ledgerId) {\n+                try {\n+                    ledgerCache.deleteLedger(ledgerId);\n+                } catch (IOException e) {\n+                    LOG.error(\"Exception when deleting the ledger index file on the Bookie: \", e);\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Garbage collect those entry loggers which are not associated with any active ledgers\n+     */\n+    private void doGcEntryLogs() {\n+        // Loop through all of the entry logs and remove the non-active ledgers.\n+        for (Long entryLogId : entryLogMetaMap.keySet()) {\n+            EntryLogMetadata meta = entryLogMetaMap.get(entryLogId);\n+            for (Long entryLogLedger : meta.ledgersMap.keySet()) {\n+                // Remove the entry log ledger from the set if it isn't active.\n+                if (!ledgerCache.activeLedgerManager.containsActiveLedger(entryLogLedger)) {\n+                    meta.removeLedger(entryLogLedger);\n+                }\n+            }\n+            if (meta.isEmpty()) {\n+                // This means the entry log is not associated with any active ledgers anymore.\n+                // We can remove this entry log file now.\n+                LOG.info(\"Deleting entryLogId \" + entryLogId + \" as it has no active ledgers!\");\n+                removeEntryLog(entryLogId);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Compact entry logs if necessary.\n+     *\n+     * <p>\n+     * Compaction will be executed from low unused space to high unused space.\n+     * Those entry log files whose remaining size percentage is higher than threshold\n+     * would not be compacted.\n+     * </p>\n+     */\n+    private void doCompactEntryLogs(double threshold) {\n+        LOG.info(\"Do compaction to compact those files lower than \" + threshold);\n+        // sort the ledger meta by occupied unused space\n+        Comparator<EntryLogMetadata> sizeComparator = new Comparator<EntryLogMetadata>() {\n+            @Override\n+            public int compare(EntryLogMetadata m1, EntryLogMetadata m2) {\n+                long unusedSize1 = m1.totalSize - m1.remainingSize;\n+                long unusedSize2 = m2.totalSize - m2.remainingSize;\n+                if (unusedSize1 > unusedSize2) {\n+                    return -1;\n+                } else if (unusedSize1 < unusedSize2) {\n+                    return 1;\n+                } else {\n+                    return 0;\n+                }\n+            }\n+        };\n+        List<EntryLogMetadata> logsToCompact = new ArrayList();\n+        logsToCompact.addAll(entryLogMetaMap.values());\n+        Collections.sort(logsToCompact, sizeComparator);\n+        for (EntryLogMetadata meta : logsToCompact) {\n+            if (meta.getUsage() >= threshold) {\n+                break;\n+            }\n+            if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Compacting entry log \" + meta.entryLogId + \" below threshold \"\n+                        + threshold + \".\");\n+            }\n+            compactEntryLog(meta.entryLogId);\n+            if (!running) { // if gc thread is not running, stop compaction\n+                return;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Shutdown the garbage collector thread.\n+     *\n+     * @throws InterruptedException if there is an exception stopping gc thread.\n+     */\n+    public void shutdown() throws InterruptedException {\n+        this.running = false;\n+        if (compacting.compareAndSet(false, true)) {\n+            // if setting compacting flag succeed, means gcThread is not compacting now\n+            // it is safe to interrupt itself now\n+            this.interrupt();\n+        }\n+        this.join();\n+    }\n+\n+    /**\n+     * Remove entry log.\n+     *\n+     * @param entryLogId\n+     *          Entry Log File Id\n+     */\n+    private void removeEntryLog(long entryLogId) {\n+        // remove entry log file successfully\n+        if (entryLogger.removeEntryLog(entryLogId)) {\n+            entryLogMetaMap.remove(entryLogId);\n+        }\n+    }\n+\n+    /**\n+     * Compact an entry log.\n+     *\n+     * @param entryLogId\n+     *          Entry Log File Id\n+     */\n+    protected void compactEntryLog(long entryLogId) {\n+        EntryLogMetadata entryLogMeta = entryLogMetaMap.get(entryLogId);\n+        if (null == entryLogMeta) {\n+            LOG.warn(\"Can't get entry log meta when compacting entry log \" + entryLogId + \".\");\n+            return;\n+        }\n+\n+        // Similar with Sync Thread\n+        // try to mark compacting flag to make sure it would not be interrupted\n+        // by shutdown during compaction. otherwise it will receive\n+        // ClosedByInterruptException which may cause index file & entry logger\n+        // closed and corrupted.\n+        if (!compacting.compareAndSet(false, true)) {\n+            // set compacting flag failed, means compacting is true now\n+            // indicates another thread wants to interrupt gc thread to exit\n+            return;\n+        }\n+\n+        LOG.info(\"Compacting entry log : \" + entryLogId);\n+\n+        try {\n+            entryLogger.scanEntryLog(entryLogId, new CompactionScanner(entryLogMeta));\n+            // after moving entries to new entry log, remove this old one\n+            removeEntryLog(entryLogId);\n+        } catch (IOException e) {\n+            LOG.info(\"Premature exception when compacting \" + entryLogId, e);\n+        } finally {\n+            // clear compacting flag\n+            compacting.set(false);\n+        }\n+    }\n+}"},{"sha":"5783880a7d523105f3bfe0823ccaa37111f054b1","filename":"bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java","status":"modified","additions":110,"deletions":0,"changes":110,"blob_url":"https://github.com/apache/bookkeeper/blob/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java","raw_url":"https://github.com/apache/bookkeeper/raw/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java?ref=e911559291b6ae9aa310a695fb4174796921ac81","patch":"@@ -25,6 +25,10 @@\n public class ServerConfiguration extends AbstractConfiguration {\n     // Entry Log Parameters\n     protected final static String ENTRY_LOG_SIZE_LIMIT = \"logSizeLimit\";\n+    protected final static String MINOR_COMPACTION_INTERVAL = \"minorCompactionInterval\";\n+    protected final static String MINOR_COMPACTION_THRESHOLD = \"minorCompactionThreshold\";\n+    protected final static String MAJOR_COMPACTION_INTERVAL = \"majorCompactionInterval\";\n+    protected final static String MAJOR_COMPACTION_THRESHOLD = \"majorCompactionThreshold\";\n \n     // Gc Parameters\n     protected final static String GC_WAIT_TIME = \"gcWaitTime\";\n@@ -405,4 +409,110 @@ public ServerConfiguration setStatisticsEnabled(boolean enabled) {\n         setProperty(ENABLE_STATISTICS, Boolean.toString(enabled));\n         return this;\n     }\n+\n+    /**\n+     * Get threshold of minor compaction.\n+     *\n+     * For those entry log files whose remaining size percentage reaches below\n+     * this threshold  will be compacted in a minor compaction.\n+     *\n+     * If it is set to less than zero, the minor compaction is disabled.\n+     *\n+     * @return threshold of minor compaction\n+     */\n+    public double getMinorCompactionThreshold() {\n+        return getDouble(MINOR_COMPACTION_THRESHOLD, 0.2f);\n+    }\n+\n+    /**\n+     * Set threshold of minor compaction\n+     *\n+     * @see #getMinorCompactionThreshold()\n+     *\n+     * @param threshold\n+     *          Threshold for minor compaction\n+     * @return server configuration\n+     */\n+    public ServerConfiguration setMinorCompactionThreshold(double threshold) {\n+        setProperty(MINOR_COMPACTION_THRESHOLD, threshold);\n+        return this;\n+    }\n+\n+    /**\n+     * Get threshold of major compaction.\n+     *\n+     * For those entry log files whose remaining size percentage reaches below\n+     * this threshold  will be compacted in a major compaction.\n+     *\n+     * If it is set to less than zero, the major compaction is disabled.\n+     *\n+     * @return threshold of major compaction\n+     */\n+    public double getMajorCompactionThreshold() {\n+        return getDouble(MAJOR_COMPACTION_THRESHOLD, 0.8f);\n+    }\n+\n+    /**\n+     * Set threshold of major compaction.\n+     *\n+     * @see #getMajorCompactionThreshold()\n+     *\n+     * @param threshold\n+     *          Threshold of major compaction\n+     * @return server configuration\n+     */\n+    public ServerConfiguration setMajorCompactionThreshold(double threshold) {\n+        setProperty(MAJOR_COMPACTION_THRESHOLD, threshold);\n+        return this;\n+    }\n+\n+    /**\n+     * Get interval to run minor compaction, in seconds.\n+     *\n+     * If it is set to less than zero, the minor compaction is disabled.\n+     *\n+     * @return threshold of minor compaction\n+     */\n+    public long getMinorCompactionInterval() {\n+        return getLong(MINOR_COMPACTION_INTERVAL, 3600);\n+    }\n+\n+    /**\n+     * Set interval to run minor compaction\n+     *\n+     * @see #getMinorCompactionInterval()\n+     *\n+     * @param interval\n+     *          Interval to run minor compaction\n+     * @return server configuration\n+     */\n+    public ServerConfiguration setMinorCompactionInterval(long interval) {\n+        setProperty(MINOR_COMPACTION_INTERVAL, interval);\n+        return this;\n+    }\n+\n+    /**\n+     * Get interval to run major compaction, in seconds.\n+     *\n+     * If it is set to less than zero, the major compaction is disabled.\n+     *\n+     * @return high water mark\n+     */\n+    public long getMajorCompactionInterval() {\n+        return getLong(MAJOR_COMPACTION_INTERVAL, 86400);\n+    }\n+\n+    /**\n+     * Set interval to run major compaction.\n+     *\n+     * @see #getMajorCompactionInterval()\n+     *\n+     * @param interval\n+     *          Interval to run major compaction\n+     * @return server configuration\n+     */\n+    public ServerConfiguration setMajorCompactionInterval(long interval) {\n+        setProperty(MAJOR_COMPACTION_INTERVAL, interval);\n+        return this;\n+    }\n }"},{"sha":"016289d133c7811c7abbe290e42f27f70e956cf7","filename":"bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","status":"added","additions":317,"deletions":0,"changes":317,"blob_url":"https://github.com/apache/bookkeeper/blob/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","raw_url":"https://github.com/apache/bookkeeper/raw/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/CompactionTest.java?ref=e911559291b6ae9aa310a695fb4174796921ac81","patch":"@@ -0,0 +1,317 @@\n+package org.apache.bookkeeper.bookie;\n+\n+/*\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ */\n+import java.io.File;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+\n+import org.apache.bookkeeper.client.LedgerEntry;\n+import org.apache.bookkeeper.client.LedgerHandle;\n+import org.apache.bookkeeper.client.BookKeeper.DigestType;\n+import org.apache.bookkeeper.test.BaseTestCase;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * This class tests the entry log compaction functionality.\n+ */\n+public class CompactionTest extends BaseTestCase {\n+    static Logger LOG = LoggerFactory.getLogger(CompactionTest.class);\n+    DigestType digestType;\n+\n+    static int ENTRY_SIZE = 1024;\n+\n+    int numEntries;\n+    int gcWaitTime;\n+    double minorCompactionThreshold;\n+    double majorCompactionThreshold;\n+    long minorCompactionInterval;\n+    long majorCompactionInterval;\n+\n+    String msg;\n+\n+    public CompactionTest(DigestType digestType) {\n+        super(3);\n+        this.digestType = digestType;\n+\n+        numEntries = 2048;\n+        gcWaitTime = 1000;\n+        minorCompactionThreshold = 0.1f;\n+        majorCompactionThreshold = 0.5f;\n+        minorCompactionInterval = 2 * gcWaitTime / 1000;\n+        majorCompactionInterval = 4 * gcWaitTime / 1000;\n+\n+        // a dummy message\n+        StringBuilder msgSB = new StringBuilder();\n+        for (int i = 0; i < ENTRY_SIZE; i++) {\n+            msgSB.append(\"a\");\n+        }\n+        msg = msgSB.toString();\n+    }\n+\n+    @Before\n+    @Override\n+    public void setUp() throws Exception {\n+        // Set up the configuration properties needed.\n+        baseConf.setEntryLogSizeLimit(numEntries * ENTRY_SIZE);\n+        baseConf.setGcWaitTime(gcWaitTime);\n+        baseConf.setMinorCompactionThreshold(minorCompactionThreshold);\n+        baseConf.setMajorCompactionThreshold(majorCompactionThreshold);\n+        baseConf.setMinorCompactionInterval(minorCompactionInterval);\n+        baseConf.setMajorCompactionInterval(majorCompactionInterval);\n+\n+        super.setUp();\n+    }\n+\n+    LedgerHandle[] prepareData(int numEntryLogs, boolean changeNum)\n+        throws Exception {\n+        // since an entry log file can hold at most 2048 entries\n+        // first ledger write 2 entries, which is less than low water mark\n+        int num1 = 2;\n+        // third ledger write more than high water mark entries\n+        int num3 = (int)(numEntries * 0.7f);\n+        // second ledger write remaining entries, which is higher than low water mark\n+        // and less than high water mark\n+        int num2 = numEntries - num3 - num1;\n+\n+        LedgerHandle[] lhs = new LedgerHandle[3];\n+        for (int i=0; i<3; ++i) {\n+            lhs[i] = bkc.createLedger(3, 3, digestType, \"\".getBytes());\n+        }\n+\n+        for (int n = 0; n < numEntryLogs; n++) {\n+            for (int k = 0; k < num1; k++) {\n+                lhs[0].addEntry(msg.getBytes());\n+            }\n+            for (int k = 0; k < num2; k++) {\n+                lhs[1].addEntry(msg.getBytes());\n+            }\n+            for (int k = 0; k < num3; k++) {\n+                lhs[2].addEntry(msg.getBytes());\n+            }\n+            if (changeNum) {\n+                --num2;\n+                ++num3;\n+            }\n+        }\n+\n+        return lhs;\n+    }\n+\n+    private void verifyLedger(long lid, long startEntryId, long endEntryId) throws Exception {\n+        LedgerHandle lh = bkc.openLedger(lid, digestType, \"\".getBytes());\n+        Enumeration<LedgerEntry> entries = lh.readEntries(startEntryId, endEntryId);\n+        while (entries.hasMoreElements()) {\n+            LedgerEntry entry = entries.nextElement();\n+            assertEquals(msg, new String(entry.getEntry()));\n+        }\n+    }\n+\n+    private boolean[] checkLogFiles(File ledgerDirectory, int numFiles) {\n+        boolean[] hasLogFiles = new boolean[numFiles];\n+        Arrays.fill(hasLogFiles, false);\n+        for (File f : ledgerDirectory.listFiles()) {\n+            LOG.info(\"Checking file : \" + f);\n+            if (f.isFile()) {\n+                String name = f.getName();\n+                if (!name.endsWith(\".log\")) {\n+                    continue;\n+                }\n+                String idString = name.split(\"\\\\.\")[0];\n+                int id = Integer.parseInt(idString, 16);\n+                if (id >= numFiles) {\n+                    continue;\n+                }\n+                hasLogFiles[id] = true; \n+            }\n+        }\n+        return hasLogFiles;\n+    }\n+\n+    @Test\n+    public void testDisableCompaction() throws Exception {\n+        // prepare data\n+        LedgerHandle[] lhs = prepareData(3, false);\n+\n+        // disable compaction\n+        baseConf.setMinorCompactionThreshold(0.0f);\n+        baseConf.setMajorCompactionThreshold(0.0f);\n+\n+        // restart bookies\n+        restartBookies();\n+\n+        // remove ledger2 and ledger3\n+        // so entry log 1 and 2 would have ledger1 entries left\n+        bkc.deleteLedger(lhs[1].getId());\n+        bkc.deleteLedger(lhs[2].getId());\n+        LOG.info(\"Finished deleting the ledgers contains most entries.\");\n+        Thread.sleep(baseConf.getMajorCompactionInterval() * 1000\n+                   + baseConf.getGcWaitTime());\n+\n+        // entry logs ([0,1].log) should not be compacted.\n+        for (File ledgerDirectory : tmpDirs) {\n+            boolean[] hasLogFiles = checkLogFiles(ledgerDirectory, 2);\n+            assertTrue(\"Not Found entry log file ([0,1].log that should have been compacted in ledgerDirectory: \" + ledgerDirectory, hasLogFiles[0] & hasLogFiles[1]);\n+        }\n+    }\n+\n+    @Test\n+    public void testMinorCompaction() throws Exception {\n+        // prepare data\n+        LedgerHandle[] lhs = prepareData(3, false);\n+\n+        for (LedgerHandle lh : lhs) {\n+            lh.close();\n+        }\n+\n+        // disable major compaction\n+        baseConf.setMajorCompactionThreshold(0.0f);\n+\n+        // restart bookies\n+        restartBookies();\n+\n+        // remove ledger2 and ledger3\n+        bkc.deleteLedger(lhs[1].getId());\n+        bkc.deleteLedger(lhs[2].getId());\n+\n+        LOG.info(\"Finished deleting the ledgers contains most entries.\");\n+        Thread.sleep(baseConf.getMinorCompactionInterval() * 1000\n+                   + baseConf.getGcWaitTime());\n+\n+        // entry logs ([0,1,2].log) should be compacted.\n+        for (File ledgerDirectory : tmpDirs) {\n+            boolean[] hasLog = checkLogFiles(ledgerDirectory, 3); \n+            assertFalse(\"Found entry log file ([0,1,2].log that should have not been compacted in ledgerDirectory: \" + ledgerDirectory, hasLog[0] | hasLog[1] | hasLog[2]);\n+        }\n+\n+        // even entry log files are removed, we still can access entries for ledger1\n+        // since those entries has been compacted to new entry log\n+        verifyLedger(lhs[0].getId(), 0, lhs[0].getLastAddConfirmed());\n+    }\n+\n+    @Test\n+    public void testMajorCompaction() throws Exception {\n+\n+        // prepare data\n+        LedgerHandle[] lhs = prepareData(3, true);\n+\n+        for (LedgerHandle lh : lhs) {\n+            lh.close();\n+        }\n+\n+        // disable minor compaction\n+        baseConf.setMinorCompactionThreshold(0.0f);\n+\n+        // restart bookies\n+        restartBookies();\n+\n+        // remove ledger1 and ledger3\n+        bkc.deleteLedger(lhs[0].getId());\n+        bkc.deleteLedger(lhs[2].getId());\n+        LOG.info(\"Finished deleting the ledgers contains most entries.\");\n+\n+        Thread.sleep(baseConf.getMajorCompactionInterval() * 1000\n+                   + baseConf.getGcWaitTime());\n+\n+        // entry logs ([0,1,2].log) should be compacted\n+        for (File ledgerDirectory : tmpDirs) {\n+            boolean[] hasLogFiles = checkLogFiles(ledgerDirectory, 3); \n+            assertFalse(\"Found entry log file ([0,1,2].log that should have not been compacted in ledgerDirectory: \"\n+                      + ledgerDirectory, hasLogFiles[0] | hasLogFiles[1] | hasLogFiles[2]);\n+        }\n+\n+        // even entry log files are removed, we still can access entries for ledger2\n+        // since those entries has been compacted to new entry log\n+        verifyLedger(lhs[1].getId(), 0, lhs[1].getLastAddConfirmed());\n+    }\n+\n+    @Test\n+    public void testMajorCompactionAboveThreshold() throws Exception {\n+        // prepare data\n+        LedgerHandle[] lhs = prepareData(3, false);\n+\n+        for (LedgerHandle lh : lhs) {\n+            lh.close();\n+        }\n+\n+        // remove ledger1 and ledger2\n+        bkc.deleteLedger(lhs[0].getId());\n+        bkc.deleteLedger(lhs[1].getId());\n+        LOG.info(\"Finished deleting the ledgers contains less entries.\");\n+        Thread.sleep(baseConf.getMajorCompactionInterval() * 1000\n+                   + baseConf.getGcWaitTime());\n+\n+        // entry logs ([0,1,2].log) should not be compacted\n+        for (File ledgerDirectory : tmpDirs) {\n+            boolean[] hasLogFiles = checkLogFiles(ledgerDirectory, 3); \n+            assertTrue(\"Not Found entry log file ([1,2].log that should have been compacted in ledgerDirectory: \"\n+                     + ledgerDirectory, hasLogFiles[0] & hasLogFiles[1] & hasLogFiles[2]);\n+        }\n+    }\n+\n+    @Test\n+    public void testCompactionSmallEntryLogs() throws Exception {\n+\n+        // create a ledger to write a few entries\n+        LedgerHandle alh = bkc.createLedger(3, 3, digestType, \"\".getBytes());\n+        for (int i=0; i<3; i++) {\n+           alh.addEntry(msg.getBytes());\n+        }\n+        alh.close();\n+\n+        // restart bookie to roll entry log files\n+        restartBookies();\n+\n+        // prepare data\n+        LedgerHandle[] lhs = prepareData(3, false);\n+\n+        for (LedgerHandle lh : lhs) {\n+            lh.close();\n+        }\n+\n+        // remove ledger2 and ledger3\n+        bkc.deleteLedger(lhs[1].getId());\n+        bkc.deleteLedger(lhs[2].getId());\n+        LOG.info(\"Finished deleting the ledgers contains most entries.\");\n+        Thread.sleep(baseConf.getMajorCompactionInterval() * 1000\n+                   + baseConf.getGcWaitTime());\n+\n+        // entry logs (0.log) should not be compacted\n+        // entry logs ([1,2,3].log) should be compacted.\n+        for (File ledgerDirectory : tmpDirs) {\n+            boolean[] hasLog = checkLogFiles(ledgerDirectory, 4);\n+            \n+            assertTrue(\"Not Found entry log file ([0].log that should have been compacted in ledgerDirectory: \"\n+                     + ledgerDirectory, hasLog[0]);\n+            assertFalse(\"Found entry log file ([1,2,3].log that should have not been compacted in ledgerDirectory: \"\n+                      + ledgerDirectory, hasLog[1] | hasLog[2] | hasLog[3]);\n+        }\n+\n+        // even entry log files are removed, we still can access entries for ledger1\n+        // since those entries has been compacted to new entry log\n+        verifyLedger(lhs[0].getId(), 0, lhs[0].getLastAddConfirmed());\n+    }\n+}"},{"sha":"281f729d6b2dcf8abe80199153019f99c4d0db3b","filename":"bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/EntryLogTest.java","status":"modified","additions":10,"deletions":13,"changes":23,"blob_url":"https://github.com/apache/bookkeeper/blob/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/EntryLogTest.java","raw_url":"https://github.com/apache/bookkeeper/raw/e911559291b6ae9aa310a695fb4174796921ac81/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/EntryLogTest.java","contents_url":"https://api.github.com/repos/apache/bookkeeper/contents/bookkeeper-server/src/test/java/org/apache/bookkeeper/bookie/EntryLogTest.java?ref=e911559291b6ae9aa310a695fb4174796921ac81","patch":"@@ -23,12 +23,13 @@\n import java.io.File;\n import java.io.IOException;\n import java.io.RandomAccessFile;\n-import java.lang.reflect.Field;\n import java.nio.ByteBuffer;\n import java.util.Map;\n+import java.util.HashMap;\n \n import junit.framework.TestCase;\n \n+import org.apache.bookkeeper.bookie.EntryLogger.EntryLogMetadata;\n import org.apache.bookkeeper.conf.ServerConfiguration;\n import org.junit.After;\n import org.junit.Before;\n@@ -54,7 +55,7 @@ public void testCorruptEntryLog() throws Exception {\n         conf.setGcWaitTime(gcWaitTime);\n         conf.setLedgerDirNames(new String[] {tmpDir.toString()});\n         // create some entries\n-        EntryLogger logger = new EntryLogger(conf, null);\n+        EntryLogger logger = new EntryLogger(conf);\n         logger.addEntry(1, generateEntry(1, 1));\n         logger.addEntry(3, generateEntry(3, 1));\n         logger.addEntry(2, generateEntry(2, 1));\n@@ -65,18 +66,14 @@ public void testCorruptEntryLog() throws Exception {\n         raf.setLength(raf.length()-10);\n         raf.close();\n         // now see which ledgers are in the log\n-        logger = new EntryLogger(conf, null);\n-        logger.start();\n+        logger = new EntryLogger(conf);\n+        EntryLogMetadata meta =\n+            logger.extractMetaFromEntryLog(0L);\n \n-        Thread.sleep(2 * gcWaitTime);\n-        Field entryLogs2LedgersMapField = logger.getClass().getDeclaredField(\"entryLogs2LedgersMap\");\n-        entryLogs2LedgersMapField.setAccessible(true);\n-        @SuppressWarnings(\"unchecked\")\n-        Map<Long, Map<Long, Boolean>> ledgersMap = (Map<Long, Map<Long, Boolean>>) entryLogs2LedgersMapField.get(logger);\n-        LOG.info(\"LedgersMap.get(0) {}\", ledgersMap.get(0L));\n-        assertNotNull(ledgersMap.get(0L).get(1L));\n-        assertNull(ledgersMap.get(0L).get(2L));\n-        assertNotNull(ledgersMap.get(0L).get(3L));\n+        LOG.info(\"Extracted Meta From Entry Log {}\", meta);\n+        assertNotNull(meta.ledgersMap.get(1L));\n+        assertNull(meta.ledgersMap.get(2L));\n+        assertNotNull(meta.ledgersMap.get(3L));\n     }\n \n     private ByteBuffer generateEntry(long ledger, long entry) {"}]}

